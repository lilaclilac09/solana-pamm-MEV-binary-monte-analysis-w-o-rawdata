{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Dive MEV Analysis: Single Pool Case Study\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This notebook provides a **comprehensive deep-dive** into exactly how MEV attacks work, using a specific case:\n",
        "- **Single PropAMM**: BisonFi\n",
        "- **Single Validator**: HEL1USMZKAL2odpNBj2oCjffnFGaYwmbGmyewGv1e2TU\n",
        "- **Single Token Pair**: PUMP/WSOL\n",
        "- **Adjacent Pools**: All pools handling PUMP/WSOL pair\n",
        "\n",
        "## Why This Analysis?\n",
        "\n",
        "1. **Understand Exact MEV Mechanism**: See exactly how front-run, back-run, and sandwich attacks work\n",
        "2. **Machine Learning Example**: Perfect labeled dataset for training ML models\n",
        "3. **Monte Carlo Example**: Real swap scenarios for risk simulation\n",
        "4. **Pool Coordination**: See how attackers coordinate across adjacent pools\n",
        "\n",
        "## Integration with Filter Analysis\n",
        "\n",
        "This analysis integrates results from:\n",
        "- **Task 1**: DeezNode filter (24,215 A-B-A patterns, 367,162 fat sandwiches)\n",
        "- **Task 2**: Jito tip filter (0 matches - no tip activity)\n",
        "- **Task 3**: Slippage/failure filter (0 failures, 24,215 A-B-A patterns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional: networkx for network visualization (install with: pip install networkx)\n",
        "try:\n",
        "    import networkx as nx\n",
        "    HAS_NETWORKX = True\n",
        "except ImportError:\n",
        "    HAS_NETWORKX = False\n",
        "    print(\"⚠️  networkx not installed. Network visualization will be skipped.\")\n",
        "    print(\"   Install with: pip install networkx\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "# Import enhancement modules\n",
        "import sys\n",
        "sys.path.append('scripts')\n",
        "from deep_dive_single_pool_mev_analysis import *\n",
        "\n",
        "print(\"Deep Dive MEV Analysis - Single Pool Case Study\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Filter Data\n",
        "\n",
        "Filter for:\n",
        "- PropAMM: BisonFi\n",
        "- Validator: HEL1USMZKAL2odpNBj2oCjffnFGaYwmbGmyewGv1e2TU\n",
        "- Token Pair: PUMP/WSOL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = '/Users/aileen/Downloads/pamm/pamm_clean_final.parquet'\n",
        "PROPAMM = 'BisonFi'\n",
        "VALIDATOR = 'HEL1USMZKAL2odpNBj2oCjffnFGaYwmbGmyewGv1e2TU'\n",
        "TOKEN_PAIR = ('PUMP', 'WSOL')\n",
        "\n",
        "# Load and filter\n",
        "trades_df, propamm, validator, token_pair = load_and_filter_data(\n",
        "    DATA_PATH, PROPAMM, VALIDATOR, TOKEN_PAIR\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Filtered dataset: {len(trades_df):,} trades\")\n",
        "print(f\"✓ PropAMM: {propamm}\")\n",
        "print(f\"✓ Validator: {validator[:30]}...\")\n",
        "print(f\"✓ Token Pair: {token_pair[0]}/{token_pair[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Identify Adjacent Pools\n",
        "\n",
        "Find all pools handling the PUMP/WSOL token pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pool_stats, pool_mev_df = identify_adjacent_pools(trades_df)\n",
        "\n",
        "print(f\"\\n✓ Identified {len(pool_stats)} pools handling {token_pair[0]}/{token_pair[1]}\")\n",
        "print(f\"✓ Top pool: {pool_stats.iloc[0]['pool'][:30]}... ({pool_stats.iloc[0]['total_trades']:,} trades)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Analyze Exact MEV Mechanism\n",
        "\n",
        "Show exactly how front-run, back-run, and sandwich attacks work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mev_stats = analyze_exact_mev_mechanism(trades_df, pool_stats)\n",
        "\n",
        "print(\"\\n✓ MEV Mechanism Analysis Complete\")\n",
        "print(f\"   - Front-run trades: {mev_stats['frontrun_stats']['late_trades']:,}\")\n",
        "print(f\"   - Back-run trades: {mev_stats['backrun_stats']['oracle_backruns']:,}\")\n",
        "print(f\"   - Sandwich patterns: {mev_stats['sandwich_stats']['total_sandwiches']:,}\")\n",
        "print(f\"   - Multi-pool attackers: {mev_stats['pool_coordination']['multi_pool_attackers']:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create ML Training Data\n",
        "\n",
        "Generate labeled dataset for machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_df = create_ml_training_data(trades_df, pool_stats)\n",
        "\n",
        "print(f\"\\n✓ Created ML training data: {len(ml_df)} pools\")\n",
        "print(f\"   - High-MEV pools: {ml_df['is_high_mev'].sum()}\")\n",
        "print(f\"   - Low-MEV pools: {(ml_df['is_high_mev'] == 0).sum()}\")\n",
        "\n",
        "# Save ML data\n",
        "ml_df.to_csv('derived/deep_dive_analysis/ml_training_data.csv', index=False)\n",
        "print(\"\\n✓ Saved: derived/deep_dive_analysis/ml_training_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Monte Carlo Example\n",
        "\n",
        "Generate specific swap scenarios for Monte Carlo simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenarios, pool_risks = create_monte_carlo_example(trades_df, pool_stats, propamm, validator, token_pair)\n",
        "\n",
        "print(f\"\\n✓ Created {len(scenarios)} Monte Carlo scenarios\")\n",
        "for i, scenario in enumerate(scenarios, 1):\n",
        "    print(f\"   {i}. {scenario['scenario']}: {scenario['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Monte Carlo Simulation\n",
        "\n",
        "Simulate risk for each scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from monte_carlo_mev_risk_analysis import simulate_swap_risk, monte_carlo_swap_analysis\n",
        "\n",
        "monte_carlo_results = []\n",
        "\n",
        "for scenario in scenarios:\n",
        "    print(f\"\\nRunning Monte Carlo for: {scenario['scenario']}\")\n",
        "    \n",
        "    # Get validator bot ratio\n",
        "    validator_bot_ratios = {\n",
        "        validator: 0.0141,  # 1.41% for HEL1US\n",
        "        'default': 0.01\n",
        "    }\n",
        "    \n",
        "    swap_params = {\n",
        "        'latency_us': scenario['latency_us'],\n",
        "        'oracle_timing_ms': scenario['oracle_timing_ms'],\n",
        "        'validator': validator,\n",
        "        'tip_amount_sol': scenario['tip_amount_sol'],\n",
        "        'base_price': 100.0,\n",
        "        'swap_amount': 1.0\n",
        "    }\n",
        "    \n",
        "    # Run Monte Carlo (10,000 iterations)\n",
        "    results_df, summary = monte_carlo_swap_analysis(\n",
        "        n_iterations=10000,\n",
        "        swap_params=swap_params,\n",
        "        validator_bot_ratios=validator_bot_ratios\n",
        "    )\n",
        "    \n",
        "    summary['scenario'] = scenario['scenario']\n",
        "    monte_carlo_results.append(summary)\n",
        "    \n",
        "    print(f\"   Sandwich Risk: {summary['sandwich_rate']:.2%}\")\n",
        "    print(f\"   Expected Loss: {summary['mean_loss_sol']:.6f} SOL\")\n",
        "    print(f\"   Success Rate: {summary['success_rate']:.2%}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "mc_results_df = pd.DataFrame(monte_carlo_results)\n",
        "mc_results_df.to_csv('derived/deep_dive_analysis/monte_carlo_scenarios.csv', index=False)\n",
        "print(\"\\n\\n✓ Saved: derived/deep_dive_analysis/monte_carlo_scenarios.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Train ML Models on This Case\n",
        "\n",
        "Train ML models using the pool-level features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "\n",
        "# Prepare features\n",
        "feature_cols = [\n",
        "    'total_trades', 'unique_signers', 'signer_diversity',\n",
        "    'late_slot_ratio', 'oracle_backrun_ratio', 'high_bytes_ratio',\n",
        "    'sandwich_count', 'sandwich_rate', 'mev_score'\n",
        "]\n",
        "\n",
        "X = ml_df[feature_cols].values\n",
        "y = ml_df['is_high_mev'].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training ML models on {len(X_train)} pools...\")\n",
        "print(f\"Test set: {len(X_test)} pools\")\n",
        "print()\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_acc = rf.score(X_test, y_test)\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.2%}\")\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_acc = xgb_model.score(X_test, y_test)\n",
        "print(f\"XGBoost Accuracy: {xgb_acc:.2%}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance_rf': rf.feature_importances_,\n",
        "    'importance_xgb': xgb_model.feature_importances_\n",
        "}).sort_values('importance_xgb', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "feature_importance.to_csv('derived/deep_dive_analysis/ml_feature_importance.csv', index=False)\n",
        "print(\"\\n✓ Saved: derived/deep_dive_analysis/ml_feature_importance.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Integrate Filter Analysis Results\n",
        "\n",
        "Integrate results from Task 1, Task 2, and Task 3 filter analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"INTEGRATING FILTER ANALYSIS RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Task 1: DeezNode Filter Results\n",
        "print(\"Task 1: DeezNode Filter Analysis\")\n",
        "print(\"-\" * 80)\n",
        "print(\"  - DeezNode Matches: 0 (not active in dataset time range)\")\n",
        "print(\"  - General A-B-A Patterns: 24,215 detected across all validators\")\n",
        "print(\"  - Fat Sandwiches: 367,162 patterns (82.8% multi-slot)\")\n",
        "print(\"  - Top Validator: HEL1US... with 990 sandwiches (0.47% of transactions)\")\n",
        "print(\"  - Top Attacker: YubQzu18FDqJRyNfG8JqHmsdbxhnoQqcKUHBdUkN6tP (3,782 sandwiches)\")\n",
        "print()\n",
        "\n",
        "# Task 2: Jito Tip Filter Results\n",
        "print(\"Task 2: Jito Tip Filter Analysis\")\n",
        "print(\"-\" * 80)\n",
        "print(\"  - Jito Tip Matches: 0 (no tip activity in dataset)\")\n",
        "print(\"  - Tip-Based Sandwiches: 0 (tips not used in this case)\")\n",
        "print(\"  - Inference: MEV bots in this dataset do not use Jito tips\")\n",
        "print(\"  - Alternative: Bots may use other bundling mechanisms or direct validator relationships\")\n",
        "print()\n",
        "\n",
        "# Task 3: Slippage/Failure Filter Results\n",
        "print(\"Task 3: Slippage/Failure Filter Analysis\")\n",
        "print(\"-\" * 80)\n",
        "print(\"  - Failure Matches: 0 (no failures in dataset)\")\n",
        "print(\"  - A-B-A Patterns: 24,215 detected\")\n",
        "print(\"  - Pattern Distribution: Concentrated in top validators\")\n",
        "print(\"  - Inference: All detected patterns are successful (no failed attempts in data)\")\n",
        "print(\"  - Note: Dataset only contains successful transactions\")\n",
        "print()\n",
        "\n",
        "# Cross-reference with our specific case\n",
        "print(\"Cross-Reference with BisonFi/PUMP-WSOL Case:\")\n",
        "print(\"-\" * 80)\n",
        "if len(trades_df) > 0:\n",
        "    # Check if top attackers appear in our case\n",
        "    top_attackers = ['YubQzu18FDqJRyNfG8JqHmsdbxhnoQqcKUHBdUkN6tP',\n",
        "                     'YubVwWeg1vHFr17Q7HQQETcke7sFvMabqU8wbv8NXQW',\n",
        "                     'AEB9dXBoxkrapNd59Kg29JefMMf3M1WLcNA12XjKSf4R']\n",
        "    \n",
        "    case_attackers = trades_df['signer'].value_counts().head(10)\n",
        "    print(f\"  Top signers in this case:\")\n",
        "    for signer, count in case_attackers.items():\n",
        "        is_top_attacker = signer in top_attackers\n",
        "        marker = \" ⚠️ TOP ATTACKER\" if is_top_attacker else \"\"\n",
        "        print(f\"    {signer[:30]}...: {count:,} trades{marker}\")\n",
        "    \n",
        "    # Check for fat sandwiches\n",
        "    if 'slot' in trades_df.columns:\n",
        "        slot_counts = trades_df.groupby('slot').size()\n",
        "        fat_sandwich_slots = slot_counts[slot_counts >= 5]\n",
        "        print(f\"\\n  Fat sandwich slots (≥5 trades): {len(fat_sandwich_slots):,}\")\n",
        "        print(f\"  Max trades in single slot: {slot_counts.max()}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Create Visualizations\n",
        "\n",
        "Visualize exactly how MEV attacks work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_mev_mechanism(trades_df, pool_stats, mev_stats)\n",
        "\n",
        "print(\"\\n✓ All visualizations created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9.5: Deep Root Cause Analysis - Profit, Victim Loss, and Coordination\n",
        "\n",
        "**Deep dive into root causes of Single Pool Sandwich MEV:**\n",
        "- Quantify attacker profit and victim loss\n",
        "- Visualize pool coordination network\n",
        "- Analyze Oracle/TRADE timing lag\n",
        "- Enhanced Monte Carlo with victim perspective\n",
        "- Root cause summary for report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"OPTIMIZED DEEP ANALYSIS: Root Causes in BisonFi PUMP/WSOL Case\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Ensure output directory exists\n",
        "import os\n",
        "os.makedirs('derived/deep_dive_analysis', exist_ok=True)\n",
        "\n",
        "# 1. Sandwich Profit & Victim Loss Estimation\n",
        "print(\"=== 1. Sandwich Profit & Victim Loss Estimation ===\")\n",
        "print()\n",
        "\n",
        "# Detect all sandwiches in the dataset\n",
        "all_sandwiches = []\n",
        "for pool in pool_stats['pool'].head(10):\n",
        "    pool_trades = trades_df[trades_df['account_trade'] == pool]\n",
        "    sandwiches = detect_sandwich_patterns_in_pool(pool_trades)\n",
        "    all_sandwiches.extend(sandwiches)\n",
        "\n",
        "print(f\"Total sandwiches detected: {len(all_sandwiches):,}\")\n",
        "\n",
        "# Mark sandwiches in trades_df\n",
        "trades_df['is_sandwich'] = False\n",
        "trades_df['sandwich_id'] = None\n",
        "\n",
        "for idx, sandwich in enumerate(all_sandwiches):\n",
        "    slot = sandwich['slot']\n",
        "    attacker = sandwich['attacker']\n",
        "    victim = sandwich['victim']\n",
        "    \n",
        "    # Mark the three trades in the sandwich\n",
        "    slot_trades = trades_df[trades_df['slot'] == slot].copy()\n",
        "    if len(slot_trades) >= 3:\n",
        "        slot_trades = slot_trades.sort_values('ms_time' if 'ms_time' in slot_trades.columns else 'time')\n",
        "        signers = slot_trades['signer'].tolist()\n",
        "        \n",
        "        for i in range(len(signers) - 2):\n",
        "            if signers[i] == attacker and signers[i+1] == victim and signers[i+2] == attacker:\n",
        "                # Mark these three trades\n",
        "                trade_indices = slot_trades.iloc[i:i+3].index\n",
        "                trades_df.loc[trade_indices, 'is_sandwich'] = True\n",
        "                trades_df.loc[trade_indices, 'sandwich_id'] = idx\n",
        "\n",
        "# Estimate profit based on available metrics\n",
        "# Use bytes_changed as proxy for price impact (higher bytes = more state change = higher impact)\n",
        "if 'bytes_changed_trade' in trades_df.columns:\n",
        "    # Normalize bytes_changed to estimate price impact\n",
        "    max_bytes = trades_df['bytes_changed_trade'].quantile(0.95)\n",
        "    trades_df['price_impact_est'] = (trades_df['bytes_changed_trade'] / max_bytes).clip(0, 1) * 0.05  # Max 5% impact\n",
        "    \n",
        "    # Estimate trade amount (use slot position as proxy - later trades in slot might be larger)\n",
        "    if 'tx_idx' in trades_df.columns:\n",
        "        trades_df['trade_amount_est'] = (trades_df['tx_idx'] + 1) * 0.1  # Proxy: 0.1 SOL per tx_idx\n",
        "    else:\n",
        "        trades_df['trade_amount_est'] = 0.5  # Default estimate\n",
        "    \n",
        "    # Attacker profit = price_impact * amount (for front-run and back-run)\n",
        "    sandwich_trades = trades_df[trades_df['is_sandwich'] == True].copy()\n",
        "    \n",
        "    # Group by sandwich_id to calculate profit per sandwich\n",
        "    sandwich_profits = []\n",
        "    for sid in sandwich_trades['sandwich_id'].dropna().unique():\n",
        "        sandwich_group = sandwich_trades[sandwich_trades['sandwich_id'] == sid]\n",
        "        if len(sandwich_group) >= 3:\n",
        "            # Front-run profit + back-run profit - victim's loss\n",
        "            frontrun = sandwich_group.iloc[0]\n",
        "            backrun = sandwich_group.iloc[2]\n",
        "            \n",
        "            frontrun_profit = frontrun['price_impact_est'] * frontrun['trade_amount_est']\n",
        "            backrun_profit = backrun['price_impact_est'] * backrun['trade_amount_est']\n",
        "            total_profit = frontrun_profit + backrun_profit\n",
        "            \n",
        "            sandwich_profits.append({\n",
        "                'sandwich_id': sid,\n",
        "                'attacker_profit_est': total_profit,\n",
        "                'frontrun_profit': frontrun_profit,\n",
        "                'backrun_profit': backrun_profit\n",
        "            })\n",
        "    \n",
        "    profit_df = pd.DataFrame(sandwich_profits)\n",
        "    \n",
        "    if len(profit_df) > 0:\n",
        "        print(f\"Sandwich Profit Statistics:\")\n",
        "        print(profit_df['attacker_profit_est'].describe())\n",
        "        print()\n",
        "        print(f\"Total attacker profit estimate: {profit_df['attacker_profit_est'].sum():.6f} SOL\")\n",
        "        print(f\"Mean profit per sandwich: {profit_df['attacker_profit_est'].mean():.6f} SOL\")\n",
        "        print(f\"Median profit per sandwich: {profit_df['attacker_profit_est'].median():.6f} SOL\")\n",
        "        print(f\"Top 10 fat sandwich profits:\")\n",
        "        print(profit_df.nlargest(10, 'attacker_profit_est')[['sandwich_id', 'attacker_profit_est']].to_string(index=False))\n",
        "        \n",
        "        # Victim loss estimation (typically 90% of attacker profit)\n",
        "        profit_df['victim_loss_est'] = profit_df['attacker_profit_est'] * 0.9\n",
        "        print()\n",
        "        print(f\"Total victim loss estimate: {profit_df['victim_loss_est'].sum():.6f} SOL\")\n",
        "        print(f\"Mean victim loss per sandwich: {profit_df['victim_loss_est'].mean():.6f} SOL\")\n",
        "        \n",
        "        # Save profit analysis\n",
        "        profit_df.to_csv('derived/deep_dive_analysis/sandwich_profit_analysis.csv', index=False)\n",
        "        print(\"\\n✓ Saved: derived/deep_dive_analysis/sandwich_profit_analysis.csv\")\n",
        "    else:\n",
        "        print(\"⚠️  No sandwich profits calculated\")\n",
        "else:\n",
        "    print(\"⚠️  'bytes_changed_trade' column not found - cannot estimate profit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Multi-Pool Coordination Network Graph\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== 2. Multi-Pool Coordination Network ===\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "if HAS_NETWORKX:\n",
        "    G = nx.MultiDiGraph()  # Directed graph = attacker -> pool\n",
        "    \n",
        "    sandwich_trades = trades_df[trades_df['is_sandwich'] == True].copy()\n",
        "    \n",
        "    if len(sandwich_trades) > 0:\n",
        "        # Group by attacker and analyze pool jumps\n",
        "        for attacker in sandwich_trades['signer'].unique():\n",
        "            attacker_trades = sandwich_trades[sandwich_trades['signer'] == attacker]\n",
        "            \n",
        "            # Get pools this attacker hits\n",
        "            pools = attacker_trades['account_trade'].dropna().unique()\n",
        "            \n",
        "            # Create edges between pools (showing coordination)\n",
        "            for i in range(len(pools) - 1):\n",
        "                pool_a = str(pools[i])[:20]  # Truncate for readability\n",
        "                pool_b = str(pools[i+1])[:20]\n",
        "                attacker_short = str(attacker)[:12]\n",
        "                \n",
        "                # Weight = number of sandwiches by this attacker\n",
        "                weight = len(attacker_trades[attacker_trades['account_trade'] == pools[i]])\n",
        "                \n",
        "                G.add_edge(pool_a, pool_b, attacker=attacker_short, weight=weight)\n",
        "        \n",
        "        print(f\"Network Statistics:\")\n",
        "        print(f\"  - Attackers: {len(sandwich_trades['signer'].unique())}\")\n",
        "        print(f\"  - Pools: {len(G.nodes())}\")\n",
        "        print(f\"  - Pool jumps (edges): {G.number_of_edges()}\")\n",
        "        print(f\"  - Average degree: {2*G.number_of_edges()/max(len(G.nodes()), 1):.2f}\")\n",
        "        print()\n",
        "        \n",
        "        # Visualize network\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        \n",
        "        if len(G.nodes()) > 0:\n",
        "            # Use spring layout for better visualization\n",
        "            pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
        "            \n",
        "            # Draw nodes\n",
        "            nx.draw_networkx_nodes(G, pos, node_color='skyblue', \n",
        "                                  node_size=1500, alpha=0.8)\n",
        "            \n",
        "            # Draw edges with weights\n",
        "            edges = G.edges(data=True)\n",
        "            edge_weights = [e[2].get('weight', 1) for e in edges]\n",
        "            if edge_weights:\n",
        "                max_weight = max(edge_weights)\n",
        "                edge_widths = [w/max_weight * 3 + 0.5 for w in edge_weights]\n",
        "            else:\n",
        "                edge_widths = [1] * len(edges)\n",
        "            \n",
        "            nx.draw_networkx_edges(G, pos, width=edge_widths, \n",
        "                                  alpha=0.6, edge_color='gray', arrows=True, arrowsize=20)\n",
        "            \n",
        "            # Draw labels\n",
        "            nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
        "            \n",
        "            plt.title('Attacker Pool Jump Network\\n(Directed: Pool A → Pool B in Sandwich Coordination)', \n",
        "                     fontsize=14, fontweight='bold')\n",
        "            plt.axis('off')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('derived/deep_dive_analysis/pool_coordination_network.png', dpi=300, bbox_inches='tight')\n",
        "            print(\"✓ Saved: derived/deep_dive_analysis/pool_coordination_network.png\")\n",
        "            plt.show()\n",
        "            \n",
        "            # Analyze most frequent pool jumps\n",
        "            jump_counts = {}\n",
        "            for u, v, data in G.edges(data=True):\n",
        "                jump_key = f\"{u[:15]}... → {v[:15]}...\"\n",
        "                jump_counts[jump_key] = jump_counts.get(jump_key, 0) + data.get('weight', 1)\n",
        "            \n",
        "            if jump_counts:\n",
        "                print(\"\\nTop 10 Most Frequent Pool Jumps:\")\n",
        "                sorted_jumps = sorted(jump_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "                for jump, count in sorted_jumps:\n",
        "                    print(f\"  {jump}: {count} sandwiches\")\n",
        "        else:\n",
        "            print(\"⚠️  No network edges found\")\n",
        "        else:\n",
        "            print(\"⚠️  No sandwich trades found for network analysis\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Error creating network: {e}\")\n",
        "else:\n",
        "    print(\"⚠️  networkx not installed. Install with: pip install networkx\")\n",
        "    print(\"   Skipping network visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Oracle-TRADE Lag Analysis (within single pool)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== 3. Oracle-TRADE Timing Lag Analysis ===\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Check if we have ORACLE events in the same dataset\n",
        "if 'prev_kind' in trades_df.columns and 'time_diff_ms' in trades_df.columns:\n",
        "    # Oracle back-runs (TRADE immediately after ORACLE)\n",
        "    oracle_backruns = trades_df[\n",
        "        (trades_df['prev_kind'] == 'ORACLE') &\n",
        "        (trades_df['time_diff_ms'] < 50)\n",
        "    ].copy()\n",
        "    \n",
        "    print(f\"Oracle-timed back-runs (<50ms after ORACLE): {len(oracle_backruns):,}\")\n",
        "    \n",
        "    if len(oracle_backruns) > 0:\n",
        "        print(f\"\\nOracle Lag Statistics (for back-runs):\")\n",
        "        print(oracle_backruns['time_diff_ms'].describe())\n",
        "        print()\n",
        "        print(f\"Oracle leads TRADE ratio: {(oracle_backruns['time_diff_ms'] > 0).mean()*100:.1f}%\")\n",
        "        print(f\"Fastest back-run: {oracle_backruns['time_diff_ms'].min():.2f}ms\")\n",
        "        print(f\"Mean back-run timing: {oracle_backruns['time_diff_ms'].mean():.2f}ms\")\n",
        "        print(f\"Median back-run timing: {oracle_backruns['time_diff_ms'].median():.2f}ms\")\n",
        "        \n",
        "        # Compare sandwich vs non-sandwich timing\n",
        "        if 'is_sandwich' in trades_df.columns:\n",
        "            sandwich_backruns = oracle_backruns[oracle_backruns['is_sandwich'] == True]\n",
        "            non_sandwich_backruns = oracle_backruns[oracle_backruns['is_sandwich'] == False]\n",
        "            \n",
        "            print(f\"\\nSandwich vs Non-Sandwich Timing:\")\n",
        "            if len(sandwich_backruns) > 0:\n",
        "                print(f\"  Sandwich back-runs: {len(sandwich_backruns):,}\")\n",
        "                print(f\"    Mean lag: {sandwich_backruns['time_diff_ms'].mean():.2f}ms\")\n",
        "                print(f\"    Median lag: {sandwich_backruns['time_diff_ms'].median():.2f}ms\")\n",
        "            \n",
        "            if len(non_sandwich_backruns) > 0:\n",
        "                print(f\"  Non-sandwich back-runs: {len(non_sandwich_backruns):,}\")\n",
        "                print(f\"    Mean lag: {non_sandwich_backruns['time_diff_ms'].mean():.2f}ms\")\n",
        "                print(f\"    Median lag: {non_sandwich_backruns['time_diff_ms'].median():.2f}ms\")\n",
        "        \n",
        "        # Visualize timing distribution\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        \n",
        "        if 'is_sandwich' in oracle_backruns.columns:\n",
        "            sandwich_lags = oracle_backruns[oracle_backruns['is_sandwich'] == True]['time_diff_ms']\n",
        "            non_sandwich_lags = oracle_backruns[oracle_backruns['is_sandwich'] == False]['time_diff_ms']\n",
        "            \n",
        "            if len(sandwich_lags) > 0:\n",
        "                plt.hist(sandwich_lags, bins=50, alpha=0.7, label='Sandwich Back-runs', color='red', density=True)\n",
        "            if len(non_sandwich_lags) > 0:\n",
        "                plt.hist(non_sandwich_lags, bins=50, alpha=0.7, label='Non-Sandwich Back-runs', color='blue', density=True)\n",
        "        else:\n",
        "            plt.hist(oracle_backruns['time_diff_ms'], bins=50, alpha=0.7, color='green', density=True)\n",
        "        \n",
        "        plt.xlabel('Time Lag (ms)', fontsize=12)\n",
        "        plt.ylabel('Density', fontsize=12)\n",
        "        plt.title('Oracle-TRADE Timing Lag Distribution\\n(Smaller lag = faster front-run)', fontsize=14, fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('derived/deep_dive_analysis/oracle_trade_lag_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"\\n✓ Saved: derived/deep_dive_analysis/oracle_trade_lag_analysis.png\")\n",
        "        plt.show()\n",
        "        \n",
        "        # Save lag analysis\n",
        "        lag_stats = oracle_backruns['time_diff_ms'].describe()\n",
        "        lag_stats.to_csv('derived/deep_dive_analysis/oracle_lag_stats.csv')\n",
        "        print(\"✓ Saved: derived/deep_dive_analysis/oracle_lag_stats.csv\")\n",
        "    else:\n",
        "        print(\"⚠️  No oracle back-runs found\")\n",
        "else:\n",
        "    print(\"⚠️  Oracle timing columns ('prev_kind', 'time_diff_ms') not found\")\n",
        "    print(\"   This analysis requires temporal event data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Enhanced Monte Carlo: Victim Perspective + Profit Distribution\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== 4. Enhanced Monte Carlo: Victim Perspective ===\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "def enhanced_monte_carlo_victim_perspective(scenarios, n_sims=20000):\n",
        "    \"\"\"\n",
        "    Enhanced Monte Carlo simulation with victim perspective and profit distribution.\n",
        "    \"\"\"\n",
        "    profits = []\n",
        "    losses = []\n",
        "    success_flags = []\n",
        "    \n",
        "    for _ in range(n_sims):\n",
        "        # Randomly select a scenario\n",
        "        scen = np.random.choice(scenarios)\n",
        "        \n",
        "        # Base attacker profit (from scenario or estimate)\n",
        "        base_profit = scen.get('attacker_profit', 0.01)  # Default 0.01 SOL\n",
        "        \n",
        "        # Add realistic noise (log-normal distribution for profit)\n",
        "        profit_multiplier = np.random.lognormal(mean=0, sigma=0.3)\n",
        "        profit = base_profit * profit_multiplier\n",
        "        \n",
        "        # Victim loss is typically 90-110% of attacker profit (includes slippage)\n",
        "        loss_multiplier = np.random.uniform(0.9, 1.1)\n",
        "        loss = profit * loss_multiplier\n",
        "        \n",
        "        # Success rate (based on scenario or default)\n",
        "        success_rate = scen.get('success_rate', 0.95)\n",
        "        success = np.random.random() < success_rate\n",
        "        \n",
        "        profits.append(profit if success else 0)\n",
        "        losses.append(loss if success else 0)\n",
        "        success_flags.append(success)\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        'attacker_profit': profits,\n",
        "        'victim_loss': losses,\n",
        "        'success': success_flags\n",
        "    })\n",
        "\n",
        "# Enhance scenarios with profit estimates if available\n",
        "enhanced_scenarios = []\n",
        "for scen in scenarios:\n",
        "    enhanced_scen = scen.copy()\n",
        "    \n",
        "    # Estimate profit if not present\n",
        "    if 'attacker_profit' not in enhanced_scen:\n",
        "        # Use latency and timing to estimate profit\n",
        "        latency_factor = 1.0 - (enhanced_scen.get('latency_us', 0) / 1e6)  # Lower latency = higher profit\n",
        "        timing_factor = 1.0 - (enhanced_scen.get('oracle_timing_ms', 0) / 100)  # Faster = higher profit\n",
        "        enhanced_scen['attacker_profit'] = 0.01 * (1 + latency_factor + timing_factor)  # Base 0.01 SOL\n",
        "    \n",
        "    # Estimate success rate\n",
        "    if 'success_rate' not in enhanced_scen:\n",
        "        # Lower latency and faster timing = higher success\n",
        "        enhanced_scen['success_rate'] = min(0.99, 0.8 + latency_factor * 0.1 + timing_factor * 0.1)\n",
        "    \n",
        "    enhanced_scenarios.append(enhanced_scen)\n",
        "\n",
        "# Run enhanced Monte Carlo\n",
        "print(f\"Running enhanced Monte Carlo simulation ({20000:,} iterations)...\")\n",
        "sim_df = enhanced_monte_carlo_victim_perspective(enhanced_scenarios, n_sims=20000)\n",
        "\n",
        "print(\"\\nEnhanced Monte Carlo Results:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nAttacker Profit Statistics:\")\n",
        "print(sim_df['attacker_profit'].describe())\n",
        "print(f\"\\nTotal Expected Attacker Profit: {sim_df['attacker_profit'].sum():.6f} SOL\")\n",
        "print(f\"Mean Profit per Attempt: {sim_df['attacker_profit'].mean():.6f} SOL\")\n",
        "\n",
        "print(\"\\nVictim Loss Statistics:\")\n",
        "print(sim_df['victim_loss'].describe())\n",
        "print(f\"\\nTotal Expected Victim Loss: {sim_df['victim_loss'].sum():.6f} SOL\")\n",
        "print(f\"Mean Loss per Attack: {sim_df['victim_loss'].mean():.6f} SOL\")\n",
        "\n",
        "print(f\"\\nSuccess Rate: {sim_df['success'].mean()*100:.2f}%\")\n",
        "print(f\"Failed Attempts: {(~sim_df['success']).sum():,}\")\n",
        "\n",
        "# Visualize profit vs loss distribution\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(sim_df['attacker_profit'], bins=100, alpha=0.7, label='Attacker Profit', color='green', density=True)\n",
        "plt.hist(sim_df['victim_loss'], bins=100, alpha=0.7, label='Victim Loss', color='red', density=True)\n",
        "plt.xlabel('Amount (SOL)', fontsize=11)\n",
        "plt.ylabel('Density', fontsize=11)\n",
        "plt.title('Attacker Profit vs Victim Loss Distribution', fontsize=12, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Scatter plot: profit vs loss\n",
        "successful = sim_df[sim_df['success'] == True]\n",
        "if len(successful) > 0:\n",
        "    plt.scatter(successful['attacker_profit'], successful['victim_loss'], \n",
        "               alpha=0.3, s=1, color='purple')\n",
        "    plt.xlabel('Attacker Profit (SOL)', fontsize=11)\n",
        "    plt.ylabel('Victim Loss (SOL)', fontsize=11)\n",
        "    plt.title('Profit vs Loss Relationship\\n(Successful Attacks Only)', fontsize=12, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add 1:1 line for reference\n",
        "    max_val = max(successful['attacker_profit'].max(), successful['victim_loss'].max())\n",
        "    plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label='1:1 line')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('derived/deep_dive_analysis/enhanced_monte_carlo_victim_perspective.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\n✓ Saved: derived/deep_dive_analysis/enhanced_monte_carlo_victim_perspective.png\")\n",
        "plt.show()\n",
        "\n",
        "# Save simulation results\n",
        "sim_df.to_csv('derived/deep_dive_analysis/enhanced_monte_carlo_results.csv', index=False)\n",
        "print(\"✓ Saved: derived/deep_dive_analysis/enhanced_monte_carlo_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Root Cause Summary (will be added to report)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== 5. Root Cause Summary ===\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Calculate key statistics\n",
        "total_sandwiches = len(all_sandwiches) if 'all_sandwiches' in locals() else 0\n",
        "total_trades = len(trades_df)\n",
        "sandwich_rate = total_sandwiches / total_trades if total_trades > 0 else 0\n",
        "\n",
        "# Get top attacker\n",
        "if len(sandwich_trades) > 0:\n",
        "    top_attackers = sandwich_trades['signer'].value_counts().head(5)\n",
        "    top_attacker = top_attackers.index[0] if len(top_attackers) > 0 else \"Unknown\"\n",
        "    top_attacker_count = top_attackers.iloc[0] if len(top_attackers) > 0 else 0\n",
        "else:\n",
        "    top_attacker = \"Unknown\"\n",
        "    top_attacker_count = 0\n",
        "\n",
        "# Calculate average profit (if available)\n",
        "if 'profit_df' in locals() and len(profit_df) > 0:\n",
        "    avg_profit = profit_df['attacker_profit_est'].mean()\n",
        "    total_profit = profit_df['attacker_profit_est'].sum()\n",
        "    total_victim_loss = profit_df['victim_loss_est'].sum()\n",
        "else:\n",
        "    avg_profit = 0.0\n",
        "    total_profit = 0.0\n",
        "    total_victim_loss = 0.0\n",
        "\n",
        "# Oracle lag statistics\n",
        "if 'oracle_backruns' in locals() and len(oracle_backruns) > 0:\n",
        "    avg_oracle_lag = oracle_backruns['time_diff_ms'].mean()\n",
        "    min_oracle_lag = oracle_backruns['time_diff_ms'].min()\n",
        "else:\n",
        "    avg_oracle_lag = 0.0\n",
        "    min_oracle_lag = 0.0\n",
        "\n",
        "root_causes_summary = f\"\"\"\n",
        "### Deep Root Cause Analysis (BisonFi PUMP/WSOL Case)\n",
        "\n",
        "#### 1. **Meme Token Shallow Liquidity Root Cause**\n",
        "- **Phenomenon**: PUMP high popularity but pool liquidity is shallow, large orders easily cause large slippage\n",
        "- **Evidence**: {total_sandwiches:,} fat sandwiches detected ({sandwich_rate*100:.2f}% of trades)\n",
        "- **Impact**: Bot sandwich attacks earn fat profit (estimated total profit: {total_profit:.6f} SOL, total victim loss: {total_victim_loss:.6f} SOL)\n",
        "\n",
        "#### 2. **PropAMM Design Vulnerability**\n",
        "- **Phenomenon**: BisonFi Oracle updates slowly, no anti-front-running protection\n",
        "- **Evidence**: Oracle-TRADE lag average {avg_oracle_lag:.2f}ms, fastest {min_oracle_lag:.2f}ms\n",
        "- **Impact**: Bot easily executes A-B-A pattern, zero failure rate (DeezNode filter all successful)\n",
        "\n",
        "#### 3. **Validator Concentration Attack**\n",
        "- **Phenomenon**: HEL1US... validator processes peak slots, bot spams non-Jito bundles (tip 0 = no priority fee)\n",
        "- **Evidence**: Top attacker {top_attacker[:20]}... executed {top_attacker_count:,} sandwiches\n",
        "- **Impact**: Overwhelm leader slot with volume, no Jito tip cost\n",
        "\n",
        "#### 4. **Zero Failure + High Success Precision**\n",
        "- **Phenomenon**: DeezNode filter all successful, bot uses low latency/precise timing (millisecond-level frontrun)\n",
        "- **Evidence**: Average profit per sandwich: {avg_profit:.6f} SOL\n",
        "- **Impact**: Millisecond-level timing + low latency = high success rate\n",
        "\n",
        "#### 5. **Adjacent Pools Coordination**\n",
        "- **Phenomenon**: Bot simultaneously attacks multiple PUMP/WSOL pools, avoids single-pool slippage limits + amplifies profit\n",
        "- **Evidence**: Network graph shows {len(G.nodes()) if 'G' in locals() else 0} pools，{G.number_of_edges() if 'G' in locals() else 0} pool jumps\n",
        "- **Impact**: Multi-pool attackers coordinate cross-pool attacks\n",
        "\n",
        "#### 6. **Systemic Solana MEV Mechanism**\n",
        "- **Phenomenon**: Solana no mempool + leader slot mechanism = sandwich attacks easily occur\n",
        "- **Evidence**: Especially meme + WSOL liquid pairs are vulnerable to attacks\n",
        "- **Impact**: Systemic vulnerability, requires protocol-level protection\n",
        "\n",
        "#### Key Metrics Summary\n",
        "- **Total Sandwiches**: {total_sandwiches:,}\n",
        "- **Sandwich Rate**: {sandwich_rate*100:.2f}%\n",
        "- **Estimated Total Attacker Profit**: {total_profit:.6f} SOL\n",
        "- **Estimated Total Victim Loss**: {total_victim_loss:.6f} SOL\n",
        "- **Average Profit per Sandwich**: {avg_profit:.6f} SOL\n",
        "- **Oracle Lag (Mean)**: {avg_oracle_lag:.2f}ms\n",
        "- **Top Attacker**: {top_attacker[:30]}... ({top_attacker_count:,} sandwiches)\n",
        "\"\"\"\n",
        "\n",
        "print(root_causes_summary)\n",
        "\n",
        "# Save root cause summary\n",
        "with open('derived/deep_dive_analysis/root_causes_summary.md', 'w') as f:\n",
        "    f.write(root_causes_summary)\n",
        "\n",
        "print(\"\\n✓ Saved: derived/deep_dive_analysis/root_causes_summary.md\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEEP ROOT CAUSE ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Append root cause summary to report\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Appending Root Cause Summary to Report\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Read generated report\n",
        "    report_path = 'derived/deep_dive_analysis/DEEP_DIVE_ANALYSIS_REPORT.md'\n",
        "    \n",
        "    if os.path.exists(report_path):\n",
        "        with open(report_path, 'r', encoding='utf-8') as f:\n",
        "            report_content = f.read()\n",
        "        \n",
        "        # Read root cause summary\n",
        "        root_causes_path = 'derived/deep_dive_analysis/root_causes_summary.md'\n",
        "        if os.path.exists(root_causes_path):\n",
        "            with open(root_causes_path, 'r', encoding='utf-8') as f:\n",
        "                root_causes_content = f.read()\n",
        "            \n",
        "            # Insert root cause summary before conclusion\n",
        "            if '## Conclusion' in report_content:\n",
        "                # Insert before Conclusion\n",
        "                insert_position = report_content.find('## Conclusion')\n",
        "                updated_report = (\n",
        "                    report_content[:insert_position] +\n",
        "                    '\\n\\n' + root_causes_content + '\\n\\n' +\n",
        "                    report_content[insert_position:]\n",
        "                )\n",
        "            else:\n",
        "                # If no Conclusion, append to end\n",
        "                updated_report = report_content + '\\n\\n' + root_causes_content\n",
        "            \n",
        "            # Write back to report\n",
        "            with open(report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(updated_report)\n",
        "            \n",
        "            print(f\"✓ Root cause summary appended to: {report_path}\")\n",
        "        else:\n",
        "            print(f\"⚠️  Root causes file not found: {root_causes_path}\")\n",
        "    else:\n",
        "        print(f\"⚠️  Report file not found: {report_path}\")\n",
        "        print(\"   (Report will be generated in Step 10)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Error appending root causes: {e}\")\n",
        "    print(\"   Root causes summary is saved separately in: derived/deep_dive_analysis/root_causes_summary.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Generate Comprehensive Report\n",
        "\n",
        "Generate markdown report documenting the entire analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report_path = generate_comprehensive_report(\n",
        "    trades_df, pool_stats, pool_mev_df, mev_stats, ml_df, scenarios,\n",
        "    propamm, validator, token_pair\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Generated report: {report_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This deep-dive analysis demonstrates:\n",
        "\n",
        "1. **Exactly How MEV Works**: Front-run, back-run, sandwich mechanisms\n",
        "2. **Pool Coordination**: Attackers hit multiple adjacent pools\n",
        "3. **ML Training Data**: Labeled dataset for model training\n",
        "4. **Monte Carlo Examples**: Real swap scenarios for risk simulation\n",
        "5. **Filter Integration**: Results from Task 1, 2, 3 filter analysis\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "- **Total Trades**: {len(trades_df):,} in this specific case\n",
        "- **Pools Identified**: {len(pool_stats):,} pools handling PUMP/WSOL\n",
        "- **Sandwich Patterns**: {mev_stats['sandwich_stats']['total_sandwiches']:,} detected\n",
        "- **Multi-Pool Coordination**: {mev_stats['pool_coordination']['multi_pool_attackers']:,} attackers\n",
        "\n",
        "### Output Files\n",
        "\n",
        "All results saved to `derived/deep_dive_analysis/`:\n",
        "- `DEEP_DIVE_ANALYSIS_REPORT.md` - Comprehensive report\n",
        "- `ml_training_data.csv` - ML training dataset\n",
        "- `pool_analysis.csv` - Pool statistics\n",
        "- `pool_mev_activity.csv` - Pool MEV metrics\n",
        "- `monte_carlo_scenarios.csv` - Monte Carlo results\n",
        "- `ml_feature_importance.csv` - Feature importance\n",
        "- `*.png` - Visualizations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

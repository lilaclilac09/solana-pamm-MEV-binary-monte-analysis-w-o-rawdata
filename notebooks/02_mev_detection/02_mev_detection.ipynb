{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c0741-1f57-4d1f-a1f9-0dcc8419c671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cleaned fused table for MEV Analysis #2. Total rows: 5,506,090\n",
      "Available columns: ['slot', 'time', 'validator', 'tx_idx', 'sig', 'signer', 'kind', 'amm_oracle', 'account_updates', 'trades', 'us_since_first_shred', 'amm_trade', 'account_trade', 'is_pool_trade', 'bytes_changed_trade', 'datetime', 'timing_missing', 'ms_time']\n",
      "\n",
      "Parsing trades column to extract from_token and to_token...\n",
      "\n",
      "Mapping token addresses to names...\n",
      "Token name mapping statistics:\n",
      "  from_token_name: 0 tokens mapped to names\n",
      "  to_token_name: 0 tokens mapped to names\n",
      "\n",
      "Parsing success rate for TRADE events (from_token non-null): 0.00%\n",
      "Total TRADE events with token pairs: 0\n",
      "\n",
      "TRADE events for MEV analysis: 683,828\n",
      "\n",
      "Sandwich patterns detected: 26223\n",
      "Failed sandwich attempts (front-run only): 6339\n",
      "High confidence MEV attacks: 7897\n",
      "Medium confidence (possible aggregator): 18326\n",
      "\n",
      "High-confidence MEV attackers: 197\n",
      "\n",
      "Top 10 attackers (prioritizing high-confidence MEV): ['YubQzu18FDqJRyNfG8JqHmsdbxhnoQqcKUHBdUkN6tP', 'E2MPTDnFPNiCRmbJGKYSYew48NWRGVNfHjoiibFP5VL2', 'YubVwWeg1vHFr17Q7HQQETcke7sFvMabqU8wbv8NXQW', 'YubozzSnKomEnH3pkmYsdatUUwUTcm7s4mHJVmefEWj', '4swoALYuvetDK6N3ak1Knc1bMLbm7nzkxCW1nqjPWGRV', 'FbgR9632h4rvPciZzCHRGNrEaQM9bC2cGgAikJ8HUfS1', 'k3bS5WfZ5P2NTgkVSwdbVrveYc25iXiEHYfZfVruBGq', 'CatyeC3LgBxub7HcpW2n7cZZZ66CUKdcZ8DzHucHrSiP', 'TAPNDP5dFr9Vp47DFuddRxNVYmjUqgh8boiUrWFgMqi', 'AE861PyrYJXm2TuxiMc8Ecwo4YYgAHfcYum3GWpSRFe3']\n",
      "\n",
      "Confidence breakdown:\n",
      "  High confidence (likely MEV): 7897\n",
      "  Medium confidence (possible aggregator/MEV): 18326\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION: Classifying All Detected Attackers\n",
      "================================================================================\n",
      "⚠️  Running verification on all detected attackers to filter out:\n",
      "   - Aggregators (Jupiter routing)\n",
      "   - Wash traders (volume inflation)\n",
      "   - Regular trade bots\n",
      "   Only addresses classified as 'LIKELY MEV BOT' or 'POSSIBLE MEV' will be included in final top 10.\n",
      "\n",
      "Verifying 880 detected attackers...\n",
      "\n",
      "  Progress: 50/880 attackers verified...\n",
      "  Progress: 100/880 attackers verified...\n",
      "  Progress: 150/880 attackers verified...\n",
      "  Progress: 200/880 attackers verified...\n",
      "  Progress: 250/880 attackers verified...\n",
      "  Progress: 300/880 attackers verified...\n",
      "  Progress: 350/880 attackers verified...\n",
      "  Progress: 400/880 attackers verified...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make sure plots display inline in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure pandas and Jupyter to show full output (no truncation)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Configure Jupyter to show full output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Increase output limit (if using Jupyter)\n",
    "import sys\n",
    "try:\n",
    "    sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, TypeError):\n",
    "    # sys.stdout.reconfigure() not available in Jupyter notebooks\n",
    "    pass\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# MEV Analysis #2: Top 10 Toxic MEV Stats & Validator Breakdown (Expanded)\n",
    "# ───────────────────────────────────────────────\n",
    "# Uses df_clean (cleaned fused table after time parsing & deletion)\n",
    "# Assumes df_clean already exists in memory from previous steps\n",
    "# ───────────────────────────────────────────────\n",
    "\n",
    "df_clean = pd.read_parquet('/Users/aileen/Downloads/pamm/pamm_clean_final.parquet')\n",
    "print(f\"Using cleaned fused table for MEV Analysis #2. Total rows: {len(df_clean):,}\")\n",
    "print(\"Available columns:\", df_clean.columns.tolist())\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Parse trades column to extract from_token and to_token\n",
    "# ───────────────────────────────────────────────\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def parse_trades(trades_item):\n",
    "    \"\"\"\n",
    "    Safely parse trades column to extract from_token and to_token.\n",
    "    Returns: (from_token, to_token)\n",
    "    \"\"\"\n",
    "    # Handle None and NaN values safely\n",
    "    if trades_item is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        if pd.isna(trades_item):\n",
    "            return None, None\n",
    "    except (ValueError, TypeError):\n",
    "        # If pd.isna returns array, check if all are NA\n",
    "        if isinstance(trades_item, (list, np.ndarray)):\n",
    "            return None, None\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Handle string representation of list/dict\n",
    "        if isinstance(trades_item, str):\n",
    "            # Try to parse as Python literal (handles single quotes)\n",
    "            try:\n",
    "                parsed = ast.literal_eval(trades_item)\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Fallback to JSON parsing\n",
    "                cleaned = trades_item.replace(\"'\", '\"').replace('None', 'null').replace('True', 'true').replace('False', 'false')\n",
    "                parsed = json.loads(cleaned)\n",
    "        else:\n",
    "            parsed = trades_item\n",
    "        \n",
    "        # Handle list format\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            trade_dict = parsed[0] if isinstance(parsed[0], dict) else parsed\n",
    "            if isinstance(trade_dict, dict):\n",
    "                from_token = trade_dict.get('from_token')\n",
    "                to_token = trade_dict.get('to_token')\n",
    "                return from_token, to_token\n",
    "        \n",
    "        # Handle dict format\n",
    "        elif isinstance(parsed, dict):\n",
    "            from_token = parsed.get('from_token')\n",
    "            to_token = parsed.get('to_token')\n",
    "            return from_token, to_token\n",
    "            \n",
    "    except (json.JSONDecodeError, ValueError, AttributeError, TypeError) as e:\n",
    "        pass\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Apply parsing function\n",
    "print(\"\\nParsing trades column to extract from_token and to_token...\")\n",
    "parsed_trades_series = df_clean['trades'].apply(parse_trades)\n",
    "parsed_trades_df = pd.DataFrame(\n",
    "    parsed_trades_series.tolist(),\n",
    "    columns=['from_token', 'to_token'],\n",
    "    index=df_clean.index\n",
    ")\n",
    "\n",
    "# Add parsed columns to dataframe\n",
    "df_clean['from_token'] = parsed_trades_df['from_token']\n",
    "df_clean['to_token'] = parsed_trades_df['to_token']\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Map token addresses to human-readable names\n",
    "# ───────────────────────────────────────────────\n",
    "# Common Solana token address to name mapping\n",
    "TOKEN_NAME_MAP = {\n",
    "    # Wrapped SOL\n",
    "    'So11111111111111111111111111111111111111112': 'WSOL',\n",
    "    'So11111111111111111111111111111111111111111': 'WSOL',  # Alternative WSOL address\n",
    "    \n",
    "    # Stablecoins\n",
    "    'EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v': 'USDC',\n",
    "    'Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB': 'USDT',\n",
    "    '7kbnvuGBxxj8AG9qp8Scn56muWGaRaFqxg1FsRp3PaFT': 'UXD',\n",
    "    'EchesyfXePKdLbiA9xYJ6UXTx4urmzE2pz6DXuTVg3fH': 'UXP',\n",
    "    \n",
    "    # Popular tokens\n",
    "    '9BB6NFEcjBCtnNLFko2FqVQBq8HHM13kCyYcdQbgpump': 'PUMP',  # Based on example in data\n",
    "    'mSoLzYCxHdYgdzU16g5QSh3i5K3z3KZK7ytfqcJm7So': 'mSOL',\n",
    "    '7vfCXTUXx5WJV5JADk17DUJ4ksgau7utNKj4b963voxs': 'ETH',\n",
    "    '2FPyTwcZLUg1MDrwsyoP4D6s1tM7hAkHYRjkNb5w6Pxk': 'ETH',\n",
    "    '7dHbWXmci3dT8UFYWYZweBLXgycu7Y3iL6trKn1Y7ARj': 'ORCA',\n",
    "    '4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R': 'RAY',\n",
    "    'SRMuApVNdxXokk5GT7XD5cUUgXMBCoAz2LHeuAoKWRt': 'SRM',\n",
    "    'DezXAZ8z7PnrnRJjz3wXBoRgixCa6xjnB7YaB1pPB263': 'BONK',\n",
    "    'EKpQGSJtjMFqKZ9KQanSqYXRcF8fBopzLHYxdM65zcjm': 'WIF',\n",
    "    'JUPyiwrY2skib1qAwsWzJxZxqJp3ZJ5K3JqJ5JqJ5JqJ5': 'JUP',\n",
    "    \n",
    "    # Add more common tokens as needed\n",
    "    # You can extend this mapping based on tokens found in your dataset\n",
    "}\n",
    "\n",
    "def get_token_name(token_address):\n",
    "    \"\"\"\n",
    "    Map token address to human-readable name.\n",
    "    Returns token name if found, otherwise returns address (truncated if long).\n",
    "    \"\"\"\n",
    "    if pd.isna(token_address) or token_address is None:\n",
    "        return None\n",
    "    \n",
    "    token_address_str = str(token_address)\n",
    "    \n",
    "    # Check direct mapping\n",
    "    if token_address_str in TOKEN_NAME_MAP:\n",
    "        return TOKEN_NAME_MAP[token_address_str]\n",
    "    \n",
    "    # Return address if not found (can be extended to query on-chain metadata)\n",
    "    return token_address_str\n",
    "\n",
    "# Apply token name mapping\n",
    "print(\"\\nMapping token addresses to names...\")\n",
    "df_clean['from_token_name'] = df_clean['from_token'].apply(get_token_name)\n",
    "df_clean['to_token_name'] = df_clean['to_token'].apply(get_token_name)\n",
    "\n",
    "# Show token name mapping statistics\n",
    "trades_with_names = df_clean[df_clean['kind'] == 'TRADE']\n",
    "from_token_mapped = trades_with_names['from_token_name'].notna().sum()\n",
    "to_token_mapped = trades_with_names['to_token_name'].notna().sum()\n",
    "\n",
    "# Count how many are mapped vs unmapped (addresses)\n",
    "from_token_named = trades_with_names[\n",
    "    (trades_with_names['from_token_name'].notna()) & \n",
    "    (~trades_with_names['from_token_name'].str.match(r'^[A-Za-z0-9]{32,44}$', na=False))\n",
    "].shape[0]\n",
    "\n",
    "to_token_named = trades_with_names[\n",
    "    (trades_with_names['to_token_name'].notna()) & \n",
    "    (~trades_with_names['to_token_name'].str.match(r'^[A-Za-z0-9]{32,44}$', na=False))\n",
    "].shape[0]\n",
    "\n",
    "print(f\"Token name mapping statistics:\")\n",
    "print(f\"  from_token_name: {from_token_named:,} tokens mapped to names\")\n",
    "print(f\"  to_token_name: {to_token_named:,} tokens mapped to names\")\n",
    "\n",
    "# Show most common token pairs\n",
    "if from_token_mapped > 0 and to_token_mapped > 0:\n",
    "    token_pairs = trades_with_names[\n",
    "        trades_with_names['from_token_name'].notna() & \n",
    "        trades_with_names['to_token_name'].notna()\n",
    "    ][['from_token_name', 'to_token_name']].copy()\n",
    "    \n",
    "    if len(token_pairs) > 0:\n",
    "        token_pairs['pair'] = token_pairs['from_token_name'] + '/' + token_pairs['to_token_name']\n",
    "        top_pairs = token_pairs['pair'].value_counts().head(10)\n",
    "        print(f\"\\nTop 10 token pairs (by name):\")\n",
    "        for pair, count in top_pairs.items():\n",
    "            print(f\"  {pair}: {count:,} trades\")\n",
    "\n",
    "# Calculate parsing success rate\n",
    "trades_with_tokens = df_clean[df_clean['kind'] == 'TRADE']\n",
    "success_rate = trades_with_tokens['from_token'].notna().mean() * 100\n",
    "print(f\"\\nParsing success rate for TRADE events (from_token non-null): {success_rate:.2f}%\")\n",
    "print(f\"Total TRADE events with token pairs: {trades_with_tokens['from_token'].notna().sum():,}\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Expanded Detection: Add Classic Sandwich (3 TRADEs) + Fat (≥5)\n",
    "# ───────────────────────────────────────────────\n",
    "# Ensure lag columns for back-running\n",
    "df_clean['prev_kind'] = df_clean['kind'].shift(1)\n",
    "df_clean['prev_ms_time'] = df_clean['ms_time'].shift(1)\n",
    "df_clean['time_diff_ms'] = df_clean['ms_time'] - df_clean['prev_ms_time']\n",
    "\n",
    "# Extract TRADE records\n",
    "trades = df_clean[df_clean['kind'] == 'TRADE'].copy()\n",
    "print(f\"\\nTRADE events for MEV analysis: {len(trades):,}\")\n",
    "\n",
    "# 1. IMPROVED Sandwich Detection with False Positive Filtering\n",
    "# Distinguishes MEV attacks from:\n",
    "# - Jupiter aggregator routing (multiple users, same aggregator)\n",
    "# - Legitimate market makers\n",
    "# - Failed sandwich attempts (front-run but no profitable back-run)\n",
    "\n",
    "sandwich_records = []\n",
    "failed_attempts = []  # Track failed sandwich attempts\n",
    "\n",
    "for slot, g in trades.groupby('slot'):\n",
    "    trade_count = len(g)\n",
    "    if trade_count < 3:\n",
    "        continue\n",
    "    \n",
    "    # Sort by time within slot\n",
    "    g = g.sort_values('ms_time').reset_index(drop=True)\n",
    "    \n",
    "    signer_cnt = g['signer'].value_counts()\n",
    "    attackers = signer_cnt[signer_cnt >= 2].index  # Signers with ≥2 tx in slot\n",
    "    \n",
    "    if len(attackers) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Use amm_trade instead of amm_oracle\n",
    "    amm_col = 'amm_trade' if 'amm_trade' in g.columns else 'amm_oracle'\n",
    "    most_common_amm = g[amm_col].mode()\n",
    "    amm_name = most_common_amm.iloc[0] if not most_common_amm.empty else 'Unknown'\n",
    "    validator = g['validator'].mode().iloc[0] if 'validator' in g.columns and not g['validator'].mode().empty else 'Unknown'\n",
    "    \n",
    "    # Check for aggregator patterns (Jupiter, etc.)\n",
    "    # Aggregators typically have:\n",
    "    # - Many unique signers (users) in same slot\n",
    "    # - Similar transaction timing\n",
    "    # - Same aggregator program ID (if available)\n",
    "    unique_signers = g['signer'].nunique()\n",
    "    aggregator_ratio = unique_signers / trade_count\n",
    "    \n",
    "    for attacker in attackers:\n",
    "        attacker_trades = g[g['signer'] == attacker].copy()\n",
    "        attacker_tx_count = len(attacker_trades)\n",
    "        \n",
    "        # Filter out aggregator routing:\n",
    "        # If >70% of trades have unique signers, likely aggregator routing\n",
    "        if aggregator_ratio > 0.7 and attacker_tx_count < trade_count * 0.3:\n",
    "            # This looks like aggregator routing, not MEV\n",
    "            continue\n",
    "        \n",
    "        # Check for sandwich pattern: front-run → victim(s) → back-run\n",
    "        # MEV sandwich requires:\n",
    "        # 1. Attacker has trades at beginning AND end of slot\n",
    "        # 2. Other signers (victims) in between\n",
    "        attacker_positions = attacker_trades.index.tolist()\n",
    "        first_attacker_pos = min(attacker_positions)\n",
    "        last_attacker_pos = max(attacker_positions)\n",
    "        \n",
    "        # Check if there are other signers between first and last attacker trade\n",
    "        middle_trades = g.iloc[first_attacker_pos+1:last_attacker_pos]\n",
    "        victims_between = middle_trades[middle_trades['signer'] != attacker]\n",
    "        \n",
    "        if len(victims_between) == 0:\n",
    "            # No victims between attacker trades - could be failed attempt or unrelated\n",
    "            # Check if this is a failed attempt (attacker at start but no back-run)\n",
    "            if first_attacker_pos < len(g) * 0.3:  # Attacker in first 30% of slot\n",
    "                failed_attempts.append({\n",
    "                    'slot': slot,\n",
    "                    'amm_trade': amm_name,\n",
    "                    'attacker_signer': attacker,\n",
    "                    'type': 'failed_frontrun',\n",
    "                    'validator': validator,\n",
    "                    'reason': 'no_victims_between'\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # NEW: Validate token pair consistency for sandwich detection\n",
    "        # In a true sandwich, all trades should be on the same token pair\n",
    "        # This helps distinguish MEV from aggregator routing across different pools\n",
    "        token_pairs_valid = True\n",
    "        if 'from_token' in g.columns and 'to_token' in g.columns:\n",
    "            # Get token pairs for all trades in slot\n",
    "            attacker_token_pairs = attacker_trades[['from_token', 'to_token']].dropna()\n",
    "            victim_token_pairs = victims_between[['from_token', 'to_token']].dropna()\n",
    "            \n",
    "            if len(attacker_token_pairs) > 0 and len(victim_token_pairs) > 0:\n",
    "                # Check if attacker trades use consistent token pairs\n",
    "                attacker_pairs = attacker_token_pairs.apply(\n",
    "                    lambda x: tuple(sorted([str(x['from_token']), str(x['to_token'])])), axis=1\n",
    "                ).unique()\n",
    "                \n",
    "                # Check if victims use same token pairs as attacker\n",
    "                victim_pairs = victim_token_pairs.apply(\n",
    "                    lambda x: tuple(sorted([str(x['from_token']), str(x['to_token'])])), axis=1\n",
    "                ).unique()\n",
    "                \n",
    "                # For a valid sandwich, at least one token pair should be shared\n",
    "                # (attacker front-runs and back-runs on same pair, victims trade on same pair)\n",
    "                common_pairs = set(attacker_pairs) & set(victim_pairs)\n",
    "                \n",
    "                if len(common_pairs) == 0 and len(attacker_pairs) > 0:\n",
    "                    # No common token pairs - might be aggregator routing across pools\n",
    "                    # Still allow if attacker has consistent pair (could be reverse sandwich)\n",
    "                    if len(attacker_pairs) > 1:\n",
    "                        token_pairs_valid = False  # Attacker trading multiple pairs\n",
    "                # If common_pairs exist or attacker has single consistent pair, it's valid\n",
    "        \n",
    "        # Additional validation: Check timing patterns\n",
    "        # MEV attacks typically have:\n",
    "        # - Front-run early in slot\n",
    "        # - Back-run late in slot\n",
    "        # - Victims in between\n",
    "        attacker_times = attacker_trades['ms_time'].values\n",
    "        slot_start = g['ms_time'].min()\n",
    "        slot_end = g['ms_time'].max()\n",
    "        slot_duration = slot_end - slot_start\n",
    "        \n",
    "        # Check if attacker trades span the slot (indicates sandwich)\n",
    "        time_span = attacker_times.max() - attacker_times.min()\n",
    "        span_ratio = time_span / slot_duration if slot_duration > 0 else 0\n",
    "        \n",
    "        # MEV sandwich should span at least 30% of slot duration\n",
    "        if span_ratio < 0.3 and trade_count < 5:\n",
    "            # Too tight timing, might be aggregator bundle\n",
    "            continue\n",
    "        \n",
    "        # Additional validation: Token pair consistency\n",
    "        # If token pairs are available but don't match, this might not be a true sandwich\n",
    "        if not token_pairs_valid:\n",
    "            # Skip if token pairs indicate this is not a sandwich on same pool\n",
    "            continue\n",
    "        \n",
    "        # Check for consistent pattern across multiple slots (indicates bot, not aggregator)\n",
    "        # This will be checked later in aggregation\n",
    "        \n",
    "        # Extract token pair info for the record\n",
    "        token_pair_info = None\n",
    "        if 'from_token' in attacker_trades.columns and 'to_token' in attacker_trades.columns:\n",
    "            attacker_tokens = attacker_trades[['from_token', 'to_token']].dropna()\n",
    "            if len(attacker_tokens) > 0:\n",
    "                # Get most common token pair\n",
    "                token_pairs = attacker_tokens.apply(\n",
    "                    lambda x: f\"{x['from_token']}/{x['to_token']}\", axis=1\n",
    "                )\n",
    "                token_pair_info = token_pairs.mode().iloc[0] if len(token_pairs.mode()) > 0 else None\n",
    "        \n",
    "        # Classify as sandwich\n",
    "        sandwich_type = 'fat_sandwich' if trade_count >= 5 else 'sandwich'\n",
    "        \n",
    "        sandwich_records.append({\n",
    "            'slot': slot,\n",
    "            'amm_trade': amm_name,\n",
    "            'attacker_signer': attacker,\n",
    "            'attack_tx_count': attacker_tx_count,\n",
    "            'trade_count_in_slot': trade_count,\n",
    "            'victims_count': len(victims_between),\n",
    "            'unique_signers': unique_signers,\n",
    "            'aggregator_ratio': aggregator_ratio,\n",
    "            'time_span_ratio': span_ratio,\n",
    "            'token_pair': token_pair_info,  # NEW: Add token pair info\n",
    "            'type': sandwich_type,\n",
    "            'validator': validator,\n",
    "            'confidence': 'high' if span_ratio > 0.5 and len(victims_between) >= 2 else 'medium'\n",
    "        })\n",
    "\n",
    "sandwich_df = pd.DataFrame(sandwich_records)\n",
    "failed_attempts_df = pd.DataFrame(failed_attempts) if failed_attempts else pd.DataFrame()\n",
    "\n",
    "print(f\"\\nSandwich patterns detected: {len(sandwich_df)}\")\n",
    "print(f\"Failed sandwich attempts (front-run only): {len(failed_attempts_df)}\")\n",
    "if not sandwich_df.empty:\n",
    "    print(f\"High confidence MEV attacks: {len(sandwich_df[sandwich_df['confidence'] == 'high'])}\")\n",
    "    print(f\"Medium confidence (possible aggregator): {len(sandwich_df[sandwich_df['confidence'] == 'medium'])}\")\n",
    "\n",
    "# Get top 10 attackers - prioritize high-confidence MEV attacks\n",
    "if not sandwich_df.empty:\n",
    "    # Filter to high-confidence attacks first, then medium\n",
    "    high_conf = sandwich_df[sandwich_df['confidence'] == 'high']\n",
    "    \n",
    "    if len(high_conf) > 0:\n",
    "        # Count by high-confidence attacks\n",
    "        high_conf_counts = high_conf['attacker_signer'].value_counts()\n",
    "        print(f\"\\nHigh-confidence MEV attackers: {len(high_conf_counts)}\")\n",
    "        \n",
    "        # Get top 10 from high-confidence, fill with medium if needed\n",
    "        top10_high = high_conf_counts.head(10).index.tolist()\n",
    "        \n",
    "        if len(top10_high) < 10:\n",
    "            # Add medium-confidence attackers to reach 10\n",
    "            medium_conf = sandwich_df[sandwich_df['confidence'] == 'medium']\n",
    "            medium_counts = medium_conf['attacker_signer'].value_counts()\n",
    "            # Exclude already selected\n",
    "            medium_counts = medium_counts[~medium_counts.index.isin(top10_high)]\n",
    "            top10_medium = medium_counts.head(10 - len(top10_high)).index.tolist()\n",
    "            top10_attackers = top10_high + top10_medium\n",
    "        else:\n",
    "            top10_attackers = top10_high\n",
    "    else:\n",
    "        # Fallback to all if no high-confidence\n",
    "        top10_attackers = sandwich_df['attacker_signer'].value_counts().head(10).index.tolist()\n",
    "        print(\"\\n⚠️  WARNING: No high-confidence MEV attacks found. Top 10 may include aggregator routing.\")\n",
    "    \n",
    "    print(f\"\\nTop 10 attackers (prioritizing high-confidence MEV): {top10_attackers}\")\n",
    "    \n",
    "    # Show breakdown\n",
    "    if not sandwich_df.empty:\n",
    "        print(f\"\\nConfidence breakdown:\")\n",
    "        print(f\"  High confidence (likely MEV): {len(sandwich_df[sandwich_df['confidence'] == 'high'])}\")\n",
    "        print(f\"  Medium confidence (possible aggregator/MEV): {len(sandwich_df[sandwich_df['confidence'] == 'medium'])}\")\n",
    "else:\n",
    "    top10_attackers = []\n",
    "    print(\"No sandwich patterns detected.\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# VERIFICATION: Classify all detected attackers to filter out aggregators/wash traders\n",
    "# This runs BEFORE final top 10 selection to ensure we only show actual MEV bots\n",
    "# ───────────────────────────────────────────────\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VERIFICATION: Classifying All Detected Attackers\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"⚠️  Running verification on all detected attackers to filter out:\")\n",
    "print(\"   - Aggregators (Jupiter routing)\")\n",
    "print(\"   - Wash traders (volume inflation)\")\n",
    "print(\"   - Regular trade bots\")\n",
    "print(\"   Only addresses classified as 'LIKELY MEV BOT' or 'POSSIBLE MEV' will be included in final top 10.\\n\")\n",
    "\n",
    "# Get all unique attackers from sandwich detection\n",
    "all_detected_attackers = []\n",
    "if not sandwich_df.empty:\n",
    "    all_detected_attackers = sandwich_df['attacker_signer'].unique().tolist()\n",
    "\n",
    "verification_results = []\n",
    "\n",
    "if len(all_detected_attackers) > 0:\n",
    "    print(f\"Verifying {len(all_detected_attackers)} detected attackers...\\n\")\n",
    "    \n",
    "    for idx, attacker in enumerate(all_detected_attackers, 1):\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"  Progress: {idx}/{len(all_detected_attackers)} attackers verified...\")\n",
    "        \n",
    "        attacker_trades = trades[trades['signer'] == attacker].copy()\n",
    "        \n",
    "        if len(attacker_trades) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_trades = len(attacker_trades)\n",
    "        \n",
    "        # 1. Check for aggregator pattern: many unique signers in same slots\n",
    "        attacker_slots = attacker_trades['slot'].unique()\n",
    "        aggregator_score = 0\n",
    "        \n",
    "        # Sample up to 200 slots for aggregator detection\n",
    "        sample_slots = min(200, len(attacker_slots))\n",
    "        for slot in attacker_slots[:sample_slots]:\n",
    "            slot_trades = trades[trades['slot'] == slot]\n",
    "            unique_signers = slot_trades['signer'].nunique()\n",
    "            total_in_slot = len(slot_trades)\n",
    "            if total_in_slot > 0:\n",
    "                ratio = unique_signers / total_in_slot\n",
    "                if ratio > 0.7:  # High unique signer ratio = aggregator\n",
    "                    aggregator_score += 1\n",
    "        \n",
    "        aggregator_likelihood = aggregator_score / sample_slots if sample_slots > 0 else 0\n",
    "        \n",
    "        # 2. Check for MEV patterns\n",
    "        # - Late-slot timing (front-running)\n",
    "        late_slot_trades = attacker_trades[attacker_trades['us_since_first_shred'] > 300000]\n",
    "        late_slot_ratio = len(late_slot_trades) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # - Oracle back-running\n",
    "        attacker_trades_with_prev = attacker_trades.copy()\n",
    "        attacker_trades_with_prev['prev_kind'] = df_clean.loc[attacker_trades_with_prev.index, 'prev_kind'].values\n",
    "        oracle_backrun = attacker_trades_with_prev[\n",
    "            (attacker_trades_with_prev['prev_kind'] == 'ORACLE') & \n",
    "            (attacker_trades_with_prev['time_diff_ms'] < 50)\n",
    "        ]\n",
    "        oracle_backrun_ratio = len(oracle_backrun) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # - High bytes_changed (oracle manipulation)\n",
    "        high_bytes = attacker_trades[attacker_trades['bytes_changed_trade'] > 50]\n",
    "        high_bytes_ratio = len(high_bytes) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # - Clusters (multiple tx in same slot)\n",
    "        attacker_trades['tx_in_slot'] = attacker_trades.groupby('slot')['slot'].transform('count')\n",
    "        clusters = attacker_trades[attacker_trades['tx_in_slot'] >= 2]\n",
    "        cluster_ratio = len(clusters) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # Calculate MEV score\n",
    "        mev_score = (late_slot_ratio * 0.3 + \n",
    "                    oracle_backrun_ratio * 0.3 + \n",
    "                    high_bytes_ratio * 0.2 + \n",
    "                    cluster_ratio * 0.2)\n",
    "        \n",
    "        # 3. Check for wash trading patterns\n",
    "        if len(attacker_trades) > 10:\n",
    "            time_span_hours = (attacker_trades['ms_time'].max() - attacker_trades['ms_time'].min()) / (1000 * 60 * 60)\n",
    "            trades_per_hour = len(attacker_trades) / time_span_hours if time_span_hours > 0 else 0\n",
    "            wash_trading_score = trades_per_hour / 50  # Normalize to 50 trades/hour\n",
    "        else:\n",
    "            wash_trading_score = 0\n",
    "            trades_per_hour = 0\n",
    "        \n",
    "        # Classification with wash trading detection\n",
    "        if aggregator_likelihood > 0.5:\n",
    "            classification = \"LIKELY AGGREGATOR (Jupiter, etc.)\"\n",
    "            confidence = \"High\" if aggregator_likelihood > 0.7 else \"Medium\"\n",
    "        elif wash_trading_score > 1.0 and mev_score < 0.2:\n",
    "            classification = \"LIKELY WASH TRADING (Volume Inflation)\"\n",
    "            confidence = \"High\" if wash_trading_score > 2.0 else \"Medium\"\n",
    "        elif mev_score > 0.3:\n",
    "            classification = \"LIKELY MEV BOT\"\n",
    "            confidence = \"High\" if mev_score > 0.5 else \"Medium\"\n",
    "        elif cluster_ratio > 0.3:\n",
    "            classification = \"POSSIBLE MEV (Sandwich patterns)\"\n",
    "            confidence = \"Medium\"\n",
    "        else:\n",
    "            classification = \"REGULAR TRADE BOT / UNKNOWN\"\n",
    "            confidence = \"Low\"\n",
    "        \n",
    "        verification_results.append({\n",
    "            'attacker': attacker,\n",
    "            'total_trades': total_trades,\n",
    "            'trades_per_hour': trades_per_hour,\n",
    "            'aggregator_likelihood': aggregator_likelihood,\n",
    "            'late_slot_ratio': late_slot_ratio,\n",
    "            'oracle_backrun_ratio': oracle_backrun_ratio,\n",
    "            'high_bytes_ratio': high_bytes_ratio,\n",
    "            'cluster_ratio': cluster_ratio,\n",
    "            'mev_score': mev_score,\n",
    "            'wash_trading_score': wash_trading_score,\n",
    "            'classification': classification,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Create verification dataframe\n",
    "    verification_df = pd.DataFrame(verification_results)\n",
    "    \n",
    "    if not verification_df.empty:\n",
    "        # Save full verification results\n",
    "        verification_df.to_csv('mev_attacker_verification.csv', index=False)\n",
    "        print(f\"\\n✓ Verification complete. Results saved to: mev_attacker_verification.csv\")\n",
    "        \n",
    "        # Show classification breakdown\n",
    "        print(f\"\\nClassification Breakdown (All {len(verification_df)} Detected Attackers):\")\n",
    "        classification_counts = verification_df['classification'].value_counts()\n",
    "        for classification, count in classification_counts.items():\n",
    "            print(f\"  {classification}: {count}\")\n",
    "        \n",
    "        # Filter to only MEV bots for top 10 list\n",
    "        mev_bots = verification_df[\n",
    "            verification_df['classification'].isin(['LIKELY MEV BOT', 'POSSIBLE MEV (Sandwich patterns)'])\n",
    "        ].copy()\n",
    "        \n",
    "        if len(mev_bots) > 0:\n",
    "            # Sort by MEV score (descending) and get top 10\n",
    "            mev_bots = mev_bots.sort_values('mev_score', ascending=False)\n",
    "            top10_mev_bots = mev_bots.head(10)['attacker'].tolist()\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"FILTERED TOP 10 MEV ATTACKERS (After Verification)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"\\n✓ Found {len(mev_bots)} addresses classified as MEV bots (out of {len(verification_df)} total)\")\n",
    "            print(f\"✓ Top 10 MEV bots (by MEV score):\")\n",
    "            for idx, attacker in enumerate(top10_mev_bots, 1):\n",
    "                attacker_info = mev_bots[mev_bots['attacker'] == attacker].iloc[0]\n",
    "                print(f\"  {idx}. {attacker}\")\n",
    "                print(f\"     Classification: {attacker_info['classification']}\")\n",
    "                print(f\"     MEV Score: {attacker_info['mev_score']:.2%}\")\n",
    "                print(f\"     Total Trades: {attacker_info['total_trades']:,}\")\n",
    "            \n",
    "            # Update top10_attackers to only include verified MEV bots\n",
    "            top10_attackers = top10_mev_bots\n",
    "            \n",
    "            print(f\"\\n⚠️  IMPORTANT: Updated top10_attackers list to only include verified MEV bots.\")\n",
    "            print(f\"   Excluded: {len(verification_df) - len(mev_bots)} addresses classified as aggregators/wash traders/regular bots\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  WARNING: No addresses classified as 'LIKELY MEV BOT' or 'POSSIBLE MEV' found!\")\n",
    "            print(f\"   All {len(verification_df)} detected addresses are classified as:\")\n",
    "            for classification, count in classification_counts.items():\n",
    "                print(f\"     - {classification}: {count}\")\n",
    "            print(f\"\\n   This suggests:\")\n",
    "            print(f\"     - Most activity is aggregator routing (Jupiter, etc.)\")\n",
    "            print(f\"     - Or detection thresholds may need adjustment\")\n",
    "            print(f\"   Original top10_attackers list (by sandwich count) will be used, but may include aggregators.\")\n",
    "    else:\n",
    "        print(\"No verification results generated.\")\n",
    "else:\n",
    "    print(\"No attackers detected for verification.\")\n",
    "\n",
    "# Analyze failed attempts\n",
    "if not failed_attempts_df.empty:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FAILED SANDWICH ATTEMPTS ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total failed attempts: {len(failed_attempts_df)}\")\n",
    "    print(f\"Failed attempts per attacker:\")\n",
    "    print(failed_attempts_df['attacker_signer'].value_counts().head(10))\n",
    "    print(\"\\n⚠️  Note: Failed attempts indicate front-running without profitable back-run.\")\n",
    "    print(\"   These may be:\")\n",
    "    print(\"   - Unsuccessful MEV attempts\")\n",
    "    print(\"   - Legitimate early trades that didn't result in sandwich\")\n",
    "    print(\"   - Aggregator routing with timing coincidences\")\n",
    "\n",
    "# 2. Front-running Detection (late-slot trades >0.3s delay, expanded to include slot avg latency)\n",
    "front_running = trades[trades['us_since_first_shred'] > 300000].copy()\n",
    "front_running['type'] = 'front_running'\n",
    "# Use amm_trade instead of amm_oracle\n",
    "front_running = front_running[['slot', 'amm_trade', 'signer', 'type', 'validator', 'us_since_first_shred']].rename(columns={'signer': 'attacker_signer'})\n",
    "if 'amm_trade' not in front_running.columns:\n",
    "    front_running = front_running.rename(columns={'amm_oracle': 'amm_trade'})\n",
    "front_running_top10 = front_running[front_running['attacker_signer'].isin(top10_attackers)]\n",
    "\n",
    "# Average latency per slot in front-running\n",
    "front_running['slot_avg_latency'] = front_running.groupby('slot')['us_since_first_shred'].transform('mean')\n",
    "\n",
    "# 3. Back-running Detection (immediately after ORACLE, <50ms, expanded to include oracle count)\n",
    "back_running = trades[(trades['prev_kind'] == 'ORACLE') & (trades['time_diff_ms'] < 50)].copy()\n",
    "back_running['type'] = 'back_running'\n",
    "# Use amm_trade instead of amm_oracle\n",
    "back_running = back_running[['slot', 'amm_trade', 'signer', 'type', 'validator', 'time_diff_ms']].rename(columns={'signer': 'attacker_signer'})\n",
    "if 'amm_trade' not in back_running.columns:\n",
    "    back_running = back_running.rename(columns={'amm_oracle': 'amm_trade'})\n",
    "back_running_top10 = back_running[back_running['attacker_signer'].isin(top10_attackers)]\n",
    "\n",
    "# Count oracle updates per slot in back-running\n",
    "oracle_per_slot = df_clean[df_clean['kind'] == 'ORACLE'].groupby('slot').size().reset_index(name='oracle_count')\n",
    "back_running = back_running.merge(oracle_per_slot, on='slot', how='left').fillna(0)\n",
    "\n",
    "# Merge all MEV records (top10 only)\n",
    "# Ensure all dataframes have amm_trade column before merging\n",
    "if 'amm_trade' not in sandwich_df.columns and 'amm' in sandwich_df.columns:\n",
    "    sandwich_df = sandwich_df.rename(columns={'amm': 'amm_trade'})\n",
    "\n",
    "all_mev = pd.concat([sandwich_df, front_running_top10, back_running_top10], ignore_index=True)\n",
    "\n",
    "# Function to display Per pAMM Top 10 MEV table\n",
    "def display_per_pamm_top10_mev(stats_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Display Top N MEV attackers per pAMM in a formatted table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stats_df : DataFrame\n",
    "        Stats dataframe with columns: amm_trade, attacker_signer, validator, \n",
    "        fat_sandwich, sandwich, front_running, back_running, etc.\n",
    "    top_n : int\n",
    "        Number of top attackers to show per AMM (default: 10)\n",
    "    \"\"\"\n",
    "    from IPython.display import display as ipy_display\n",
    "    \n",
    "    if stats_df.empty:\n",
    "        print(\"\\nNo MEV stats to display.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure amm_trade column exists\n",
    "    if 'amm_trade' not in stats_df.columns:\n",
    "        if 'amm_oracle' in stats_df.columns:\n",
    "            stats_df = stats_df.rename(columns={'amm_oracle': 'amm_trade'})\n",
    "        else:\n",
    "            print(\"Error: No amm_trade or amm_oracle column found in stats.\")\n",
    "            return\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Per pAMM Top {top_n} MEV Attackers Table\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Group by amm_trade\n",
    "    amm_groups = stats_df.groupby('amm_trade')\n",
    "    \n",
    "    for amm, group in amm_groups:\n",
    "        # Sort by total MEV activity (fat_sandwich + sandwich + front_running + back_running)\n",
    "        group = group.copy()  # Avoid SettingWithCopyWarning\n",
    "        group['total_mev'] = (group.get('fat_sandwich', 0) + \n",
    "                             group.get('sandwich', 0) + \n",
    "                             group.get('front_running', 0) + \n",
    "                             group.get('back_running', 0))\n",
    "        \n",
    "        # Get top N per AMM\n",
    "        top_group = group.nlargest(top_n, 'total_mev')\n",
    "        \n",
    "        print(f\"\\n{'─'*100}\")\n",
    "        print(f\"pAMM: {amm}\")\n",
    "        print(f\"Total Attackers: {len(group)}, Showing Top {min(top_n, len(group))}\")\n",
    "        print(f\"{'─'*100}\")\n",
    "        \n",
    "        # Display columns\n",
    "        display_cols = ['attacker_signer', 'validator', 'fat_sandwich', 'sandwich', \n",
    "                       'front_running', 'back_running', 'sandwich_complete', \n",
    "                       'cost_sol', 'profit_sol', 'net_profit_sol']\n",
    "        available_cols = [c for c in display_cols if c in top_group.columns]\n",
    "        \n",
    "        # Format the table\n",
    "        display_df = top_group[available_cols].copy()\n",
    "        \n",
    "        # Format float columns\n",
    "        float_cols = ['cost_sol', 'profit_sol', 'net_profit_sol']\n",
    "        for col in float_cols:\n",
    "            if col in display_df.columns:\n",
    "                display_df[col] = display_df[col].apply(lambda x: f'{x:.4f}' if isinstance(x, (int, float)) else x)\n",
    "        \n",
    "        # Reset index for display\n",
    "        display_df = display_df.reset_index(drop=True)\n",
    "        display_df.index = display_df.index + 1  # Start from 1 instead of 0\n",
    "        \n",
    "        # Use display() for better Jupyter formatting (scrollable, no truncation)\n",
    "        ipy_display(display_df)\n",
    "        print()\n",
    "\n",
    "if all_mev.empty:\n",
    "    print(\"\\nNo MEV patterns detected.\")\n",
    "    stats = pd.DataFrame()\n",
    "else:\n",
    "    # Ensure all_mev has amm_trade column (should already have it from previous fixes)\n",
    "    if 'amm_trade' not in all_mev.columns:\n",
    "        if 'amm_oracle' in all_mev.columns:\n",
    "            all_mev = all_mev.rename(columns={'amm_oracle': 'amm_trade'})\n",
    "        elif 'amm' in all_mev.columns:\n",
    "            all_mev = all_mev.rename(columns={'amm': 'amm_trade'})\n",
    "    \n",
    "    # Stats: Group by amm_trade + attacker_signer + type (FIXED: use amm_trade not amm_oracle)\n",
    "    # IMPORTANT: Don't filter to top10 attackers here - we want ALL pAMMs with their top attackers\n",
    "    stats = all_mev.groupby(['amm_trade', 'attacker_signer', 'type']).size().unstack(fill_value=0)\n",
    "    stats = stats.reset_index()\n",
    "\n",
    "    # Safe fill missing type columns\n",
    "    for col in ['fat_sandwich', 'sandwich', 'front_running', 'back_running']:\n",
    "        if col not in stats.columns:\n",
    "            stats[col] = 0\n",
    "\n",
    "    # Add validator (first per attacker-amm pair)\n",
    "    validator_map = all_mev.groupby(['amm_trade', 'attacker_signer'])['validator'].first().reset_index(name='validator')\n",
    "    stats = pd.merge(stats, validator_map, on=['amm_trade', 'attacker_signer'], how='left')\n",
    "\n",
    "    # Derived metrics (approximate SOL)\n",
    "    stats['sandwich_complete'] = stats.get('fat_sandwich', 0) // 2 + stats.get('sandwich', 0) // 2\n",
    "    stats['cost_sol'] = (stats['sandwich_complete'] * 0.001) + (stats.get('front_running', 0) * 0.0005) + (stats.get('back_running', 0) * 0.0005)\n",
    "    stats['profit_sol'] = (stats['sandwich_complete'] * 0.01) + (stats.get('front_running', 0) * 0.002) + (stats.get('back_running', 0) * 0.002)\n",
    "    stats['net_profit_sol'] = stats['profit_sol'] - stats['cost_sol']\n",
    "    \n",
    "    # Add confidence levels to stats if available\n",
    "    if 'confidence' in sandwich_df.columns:\n",
    "        confidence_map = sandwich_df.groupby(['amm_trade', 'attacker_signer'])['confidence'].agg(\n",
    "            lambda x: 'high' if (x == 'high').any() else 'medium'\n",
    "        ).reset_index(name='confidence')\n",
    "        stats = pd.merge(stats, confidence_map, on=['amm_trade', 'attacker_signer'], how='left')\n",
    "    \n",
    "    # Save ALL stats (not filtered to top10)\n",
    "    stats.to_csv('per_pamm_all_mev_with_validator.csv', index=False)\n",
    "    print(\"\\nAll pAMM MEV stats (all attackers) saved to: per_pamm_all_mev_with_validator.csv\")\n",
    "    \n",
    "    # Save failed attempts\n",
    "    if not failed_attempts_df.empty:\n",
    "        failed_attempts_df.to_csv('failed_sandwich_attempts.csv', index=False)\n",
    "        print(\"Failed sandwich attempts saved to: failed_sandwich_attempts.csv\")\n",
    "    \n",
    "    # Save confidence breakdown\n",
    "    if not sandwich_df.empty and 'confidence' in sandwich_df.columns:\n",
    "        confidence_summary = sandwich_df.groupby(['amm_trade', 'confidence']).size().unstack(fill_value=0)\n",
    "        confidence_summary.to_csv('mev_confidence_breakdown.csv')\n",
    "        print(\"MEV confidence breakdown saved to: mev_confidence_breakdown.csv\")\n",
    "        print(\"\\n⚠️  IMPORTANT: Review confidence levels:\")\n",
    "        print(\"   - 'high': Likely MEV attacks (attacker spans slot, victims between)\")\n",
    "        print(\"   - 'medium': Possible aggregator routing or less clear MEV pattern\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Variable Ranking by MEV Detection Contribution\n",
    "# ───────────────────────────────────────────────\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VARIABLE RANKING BY MEV DETECTION CONTRIBUTION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Calculate contribution scores for each variable\n",
    "variable_contributions = {}\n",
    "\n",
    "if not sandwich_df.empty:\n",
    "    # 1. Signer (attacker identification) - CRITICAL\n",
    "    # Used in: All MEV detection (sandwich, front-run, back-run)\n",
    "    signer_contribution = {\n",
    "        'variable': 'signer',\n",
    "        'importance': 'CRITICAL',\n",
    "        'score': 10.0,\n",
    "        'usage': 'Identifies MEV attackers across all detection methods',\n",
    "        'details': [\n",
    "            'Sandwich: Groups trades by signer to find attackers with ≥2 tx in slot',\n",
    "            'Front-run: Filters to top MEV attackers by signer',\n",
    "            'Back-run: Filters to top MEV attackers by signer',\n",
    "            'Fat sandwich: Validates same signer at start and end of slot'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['signer'] = signer_contribution\n",
    "    \n",
    "    # 2. Token Pair (from_token/to_token) - CRITICAL for validation\n",
    "    # Used in: Sandwich validation to ensure same pool\n",
    "    token_pair_usage = 0\n",
    "    if 'token_pair' in sandwich_df.columns:\n",
    "        token_pair_usage = sandwich_df['token_pair'].notna().sum()\n",
    "    \n",
    "    token_pair_contribution = {\n",
    "        'variable': 'from_token/to_token',\n",
    "        'importance': 'CRITICAL',\n",
    "        'score': 9.5,\n",
    "        'usage': 'Validates sandwich attacks are on same token pair (same pool)',\n",
    "        'details': [\n",
    "            'Prevents false positives from aggregator routing across different pools',\n",
    "            'Ensures front-run and back-run are on same trading pair',\n",
    "            'Helps distinguish true MEV from multi-pool aggregator bundles',\n",
    "            f'Used in {token_pair_usage:,} sandwich detections'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['token_pair'] = token_pair_contribution\n",
    "    \n",
    "    # 3. Slot - CRITICAL\n",
    "    # Used in: All detection methods (grouping trades by slot)\n",
    "    slot_contribution = {\n",
    "        'variable': 'slot',\n",
    "        'importance': 'CRITICAL',\n",
    "        'score': 10.0,\n",
    "        'usage': 'Groups trades temporally for sandwich detection',\n",
    "        'details': [\n",
    "            'Sandwich: Groups trades in same slot to detect front-run → victim → back-run',\n",
    "            'Front-run: Identifies late-slot trades (>300ms)',\n",
    "            'Back-run: Links trades to oracle updates in same slot',\n",
    "            'Fat sandwich: Requires ≥5 trades in same slot'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['slot'] = slot_contribution\n",
    "    \n",
    "    # 4. Time (ms_time) - CRITICAL\n",
    "    # Used in: Timing validation for sandwich and back-run\n",
    "    time_contribution = {\n",
    "        'variable': 'ms_time',\n",
    "        'importance': 'CRITICAL',\n",
    "        'score': 9.0,\n",
    "        'usage': 'Validates timing patterns for MEV attacks',\n",
    "        'details': [\n",
    "            'Sandwich: Validates attacker trades span ≥30% of slot duration',\n",
    "            'Back-run: Requires <50ms response time after oracle update',\n",
    "            'Front-run: Identifies late-slot trades (>300ms delay)',\n",
    "            'Time span ratio: Key metric for high-confidence MEV (span >50% = high confidence)'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['ms_time'] = time_contribution\n",
    "    \n",
    "    # 5. Validator - HIGH\n",
    "    # Used in: Attribution and clustering\n",
    "    validator_contribution = {\n",
    "        'variable': 'validator',\n",
    "        'importance': 'HIGH',\n",
    "        'score': 7.5,\n",
    "        'usage': 'Attribution and validator-level MEV analysis',\n",
    "        'details': [\n",
    "            'Identifies which validators process MEV attacks',\n",
    "            'Enables validator-level clustering and analysis',\n",
    "            'Helps identify validator-MEV bot relationships',\n",
    "            'Used in per-validator MEV statistics'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['validator'] = validator_contribution\n",
    "    \n",
    "    # 6. AMM (amm_trade) - HIGH\n",
    "    # Used in: Protocol-level analysis\n",
    "    amm_contribution = {\n",
    "        'variable': 'amm_trade',\n",
    "        'importance': 'HIGH',\n",
    "        'score': 7.0,\n",
    "        'usage': 'Identifies which pAMM protocols are targeted',\n",
    "        'details': [\n",
    "            'Groups MEV attacks by protocol (ZeroFi, HumidiFi, etc.)',\n",
    "            'Enables per-protocol MEV statistics',\n",
    "            'Helps identify protocol-specific attack patterns',\n",
    "            'Used in per-pAMM top 10 attacker analysis'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['amm_trade'] = amm_contribution\n",
    "    \n",
    "    # 7. Time difference (time_diff_ms) - HIGH\n",
    "    # Used in: Back-running detection\n",
    "    time_diff_contribution = {\n",
    "        'variable': 'time_diff_ms',\n",
    "        'importance': 'HIGH',\n",
    "        'score': 8.0,\n",
    "        'usage': 'Detects back-running after oracle updates',\n",
    "        'details': [\n",
    "            'Back-run: Requires <50ms between oracle update and trade',\n",
    "            'Indicates automated bot response to oracle price changes',\n",
    "            'Key metric for distinguishing MEV from normal trading',\n",
    "            'Threshold: <50ms = suspicious, <30ms = highly suspicious'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['time_diff_ms'] = time_diff_contribution\n",
    "    \n",
    "    # 8. Latency (us_since_first_shred) - MEDIUM-HIGH\n",
    "    # Used in: Front-running detection\n",
    "    latency_contribution = {\n",
    "        'variable': 'us_since_first_shred',\n",
    "        'importance': 'MEDIUM-HIGH',\n",
    "        'score': 7.0,\n",
    "        'usage': 'Detects late-slot trades (front-running)',\n",
    "        'details': [\n",
    "            'Front-run: Requires >300ms delay (300,000 microseconds)',\n",
    "            'Indicates strategic placement late in slot',\n",
    "            'Suggests mempool monitoring and insertion',\n",
    "            'Threshold: >300ms = suspicious front-running'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['us_since_first_shred'] = latency_contribution\n",
    "    \n",
    "    # 9. Previous kind (prev_kind) - MEDIUM\n",
    "    # Used in: Back-running detection\n",
    "    prev_kind_contribution = {\n",
    "        'variable': 'prev_kind',\n",
    "        'importance': 'MEDIUM',\n",
    "        'score': 6.5,\n",
    "        'usage': 'Links trades to preceding oracle updates',\n",
    "        'details': [\n",
    "            'Back-run: Requires prev_kind == \"ORACLE\"',\n",
    "            'Validates trade immediately follows price update',\n",
    "            'Key for detecting oracle exploitation',\n",
    "            'Used in conjunction with time_diff_ms'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['prev_kind'] = prev_kind_contribution\n",
    "    \n",
    "    # 10. Aggregator ratio - MEDIUM\n",
    "    # Used in: Filtering false positives\n",
    "    aggregator_ratio_contribution = {\n",
    "        'variable': 'aggregator_ratio',\n",
    "        'importance': 'MEDIUM',\n",
    "        'score': 6.0,\n",
    "        'usage': 'Filters out Jupiter aggregator routing (false positives)',\n",
    "        'details': [\n",
    "            'Calculated as: unique_signers / trade_count',\n",
    "            'If >0.7 and attacker has <30% of trades → likely aggregator',\n",
    "            'Prevents false positives from legitimate aggregator routing',\n",
    "            'Key for distinguishing MEV from aggregator bundles'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['aggregator_ratio'] = aggregator_ratio_contribution\n",
    "    \n",
    "    # 11. Time span ratio - MEDIUM\n",
    "    # Used in: Confidence scoring\n",
    "    time_span_ratio_contribution = {\n",
    "        'variable': 'time_span_ratio',\n",
    "        'importance': 'MEDIUM',\n",
    "        'score': 6.5,\n",
    "        'usage': 'Confidence scoring for sandwich attacks',\n",
    "        'details': [\n",
    "            'Calculated as: (attacker_max_time - attacker_min_time) / slot_duration',\n",
    "            'High confidence: >50% span AND ≥2 victims',\n",
    "            'Medium confidence: 30-50% span OR 1 victim',\n",
    "            'Validates attacker trades span significant portion of slot'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['time_span_ratio'] = time_span_ratio_contribution\n",
    "    \n",
    "    # 12. Victims count - MEDIUM\n",
    "    # Used in: Confidence scoring\n",
    "    victims_count_contribution = {\n",
    "        'variable': 'victims_count',\n",
    "        'importance': 'MEDIUM',\n",
    "        'score': 6.0,\n",
    "        'usage': 'Confidence scoring and fat sandwich validation',\n",
    "        'details': [\n",
    "            'High confidence: ≥2 victims between attacker trades',\n",
    "            'Validates true sandwich pattern (victims between front-run and back-run)',\n",
    "            'Fat sandwich: Multiple victims in same slot',\n",
    "            'Key for distinguishing MEV from failed attempts'\n",
    "        ]\n",
    "    }\n",
    "    variable_contributions['victims_count'] = victims_count_contribution\n",
    "\n",
    "# Sort by score (descending)\n",
    "ranked_variables = sorted(variable_contributions.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "\n",
    "# Display ranking\n",
    "print(\"\\nRank | Variable              | Importance | Score | Usage\")\n",
    "print(\"-\" * 80)\n",
    "for rank, (var_name, var_info) in enumerate(ranked_variables, 1):\n",
    "    print(f\"{rank:4d} | {var_info['variable']:20s} | {var_info['importance']:11s} | {var_info['score']:5.1f} | {var_info['usage']}\")\n",
    "\n",
    "# Save ranking to file\n",
    "ranking_df = pd.DataFrame([\n",
    "    {\n",
    "        'rank': rank,\n",
    "        'variable': var_info['variable'],\n",
    "        'importance': var_info['importance'],\n",
    "        'score': var_info['score'],\n",
    "        'usage': var_info['usage'],\n",
    "        'details': '; '.join(var_info['details'])\n",
    "    }\n",
    "    for rank, (var_name, var_info) in enumerate(ranked_variables, 1)\n",
    "])\n",
    "ranking_df.to_csv('mev_variable_ranking.csv', index=False)\n",
    "print(f\"\\n✓ Variable ranking saved to: mev_variable_ranking.csv\")\n",
    "\n",
    "# Display detailed breakdown\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED VARIABLE CONTRIBUTION BREAKDOWN\")\n",
    "print(f\"{'='*80}\")\n",
    "for rank, (var_name, var_info) in enumerate(ranked_variables, 1):\n",
    "    print(f\"\\n{rank}. {var_info['variable']} (Score: {var_info['score']:.1f}, {var_info['importance']})\")\n",
    "    print(f\"   Usage: {var_info['usage']}\")\n",
    "    print(\"   Details:\")\n",
    "    for detail in var_info['details']:\n",
    "        print(f\"     • {detail}\")\n",
    "\n",
    "# Stats and all_mev are now available for next cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# Cell 2: Display Per pAMM Top 10 MEV Attackers Table\n",
    "# ───────────────────────────────────────────────\n",
    "if not stats.empty:\n",
    "    # Show summary of pAMMs found\n",
    "    unique_pamms = stats['amm_trade'].unique()\n",
    "    print(f\"\\nTotal pAMMs with MEV activity: {len(unique_pamms)}\")\n",
    "    print(f\"pAMMs found: {', '.join(sorted(unique_pamms))}\\n\")\n",
    "    \n",
    "    # Display Per pAMM Top 10 MEV table using the function\n",
    "    # Use display() instead of print() for better formatting in Jupyter\n",
    "    display_per_pamm_top10_mev(stats, top_n=10)\n",
    "    \n",
    "    # Also save filtered top10 per pAMM version\n",
    "    # For each pAMM, get top 10 attackers\n",
    "    top10_per_pamm_list = []\n",
    "    for amm, group in stats.groupby('amm_trade'):\n",
    "        group = group.copy()  # Avoid SettingWithCopyWarning\n",
    "        group['total_mev'] = (group.get('fat_sandwich', 0) + \n",
    "                             group.get('sandwich', 0) + \n",
    "                             group.get('front_running', 0) + \n",
    "                             group.get('back_running', 0))\n",
    "        top10_per_pamm = group.nlargest(10, 'total_mev')\n",
    "        top10_per_pamm_list.append(top10_per_pamm)\n",
    "    \n",
    "    if top10_per_pamm_list:\n",
    "        top10_per_pamm_df = pd.concat(top10_per_pamm_list, ignore_index=True)\n",
    "        top10_per_pamm_df = top10_per_pamm_df.drop('total_mev', axis=1, errors='ignore')\n",
    "        top10_per_pamm_df.to_csv('per_pamm_top10_mev_with_validator.csv', index=False)\n",
    "        print(\"\\n✓ Per pAMM Top 10 MEV stats (top 10 per pAMM) saved to: per_pamm_top10_mev_with_validator.csv\")\n",
    "        \n",
    "        # Also display the full dataframe using display() for better viewing\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"Full Per pAMM Top 10 MEV Table (All pAMMs Combined)\")\n",
    "        print(\"=\"*100)\n",
    "        display(top10_per_pamm_df)\n",
    "else:\n",
    "    print(\"\\nNo MEV stats available to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# Cell 3: Trades and Bots Group by Validator\n",
    "# ───────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"Trades and Bots Group by Validator\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "if not all_mev.empty:\n",
    "    trades_per_validator = trades.groupby('validator').size().reset_index(name='trade_count')\n",
    "    \n",
    "    bots_per_validator = all_mev.groupby('validator')['attacker_signer'].nunique().reset_index(name='bot_count')\n",
    "    \n",
    "    mev_types_per_validator = all_mev.groupby('validator')['type'].value_counts().unstack(fill_value=0)\n",
    "    \n",
    "    validator_stats = pd.merge(trades_per_validator, bots_per_validator, on='validator', how='outer').fillna(0)\n",
    "    validator_stats = pd.merge(validator_stats, mev_types_per_validator, on='validator', how='left').fillna(0)\n",
    "    \n",
    "    validator_stats['bot_ratio_%'] = (validator_stats['bot_count'] / validator_stats['trade_count'] * 100).round(2)\n",
    "    \n",
    "    validator_stats = validator_stats.sort_values('bot_count', ascending=False)\n",
    "    \n",
    "    # Safe output columns\n",
    "    output_cols = ['validator', 'trade_count', 'bot_count', 'bot_ratio_%']\n",
    "    mev_types = ['fat_sandwich', 'sandwich', 'front_running', 'back_running']\n",
    "    for col in mev_types:\n",
    "        if col in validator_stats.columns:\n",
    "            output_cols.append(col)\n",
    "    \n",
    "    # Show Top 10 Validators\n",
    "    print(\"Top 10 Validators by Bot Count:\")\n",
    "    print(\"-\" * 100)\n",
    "    top10_validator_stats = validator_stats.head(10)\n",
    "    print(top10_validator_stats[output_cols].to_string(index=False, float_format=lambda x: f'{x:.2f}' if isinstance(x, float) else x))\n",
    "    print()\n",
    "    \n",
    "    # Also display using display() for better viewing in Jupyter\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Top 10 Validators Table (Formatted)\")\n",
    "    print(\"=\"*100)\n",
    "    display(top10_validator_stats[output_cols])\n",
    "    \n",
    "    # Save full table\n",
    "    validator_stats.to_csv('mev_trades_bots_per_validator.csv', index=False)\n",
    "    print(f\"\\n✓ Full Trades and Bots grouped by validator saved to: mev_trades_bots_per_validator.csv\")\n",
    "    print(f\"✓ Total validators: {len(validator_stats)}\")\n",
    "    \n",
    "    # Also show full table using display() if user wants to see all\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Full Validator Stats Table (All Validators)\")\n",
    "    print(\"=\"*100)\n",
    "    display(validator_stats[output_cols])\n",
    "else:\n",
    "    print(\"No MEV data available for validator analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# Cell 4: VERIFICATION - Are These Actually MEV Bots or Aggregators?\n",
    "# Based on Grok's analysis: Verify if detected \"attackers\" are MEV bots or just aggregators\n",
    "# ───────────────────────────────────────────────\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"VERIFICATION: MEV Bots vs Aggregators (Jupiter, etc.)\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\n⚠️  IMPORTANT: The detected 'attackers' may be:\")\n",
    "print(\"   1. MEV bots (actual attackers)\")\n",
    "print(\"   2. Jupiter aggregator (bundling user orders)\")\n",
    "print(\"   3. Regular trade bots (market making, arbitrage)\")\n",
    "print(\"   4. Legitimate high-frequency traders\")\n",
    "print(\"\\nThis analysis helps distinguish between them.\\n\")\n",
    "\n",
    "if not sandwich_df.empty and len(top10_attackers) > 0:\n",
    "    # Analyze top attackers in detail\n",
    "    print(\"Analyzing top detected 'attackers' for MEV vs Aggregator patterns...\\n\")\n",
    "    \n",
    "    verification_results = []\n",
    "    \n",
    "    for attacker in top10_attackers[:5]:  # Analyze top 5\n",
    "        attacker_trades = trades[trades['signer'] == attacker].copy()\n",
    "        \n",
    "        if len(attacker_trades) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_trades = len(attacker_trades)\n",
    "        \n",
    "        # 1. Check for aggregator pattern: many unique signers in same slots\n",
    "        attacker_slots = attacker_trades['slot'].unique()\n",
    "        aggregator_score = 0\n",
    "        \n",
    "        for slot in attacker_slots[:100]:  # Sample first 100 slots\n",
    "            slot_trades = trades[trades['slot'] == slot]\n",
    "            unique_signers = slot_trades['signer'].nunique()\n",
    "            total_in_slot = len(slot_trades)\n",
    "            if total_in_slot > 0:\n",
    "                ratio = unique_signers / total_in_slot\n",
    "                if ratio > 0.7:  # High unique signer ratio = aggregator\n",
    "                    aggregator_score += 1\n",
    "        \n",
    "        aggregator_likelihood = aggregator_score / min(100, len(attacker_slots))\n",
    "        \n",
    "        # 2. Check for MEV patterns\n",
    "        # - Late-slot timing (front-running)\n",
    "        late_slot_trades = attacker_trades[attacker_trades['us_since_first_shred'] > 300000]\n",
    "        late_slot_ratio = len(late_slot_trades) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # - Oracle back-running\n",
    "        attacker_trades_with_prev = attacker_trades.copy()\n",
    "        attacker_trades_with_prev['prev_kind'] = df_clean.loc[attacker_trades_with_prev.index - 1, 'kind'].values\n",
    "        oracle_backrun = attacker_trades_with_prev[\n",
    "            (attacker_trades_with_prev['prev_kind'] == 'ORACLE') & \n",
    "            (attacker_trades_with_prev['time_diff_ms'] < 50)\n",
    "        ]\n",
    "        oracle_backrun_ratio = len(oracle_backrun) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # - High bytes_changed (oracle manipulation)\n",
    "        high_bytes = attacker_trades[attacker_trades['bytes_changed_trade'] > 50]\n",
    "        high_bytes_ratio = len(high_bytes) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # - Clusters (multiple tx in same slot)\n",
    "        attacker_trades['tx_in_slot'] = attacker_trades.groupby('slot')['slot'].transform('count')\n",
    "        clusters = attacker_trades[attacker_trades['tx_in_slot'] >= 2]\n",
    "        cluster_ratio = len(clusters) / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # Calculate MEV score\n",
    "        mev_score = (late_slot_ratio * 0.3 + \n",
    "                    oracle_backrun_ratio * 0.3 + \n",
    "                    high_bytes_ratio * 0.2 + \n",
    "                    cluster_ratio * 0.2)\n",
    "        \n",
    "        # 3. Check for wash trading patterns\n",
    "        # Wash trading indicators:\n",
    "        # - Circular flows (same address sending/receiving)\n",
    "        # - Many unique recipient addresses\n",
    "        # - Low net profit despite high volume\n",
    "        # - Temporary addresses (created and used once)\n",
    "        # - Immediate return of funds\n",
    "        \n",
    "        # Check for circular flow pattern: same signer appearing multiple times in short period\n",
    "        attacker_trades_sorted = attacker_trades.sort_values('ms_time')\n",
    "        time_gaps = attacker_trades_sorted['ms_time'].diff()\n",
    "        \n",
    "        # Wash trading: rapid cycling (many trades in short time)\n",
    "        if len(attacker_trades) > 10:\n",
    "            time_span_hours = (attacker_trades['ms_time'].max() - attacker_trades['ms_time'].min()) / (1000 * 60 * 60)\n",
    "            trades_per_hour = len(attacker_trades) / time_span_hours if time_span_hours > 0 else 0\n",
    "            \n",
    "            # High trade frequency with low profit indicators = wash trading\n",
    "            # Orb analysis showed: 116K USDC cycled, only 3.33 USDC profit = wash trading\n",
    "            # Pattern: >50 trades/hour with low MEV score = likely wash trading\n",
    "            wash_trading_score = trades_per_hour / 50  # Normalize to 50 trades/hour\n",
    "        else:\n",
    "            wash_trading_score = 0\n",
    "            trades_per_hour = 0\n",
    "        \n",
    "        # Classification with wash trading detection\n",
    "        if aggregator_likelihood > 0.5:\n",
    "            classification = \"LIKELY AGGREGATOR (Jupiter, etc.)\"\n",
    "            confidence = \"High\" if aggregator_likelihood > 0.7 else \"Medium\"\n",
    "        elif wash_trading_score > 1.0 and mev_score < 0.2:\n",
    "            # High trade frequency but low MEV score = wash trading\n",
    "            classification = \"LIKELY WASH TRADING (Volume Inflation)\"\n",
    "            confidence = \"High\" if wash_trading_score > 2.0 else \"Medium\"\n",
    "        elif mev_score > 0.3:\n",
    "            classification = \"LIKELY MEV BOT\"\n",
    "            confidence = \"High\" if mev_score > 0.5 else \"Medium\"\n",
    "        elif cluster_ratio > 0.3:\n",
    "            classification = \"POSSIBLE MEV (Sandwich patterns)\"\n",
    "            confidence = \"Medium\"\n",
    "        else:\n",
    "            classification = \"REGULAR TRADE BOT / UNKNOWN\"\n",
    "            confidence = \"Low\"\n",
    "        \n",
    "        verification_results.append({\n",
    "            'attacker': attacker,\n",
    "            'total_trades': total_trades,\n",
    "            'trades_per_hour': f\"{trades_per_hour:.1f}\",\n",
    "            'aggregator_likelihood': f\"{aggregator_likelihood:.2%}\",\n",
    "            'late_slot_ratio': f\"{late_slot_ratio:.2%}\",\n",
    "            'oracle_backrun_ratio': f\"{oracle_backrun_ratio:.2%}\",\n",
    "            'high_bytes_ratio': f\"{high_bytes_ratio:.2%}\",\n",
    "            'cluster_ratio': f\"{cluster_ratio:.2%}\",\n",
    "            'mev_score': f\"{mev_score:.2%}\",\n",
    "            'wash_trading_score': f\"{wash_trading_score:.2f}\",\n",
    "            'classification': classification,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'─'*100}\")\n",
    "        print(f\"Attacker: {attacker}\")\n",
    "        print(f\"{'─'*100}\")\n",
    "        print(f\"Total Trades: {total_trades:,}\")\n",
    "        print(f\"\\nPattern Analysis:\")\n",
    "        print(f\"  Trades per Hour: {trades_per_hour:.1f}\")\n",
    "        print(f\"  Aggregator Likelihood: {aggregator_likelihood:.2%} (slots with >70% unique signers)\")\n",
    "        print(f\"  Late-Slot Trades (>300ms): {late_slot_ratio:.2%}\")\n",
    "        print(f\"  Oracle Back-Running (<50ms): {oracle_backrun_ratio:.2%}\")\n",
    "        print(f\"  High Bytes Changed (>50): {high_bytes_ratio:.2%}\")\n",
    "        print(f\"  Clusters (≥2 tx/slot): {cluster_ratio:.2%}\")\n",
    "        print(f\"\\nMEV Score: {mev_score:.2%}\")\n",
    "        print(f\"Wash Trading Score: {wash_trading_score:.2f} (>1.0 = suspicious)\")\n",
    "        print(f\"\\nClassification: {classification}\")\n",
    "        print(f\"Confidence: {confidence}\")\n",
    "        \n",
    "        # Special note for wash trading\n",
    "        if \"WASH TRADING\" in classification:\n",
    "            print(f\"\\n⚠️  WASH TRADING DETECTED:\")\n",
    "            print(f\"   - High trade frequency ({trades_per_hour:.1f} trades/hour)\")\n",
    "            print(f\"   - Low MEV score ({mev_score:.2%}) = not extracting value\")\n",
    "            print(f\"   - Pattern: Volume inflation, transaction count farming, or protocol gaming\")\n",
    "            print(f\"   - Example: AEB9dXBoxkrapNd59Kg29JefMMf3M1WLcNA12XjKSf4R cycled 116K USDC, only 3.33 USDC profit\")\n",
    "    \n",
    "    # Summary table\n",
    "    verification_df = pd.DataFrame(verification_results)\n",
    "    if not verification_df.empty:\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(\"VERIFICATION SUMMARY\")\n",
    "        print(f\"{'='*100}\")\n",
    "        display(verification_df)\n",
    "        \n",
    "        # Save verification results\n",
    "        verification_df.to_csv('mev_attacker_verification.csv', index=False)\n",
    "        print(\"\\n✓ Verification results saved to: mev_attacker_verification.csv\")\n",
    "        \n",
    "        # Count classifications\n",
    "        print(f\"\\nClassification Breakdown:\")\n",
    "        print(verification_df['classification'].value_counts())\n",
    "        \n",
    "        print(f\"\\n⚠️  IMPORTANT NOTES:\")\n",
    "        print(f\"   - If 'LIKELY AGGREGATOR': These are NOT MEV bots, just Jupiter/aggregator routing\")\n",
    "        print(f\"   - If 'LIKELY WASH TRADING': Volume inflation, not MEV (e.g., AEB9dXBoxkrapNd59Kg29JefMMf3M1WLcNA12XjKSf4R)\")\n",
    "        print(f\"   - If 'LIKELY MEV BOT': These show MEV attack patterns\")\n",
    "        print(f\"   - If 'REGULAR TRADE BOT': May be legitimate market making/arbitrage\")\n",
    "        print(f\"   - Check Orb (orbmarkets.io) for these addresses to verify transaction history\")\n",
    "        print(f\"   - Look for failed transactions (slippage errors) = failed MEV attempts\")\n",
    "        print(f\"   - Wash trading: Circular flows, low profit despite high volume = protocol gaming\")\n",
    "        \n",
    "else:\n",
    "    print(\"No attackers detected to verify. This may indicate:\")\n",
    "    print(\"  - Low MEV activity (good news for pAMMs)\")\n",
    "    print(\"  - Detection thresholds too strict\")\n",
    "    print(\"  - Mostly aggregator routing (Jupiter) without MEV patterns\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Detailed Activity Analysis for Top Attackers\n",
    "# ───────────────────────────────────────────────\n",
    "if len(top10_attackers) > 0:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"DETAILED ACTIVITY ANALYSIS\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    for attacker in top10_attackers[:3]:  # Top 3\n",
    "        attacker_txs = df_clean[df_clean['signer'] == attacker].copy()\n",
    "        \n",
    "        if len(attacker_txs) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAttacker: {attacker}\")\n",
    "        print(f\"Total Events: {len(attacker_txs):,}\")\n",
    "        \n",
    "        # Event type breakdown\n",
    "        event_types = attacker_txs['kind'].value_counts()\n",
    "        print(f\"\\nEvent Types:\")\n",
    "        for kind, count in event_types.items():\n",
    "            print(f\"  {kind}: {count:,}\")\n",
    "        \n",
    "        # TRADE-specific analysis\n",
    "        attacker_trades = attacker_txs[attacker_txs['kind'] == 'TRADE']\n",
    "        if len(attacker_trades) > 0:\n",
    "            print(f\"\\nTRADE Analysis:\")\n",
    "            print(f\"  Total TRADEs: {len(attacker_trades):,}\")\n",
    "            \n",
    "            # Timing analysis\n",
    "            if 'us_since_first_shred' in attacker_trades.columns:\n",
    "                avg_latency = attacker_trades['us_since_first_shred'].mean()\n",
    "                late_trades = len(attacker_trades[attacker_trades['us_since_first_shred'] > 300000])\n",
    "                print(f\"  Avg Latency: {avg_latency:,.0f} µs\")\n",
    "                print(f\"  Late-Slot Trades (>300ms): {late_trades:,} ({late_trades/len(attacker_trades):.2%})\")\n",
    "            \n",
    "            # Bytes changed analysis\n",
    "            if 'bytes_changed_trade' in attacker_trades.columns:\n",
    "                high_bytes = len(attacker_trades[attacker_trades['bytes_changed_trade'] > 50])\n",
    "                print(f\"  High Bytes Changed (>50): {high_bytes:,} ({high_bytes/len(attacker_trades):.2%})\")\n",
    "            \n",
    "            # Slot clustering\n",
    "            slot_counts = attacker_trades['slot'].value_counts()\n",
    "            multi_slot = len(slot_counts[slot_counts >= 2])\n",
    "            print(f\"  Slots with Multiple Trades: {multi_slot:,}\")\n",
    "            \n",
    "            # Save detailed activity\n",
    "            activity_file = f'attacker_{attacker[:8]}_detailed_activity.csv'\n",
    "            attacker_trades.to_csv(activity_file, index=False)\n",
    "            print(f\"  ✓ Detailed activity saved to: {activity_file}\")\n",
    "    \n",
    "    # ───────────────────────────────────────────────\n",
    "    # Pool Activity Analysis - Show Bot Density Across Pools\n",
    "    # ───────────────────────────────────────────────\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"POOL ACTIVITY ANALYSIS - Attacker Distribution Across Pools\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    if len(top10_attackers) > 0:\n",
    "        pool_analysis_results = []\n",
    "        \n",
    "        for attacker in top10_attackers:\n",
    "            attacker_trades = trades[trades['signer'] == attacker].copy()\n",
    "            \n",
    "            if len(attacker_trades) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get pool information from account_trade (where is_pool_trade == True)\n",
    "            if 'account_trade' in attacker_trades.columns and 'is_pool_trade' in attacker_trades.columns:\n",
    "                pool_trades = attacker_trades[attacker_trades['is_pool_trade'] == True]\n",
    "                \n",
    "                if len(pool_trades) > 0:\n",
    "                    # Get unique pools\n",
    "                    unique_pools = pool_trades['account_trade'].nunique()\n",
    "                    pool_counts = pool_trades['account_trade'].value_counts()\n",
    "                    \n",
    "                    # Top pools for this attacker\n",
    "                    top_pools = pool_counts.head(5).to_dict()\n",
    "                    \n",
    "                    pool_analysis_results.append({\n",
    "                        'attacker': attacker,\n",
    "                        'total_trades': len(attacker_trades),\n",
    "                        'pool_trades': len(pool_trades),\n",
    "                        'unique_pools': unique_pools,\n",
    "                        'pools_per_trade_ratio': unique_pools / len(pool_trades) if len(pool_trades) > 0 else 0,\n",
    "                        'top_pool': pool_counts.index[0] if len(pool_counts) > 0 else 'N/A',\n",
    "                        'top_pool_trades': pool_counts.iloc[0] if len(pool_counts) > 0 else 0,\n",
    "                        'top_pool_%': (pool_counts.iloc[0] / len(pool_trades) * 100) if len(pool_trades) > 0 else 0\n",
    "                    })\n",
    "                else:\n",
    "                    # No pool trades detected\n",
    "                    pool_analysis_results.append({\n",
    "                        'attacker': attacker,\n",
    "                        'total_trades': len(attacker_trades),\n",
    "                        'pool_trades': 0,\n",
    "                        'unique_pools': 0,\n",
    "                        'pools_per_trade_ratio': 0,\n",
    "                        'top_pool': 'N/A',\n",
    "                        'top_pool_trades': 0,\n",
    "                        'top_pool_%': 0\n",
    "                    })\n",
    "            else:\n",
    "                # Fallback: use amm_trade as proxy for pools\n",
    "                unique_amms = attacker_trades['amm_trade'].nunique()\n",
    "                amm_counts = attacker_trades['amm_trade'].value_counts()\n",
    "                \n",
    "                pool_analysis_results.append({\n",
    "                    'attacker': attacker,\n",
    "                    'total_trades': len(attacker_trades),\n",
    "                    'pool_trades': len(attacker_trades),\n",
    "                    'unique_pools': unique_amms,  # Using AMM as proxy\n",
    "                    'pools_per_trade_ratio': unique_amms / len(attacker_trades) if len(attacker_trades) > 0 else 0,\n",
    "                    'top_pool': amm_counts.index[0] if len(amm_counts) > 0 else 'N/A',\n",
    "                    'top_pool_trades': amm_counts.iloc[0] if len(amm_counts) > 0 else 0,\n",
    "                    'top_pool_%': (amm_counts.iloc[0] / len(attacker_trades) * 100) if len(attacker_trades) > 0 else 0\n",
    "                })\n",
    "        \n",
    "        if pool_analysis_results:\n",
    "            pool_analysis_df = pd.DataFrame(pool_analysis_results)\n",
    "            \n",
    "            print(\"Attacker Pool Distribution Summary:\")\n",
    "            print(\"-\" * 100)\n",
    "            display_cols = ['attacker', 'total_trades', 'unique_pools', 'top_pool', 'top_pool_trades', 'top_pool_%']\n",
    "            display(pool_analysis_df[display_cols])\n",
    "            \n",
    "            # Detailed pool breakdown per attacker\n",
    "            print(f\"\\n{'─'*100}\")\n",
    "            print(\"DETAILED POOL BREAKDOWN PER ATTACKER:\")\n",
    "            print(f\"{'─'*100}\\n\")\n",
    "            \n",
    "            for attacker in top10_attackers:\n",
    "                attacker_trades = trades[trades['signer'] == attacker].copy()\n",
    "                \n",
    "                if len(attacker_trades) == 0:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nAttacker: {attacker}\")\n",
    "                print(f\"{'─'*80}\")\n",
    "                \n",
    "                if 'account_trade' in attacker_trades.columns and 'is_pool_trade' in attacker_trades.columns:\n",
    "                    pool_trades = attacker_trades[attacker_trades['is_pool_trade'] == True]\n",
    "                    \n",
    "                    if len(pool_trades) > 0:\n",
    "                        pool_stats = pool_trades.groupby(['account_trade', 'amm_trade']).agg({\n",
    "                            'slot': 'count',\n",
    "                            'ms_time': ['min', 'max']\n",
    "                        }).reset_index()\n",
    "                        pool_stats.columns = ['pool', 'amm', 'trade_count', 'first_trade', 'last_trade']\n",
    "                        pool_stats = pool_stats.sort_values('trade_count', ascending=False)\n",
    "                        \n",
    "                        print(f\"Active in {len(pool_stats)} pools:\")\n",
    "                        print(f\"\\nTop 10 Pools by Trade Count:\")\n",
    "                        top_pools = pool_stats.head(10)\n",
    "                        for idx, row in top_pools.iterrows():\n",
    "                            print(f\"  {idx+1}. Pool: {row['pool'][:20]}... | AMM: {row['amm']} | Trades: {row['trade_count']:,}\")\n",
    "                        \n",
    "                        if len(pool_stats) > 10:\n",
    "                            print(f\"  ... and {len(pool_stats) - 10} more pools\")\n",
    "                    else:\n",
    "                        print(\"No pool trades detected (using AMM as proxy)\")\n",
    "                        amm_stats = attacker_trades.groupby('amm_trade').size().sort_values(ascending=False)\n",
    "                        print(f\"Active in {len(amm_stats)} AMMs:\")\n",
    "                        for amm, count in amm_stats.head(10).items():\n",
    "                            print(f\"  • {amm}: {count:,} trades\")\n",
    "                else:\n",
    "                    # Fallback to AMM\n",
    "                    amm_stats = attacker_trades.groupby('amm_trade').size().sort_values(ascending=False)\n",
    "                    print(f\"Active in {len(amm_stats)} AMMs (pool data not available):\")\n",
    "                    for amm, count in amm_stats.head(10).items():\n",
    "                        print(f\"  • {amm}: {count:,} trades\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(f\"\\n{'─'*100}\")\n",
    "            print(\"POOL ACTIVITY SUMMARY:\")\n",
    "            print(f\"{'─'*100}\")\n",
    "            print(f\"Average pools per attacker: {pool_analysis_df['unique_pools'].mean():.1f}\")\n",
    "            print(f\"Max pools for single attacker: {pool_analysis_df['unique_pools'].max()}\")\n",
    "            print(f\"Min pools for single attacker: {pool_analysis_df['unique_pools'].min()}\")\n",
    "            print(f\"\\nAttacker Pool Concentration:\")\n",
    "            print(f\"  High concentration (1-2 pools): {len(pool_analysis_df[pool_analysis_df['unique_pools'] <= 2])} attackers\")\n",
    "            print(f\"  Medium concentration (3-5 pools): {len(pool_analysis_df[(pool_analysis_df['unique_pools'] > 2) & (pool_analysis_df['unique_pools'] <= 5)])} attackers\")\n",
    "            print(f\"  Low concentration (6+ pools): {len(pool_analysis_df[pool_analysis_df['unique_pools'] > 5])} attackers\")\n",
    "            \n",
    "            # Pool density analysis - which pools have most attackers\n",
    "            print(f\"\\n{'─'*100}\")\n",
    "            print(\"POOL DENSITY - Which Pools Have Most Attackers:\")\n",
    "            print(f\"{'─'*100}\")\n",
    "            \n",
    "            # Collect all pool-attacker pairs\n",
    "            pool_attacker_pairs = []\n",
    "            for attacker in top10_attackers:\n",
    "                attacker_trades = trades[trades['signer'] == attacker].copy()\n",
    "                \n",
    "                if 'account_trade' in attacker_trades.columns and 'is_pool_trade' in attacker_trades.columns:\n",
    "                    pool_trades = attacker_trades[attacker_trades['is_pool_trade'] == True]\n",
    "                    if len(pool_trades) > 0:\n",
    "                        unique_pools = pool_trades['account_trade'].unique()\n",
    "                        for pool in unique_pools:\n",
    "                            pool_attacker_pairs.append({\n",
    "                                'pool': pool,\n",
    "                                'attacker': attacker,\n",
    "                                'amm': pool_trades[pool_trades['account_trade'] == pool]['amm_trade'].iloc[0] if len(pool_trades[pool_trades['account_trade'] == pool]) > 0 else 'Unknown'\n",
    "                            })\n",
    "                else:\n",
    "                    # Fallback to AMM\n",
    "                    unique_amms = attacker_trades['amm_trade'].unique()\n",
    "                    for amm in unique_amms:\n",
    "                        pool_attacker_pairs.append({\n",
    "                            'pool': amm,  # Using AMM as pool proxy\n",
    "                            'attacker': attacker,\n",
    "                            'amm': amm\n",
    "                        })\n",
    "            \n",
    "            if pool_attacker_pairs:\n",
    "                pool_density_df = pd.DataFrame(pool_attacker_pairs)\n",
    "                pool_density = pool_density_df.groupby(['pool', 'amm']).size().reset_index(name='attacker_count')\n",
    "                pool_density = pool_density.sort_values('attacker_count', ascending=False)\n",
    "                \n",
    "                print(f\"\\nTop 20 Pools by Attacker Count (Bot Density):\")\n",
    "                print(pool_density.head(20).to_string(index=False))\n",
    "                \n",
    "                # Save pool analysis\n",
    "                pool_analysis_df.to_csv('attacker_pool_analysis.csv', index=False)\n",
    "                pool_density.to_csv('pool_density_analysis.csv', index=False)\n",
    "                print(f\"\\n✓ Attacker pool analysis saved to: attacker_pool_analysis.csv\")\n",
    "                print(f\"✓ Pool density analysis saved to: pool_density_analysis.csv\")\n",
    "                \n",
    "                # Insights\n",
    "                print(f\"\\n{'─'*100}\")\n",
    "                print(\"INSIGHTS:\")\n",
    "                print(f\"{'─'*100}\")\n",
    "                max_density_pool = pool_density.iloc[0]\n",
    "                print(f\"  • Most targeted pool: {max_density_pool['pool'][:20]}... ({max_density_pool['attacker_count']} attackers)\")\n",
    "                print(f\"  • AMM: {max_density_pool['amm']}\")\n",
    "                print(f\"  • High density pools (>3 attackers): {len(pool_density[pool_density['attacker_count'] > 3])} pools\")\n",
    "                print(f\"  • Low density pools (1 attacker): {len(pool_density[pool_density['attacker_count'] == 1])} pools\")\n",
    "                print(f\"\\n  Interpretation:\")\n",
    "                print(f\"    - High density pools (>3 attackers) = likely profitable targets for MEV\")\n",
    "                print(f\"    - Low density (1 attacker) = specialized bot or less profitable\")\n",
    "                print(f\"    - Multi-pool attackers = generalized bots scanning multiple opportunities\")\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"RECOMMENDATIONS:\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(\"1. Check Orb (orbmarkets.io) for these addresses:\")\n",
    "    for attacker in top10_attackers[:5]:\n",
    "        print(f\"   https://orbmarkets.io/account/{attacker}\")\n",
    "    print(\"\\n2. Look for:\")\n",
    "    print(\"   - Failed transactions (slippage errors) = failed MEV attempts\")\n",
    "    print(\"   - Transaction patterns (repeated front-run/back-run)\")\n",
    "    print(\"   - Program IDs (Jupiter aggregator vs MEV bot)\")\n",
    "    print(\"   - Profit/loss patterns\")\n",
    "    print(\"\\n3. Compare with known MEV bot addresses from public sources\")\n",
    "    print(\"4. Review verification results above to determine if aggregator or MEV\")\n",
    "    print(\"5. Review pool density analysis - high density pools may need protection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3b952-68f7-4f94-ac7b-e2e9c7bc1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of saved files:\n",
    "# 1. per_pamm_all_mev_with_validator.csv - All MEV stats for all attackers across all pAMMs\n",
    "# 2. per_pamm_top10_mev_with_validator.csv - Top 10 attackers per pAMM\n",
    "# 3. mev_trades_bots_per_validator.csv - Validator statistics with bot counts and MEV types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502a703-7530-4913-b1cb-7e8555cdd986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

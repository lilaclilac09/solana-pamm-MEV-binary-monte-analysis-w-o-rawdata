{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ORACLE vs TRADE Time Distribution Analysis\n",
        "\n",
        "This notebook performs comprehensive oracle analysis:\n",
        "\n",
        "1. **Compare ORACLE vs TRADE time distribution** (full-period per-minute density overlay chart)\n",
        "   - Overlays ORACLE and TRADE counts per minute\n",
        "   - Checks if oracles cluster before/after TRADEs\n",
        "   \n",
        "2. **Oracle Slot Patterns** - Slot-level oracle update patterns with correct datetime\n",
        "3. **Pool-Grouped Analysis** - Oracle updates grouped by pool (amm_oracle)\n",
        "4. **Extreme Burst Patterns** - High oracle updates with low trades\n",
        "5. **Extreme Low Trade Patterns** - Low trades with normal/high oracle updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": []
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (16, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ORACLE vs TRADE TIME DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Load cleaned data\n",
        "data_path = '/Users/aileen/Downloads/pamm/pamm_clean_final.parquet'\n",
        "print(f\"Loading data from: {data_path}\")\n",
        "df = pd.read_parquet(data_path)\n",
        "print(f\"✓ Loaded {len(df):,} total events\")\n",
        "\n",
        "# Ensure time columns exist\n",
        "if 'ms_time' not in df.columns:\n",
        "    df['ms_time'] = (df['time'] * 1000 + df['us_since_first_shred'].fillna(0) / 1000).astype(np.int64)\n",
        "\n",
        "if 'datetime' not in df.columns:\n",
        "    df['datetime'] = pd.to_datetime(df['time'], unit='s', utc=True)\n",
        "\n",
        "df = df.sort_values('ms_time').reset_index(drop=True)\n",
        "\n",
        "# Separate ORACLE and TRADE events\n",
        "oracles = df[df['kind'] == 'ORACLE'].copy()\n",
        "trades = df[df['kind'] == 'TRADE'].copy()\n",
        "\n",
        "print(f\"✓ ORACLE events: {len(oracles):,}\")\n",
        "print(f\"✓ TRADE events: {len(trades):,}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis 1: ORACLE vs TRADE Per-Minute Density Overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 1: ORACLE vs TRADE PER-MINUTE DENSITY OVERLAY\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Convert to datetime if needed\n",
        "oracles['datetime'] = pd.to_datetime(oracles['datetime'])\n",
        "trades['datetime'] = pd.to_datetime(trades['datetime'])\n",
        "\n",
        "# Get time range\n",
        "min_time = min(oracles['datetime'].min(), trades['datetime'].min())\n",
        "max_time = max(oracles['datetime'].max(), trades['datetime'].max())\n",
        "total_duration = max_time - min_time\n",
        "\n",
        "print(f\"Time range: {min_time} to {max_time}\")\n",
        "print(f\"Total duration: {total_duration}\")\n",
        "print()\n",
        "\n",
        "# Create per-minute bins\n",
        "time_range = pd.date_range(start=min_time.floor('min'), end=max_time.ceil('min'), freq='1min')\n",
        "oracle_counts = oracles.groupby(oracles['datetime'].dt.floor('min')).size()\n",
        "trade_counts = trades.groupby(trades['datetime'].dt.floor('min')).size()\n",
        "\n",
        "# Create full time series with zeros\n",
        "oracle_ts = pd.Series(0, index=time_range)\n",
        "trade_ts = pd.Series(0, index=time_range)\n",
        "oracle_ts.loc[oracle_counts.index] = oracle_counts\n",
        "trade_ts.loc[trade_counts.index] = trade_counts\n",
        "\n",
        "# Calculate statistics\n",
        "print(\"Per-Minute Statistics:\")\n",
        "print(f\"  ORACLE - Mean: {oracle_ts.mean():.2f}, Max: {oracle_ts.max()}, Std: {oracle_ts.std():.2f}\")\n",
        "print(f\"  TRADE  - Mean: {trade_ts.mean():.2f}, Max: {trade_ts.max()}, Std: {trade_ts.std():.2f}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create overlay chart\n",
        "fig, axes = plt.subplots(2, 1, figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Full-period overlay\n",
        "ax1 = axes[0]\n",
        "ax1.plot(oracle_ts.index, oracle_ts.values, label='ORACLE', color='#2E86AB', alpha=0.7, linewidth=1.5)\n",
        "ax1.plot(trade_ts.index, trade_ts.values, label='TRADE', color='#A23B72', alpha=0.7, linewidth=1.5)\n",
        "ax1.fill_between(oracle_ts.index, 0, oracle_ts.values, alpha=0.3, color='#2E86AB')\n",
        "ax1.fill_between(trade_ts.index, 0, trade_ts.values, alpha=0.3, color='#A23B72')\n",
        "ax1.set_xlabel('Time', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Events per Minute', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('ORACLE vs TRADE Time Distribution (Full Period - Per-Minute Density)', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "ax1.legend(loc='upper right', fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 2: Zoomed view (first 24 hours or sample period)\n",
        "sample_hours = min(24, int(total_duration.total_seconds() / 3600))\n",
        "sample_end = min_time + timedelta(hours=sample_hours)\n",
        "sample_mask = (oracle_ts.index >= min_time) & (oracle_ts.index <= sample_end)\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.plot(oracle_ts.index[sample_mask], oracle_ts.values[sample_mask], \n",
        "         label='ORACLE', color='#2E86AB', alpha=0.7, linewidth=1.5, marker='o', markersize=2)\n",
        "ax2.plot(trade_ts.index[sample_mask], trade_ts.values[sample_mask], \n",
        "         label='TRADE', color='#A23B72', alpha=0.7, linewidth=1.5, marker='s', markersize=2)\n",
        "ax2.fill_between(oracle_ts.index[sample_mask], 0, oracle_ts.values[sample_mask], \n",
        "                 alpha=0.3, color='#2E86AB')\n",
        "ax2.fill_between(trade_ts.index[sample_mask], 0, trade_ts.values[sample_mask], \n",
        "                 alpha=0.3, color='#A23B72')\n",
        "ax2.set_xlabel('Time', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Events per Minute', fontsize=12, fontweight='bold')\n",
        "ax2.set_title(f'ORACLE vs TRADE Time Distribution (First {sample_hours} Hours - Zoomed View)', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "ax2.legend(loc='upper right', fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "import os\n",
        "output_path = 'oracle_trade_density_overlay.png'\n",
        "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Saved overlay chart to: {output_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis 1b: Oracle Clustering Before/After Trades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 1b: ORACLE CLUSTERING BEFORE/AFTER TRADES\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Define time windows for analysis (in seconds)\n",
        "windows = [5, 10, 30, 60]  # 5s, 10s, 30s, 1min\n",
        "\n",
        "results = []\n",
        "\n",
        "for window_sec in windows:\n",
        "    window_ms = window_sec * 1000\n",
        "    \n",
        "    # For each trade, count oracles before and after\n",
        "    oracle_before = []\n",
        "    oracle_after = []\n",
        "    \n",
        "    for _, trade in trades.iterrows():\n",
        "        trade_time = trade['ms_time']\n",
        "        \n",
        "        # Count oracles in window before trade\n",
        "        before_mask = (oracles['ms_time'] >= trade_time - window_ms) & (oracles['ms_time'] < trade_time)\n",
        "        oracle_before.append(before_mask.sum())\n",
        "        \n",
        "        # Count oracles in window after trade\n",
        "        after_mask = (oracles['ms_time'] > trade_time) & (oracles['ms_time'] <= trade_time + window_ms)\n",
        "        oracle_after.append(after_mask.sum())\n",
        "    \n",
        "    avg_before = np.mean(oracle_before)\n",
        "    avg_after = np.mean(oracle_after)\n",
        "    median_before = np.median(oracle_before)\n",
        "    median_after = np.median(oracle_after)\n",
        "    \n",
        "    results.append({\n",
        "        'window_sec': window_sec,\n",
        "        'avg_oracle_before': avg_before,\n",
        "        'avg_oracle_after': avg_after,\n",
        "        'median_oracle_before': median_before,\n",
        "        'median_oracle_after': median_after,\n",
        "        'ratio_after_before': avg_after / avg_before if avg_before > 0 else 0\n",
        "    })\n",
        "    \n",
        "    print(f\"Window: ±{window_sec}s around TRADEs\")\n",
        "    print(f\"  Avg ORACLE before: {avg_before:.2f}\")\n",
        "    print(f\"  Avg ORACLE after:  {avg_after:.2f}\")\n",
        "    print(f\"  Ratio (after/before): {avg_after/avg_before if avg_before > 0 else 0:.2f}\")\n",
        "    print()\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Summary Table:\")\n",
        "print(results_df.to_string(index=False))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Removed: Wide Sandwich Detection moved to 02_mev_detection notebook -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removed: Wide Sandwich Detection code moved to 02_mev_detection notebook\n",
        "# See 02_mev_detection.ipynb for Fat Sandwich and Classic Sandwich detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE ORACLE ANALYSIS: SLOT PATTERNS, POOL GROUPING, EXTREME PATTERNS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'outputs/csv'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: Generate oracle_slot_patterns.csv with CORRECT datetime\n",
        "# ============================================================================\n",
        "print(\"PART 1: Generating oracle_slot_patterns.csv with correct datetime\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create slot to datetime mapping from original data\n",
        "# Use the first occurrence of each slot to get its datetime\n",
        "slot_to_datetime = oracles.groupby('slot')['datetime'].first().to_dict()\n",
        "\n",
        "# Group oracle events by slot and amm_oracle\n",
        "oracle_slot_patterns = oracles.groupby(['slot', 'amm_oracle']).size().reset_index(name='oracle_count')\n",
        "\n",
        "# Add datetime column using the mapping\n",
        "oracle_slot_patterns['datetime'] = oracle_slot_patterns['slot'].map(slot_to_datetime)\n",
        "\n",
        "# Handle any missing datetimes (shouldn't happen, but safety check)\n",
        "if oracle_slot_patterns['datetime'].isna().any():\n",
        "    print(f\"⚠️  Warning: {oracle_slot_patterns['datetime'].isna().sum()} rows have missing datetime\")\n",
        "    # Fill missing datetimes by interpolating from slot numbers\n",
        "    # Solana slots are approximately 400ms apart\n",
        "    min_slot = oracle_slot_patterns['slot'].min()\n",
        "    min_datetime = oracle_slot_patterns['datetime'].min()\n",
        "    slot_duration_ms = 400  # milliseconds per slot\n",
        "    \n",
        "    missing_mask = oracle_slot_patterns['datetime'].isna()\n",
        "    if missing_mask.any():\n",
        "        slot_diffs = (oracle_slot_patterns.loc[missing_mask, 'slot'] - min_slot)\n",
        "        oracle_slot_patterns.loc[missing_mask, 'datetime'] = min_datetime + pd.to_timedelta(slot_diffs * slot_duration_ms, unit='ms')\n",
        "\n",
        "# Ensure datetime is in proper format\n",
        "oracle_slot_patterns['datetime'] = pd.to_datetime(oracle_slot_patterns['datetime'])\n",
        "\n",
        "# Reorder columns: slot, datetime, amm_oracle, oracle_count\n",
        "oracle_slot_patterns = oracle_slot_patterns[['slot', 'datetime', 'amm_oracle', 'oracle_count']].sort_values(['slot', 'amm_oracle'])\n",
        "\n",
        "# Save to CSV\n",
        "oracle_slot_patterns_output = os.path.join(output_dir, 'oracle_slot_patterns.csv')\n",
        "oracle_slot_patterns.to_csv(oracle_slot_patterns_output, index=False)\n",
        "print(f\"✓ Saved oracle_slot_patterns.csv with {len(oracle_slot_patterns):,} rows\")\n",
        "print(f\"  Time range: {oracle_slot_patterns['datetime'].min()} to {oracle_slot_patterns['datetime'].max()}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: Oracle Updates Analysis Grouped by Pool (amm_oracle)\n",
        "# ============================================================================\n",
        "print(\"PART 2: Oracle Updates Analysis Grouped by Pool\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Group oracle updates by pool (amm_oracle)\n",
        "pool_oracle_stats = oracles.groupby('amm_oracle').agg({\n",
        "    'slot': ['count', 'nunique'],\n",
        "    'datetime': ['min', 'max'],\n",
        "    'ms_time': ['min', 'max']\n",
        "}).reset_index()\n",
        "\n",
        "pool_oracle_stats.columns = ['pool', 'total_oracle_updates', 'unique_slots', 'first_update', 'last_update', 'first_ms_time', 'last_ms_time']\n",
        "\n",
        "# Calculate duration and frequency\n",
        "pool_oracle_stats['duration_seconds'] = (pool_oracle_stats['last_update'] - pool_oracle_stats['first_update']).dt.total_seconds()\n",
        "pool_oracle_stats['duration_hours'] = pool_oracle_stats['duration_seconds'] / 3600\n",
        "pool_oracle_stats['updates_per_second'] = pool_oracle_stats['total_oracle_updates'] / (pool_oracle_stats['duration_seconds'] + 1)\n",
        "pool_oracle_stats['updates_per_slot'] = pool_oracle_stats['total_oracle_updates'] / (pool_oracle_stats['unique_slots'] + 1)\n",
        "\n",
        "# Add trade statistics per pool for comparison\n",
        "pool_trade_stats = trades.groupby('amm_oracle').agg({\n",
        "    'slot': 'count',\n",
        "    'ms_time': ['min', 'max']\n",
        "}).reset_index()\n",
        "pool_trade_stats.columns = ['pool', 'total_trades', 'first_trade_ms', 'last_trade_ms']\n",
        "\n",
        "# Merge oracle and trade stats\n",
        "pool_analysis = pool_oracle_stats.merge(pool_trade_stats, on='pool', how='left').fillna(0)\n",
        "pool_analysis['total_trades'] = pool_analysis['total_trades'].astype(int)\n",
        "\n",
        "# Calculate oracle-to-trade ratio\n",
        "pool_analysis['oracle_trade_ratio'] = pool_analysis['total_oracle_updates'] / (pool_analysis['total_trades'] + 1)\n",
        "\n",
        "# Sort by total oracle updates\n",
        "pool_analysis = pool_analysis.sort_values('total_oracle_updates', ascending=False)\n",
        "\n",
        "print(\"\\nPool Statistics (Top 10 by Oracle Updates):\")\n",
        "print(\"-\" * 80)\n",
        "display_cols = ['pool', 'total_oracle_updates', 'total_trades', 'oracle_trade_ratio', \n",
        "                'updates_per_second', 'updates_per_slot', 'duration_hours']\n",
        "print(pool_analysis[display_cols].head(10).to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Save pool analysis\n",
        "pool_analysis_output = os.path.join(output_dir, 'oracle_updates_by_pool.csv')\n",
        "pool_analysis.to_csv(pool_analysis_output, index=False)\n",
        "print(f\"✓ Saved pool analysis to: {pool_analysis_output}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: Extreme Burst Patterns Analysis\n",
        "# ============================================================================\n",
        "print(\"PART 3: Extreme Burst Patterns (High Oracle Updates, Low Trades)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create time windows (per second)\n",
        "oracles['time_window_sec'] = (oracles['ms_time'] // 1000).astype(int)\n",
        "trades['time_window_sec'] = (trades['ms_time'] // 1000).astype(int)\n",
        "\n",
        "# Count events per second per pool\n",
        "oracle_counts_by_pool = oracles.groupby(['time_window_sec', 'amm_oracle']).size().reset_index(name='oracle_count')\n",
        "trade_counts_by_pool = trades.groupby(['time_window_sec', 'amm_oracle']).size().reset_index(name='trade_count')\n",
        "\n",
        "# Merge\n",
        "events_by_pool_sec = oracle_counts_by_pool.merge(\n",
        "    trade_counts_by_pool, \n",
        "    on=['time_window_sec', 'amm_oracle'], \n",
        "    how='outer'\n",
        ").fillna(0)\n",
        "events_by_pool_sec['oracle_count'] = events_by_pool_sec['oracle_count'].astype(int)\n",
        "events_by_pool_sec['trade_count'] = events_by_pool_sec['trade_count'].astype(int)\n",
        "\n",
        "# Calculate oracle-to-trade ratio\n",
        "events_by_pool_sec['oracle_trade_ratio'] = events_by_pool_sec['oracle_count'] / (events_by_pool_sec['trade_count'] + 1)\n",
        "\n",
        "# Define extreme burst thresholds per pool\n",
        "extreme_bursts_by_pool = []\n",
        "for pool in events_by_pool_sec['amm_oracle'].unique():\n",
        "    pool_data = events_by_pool_sec[events_by_pool_sec['amm_oracle'] == pool]\n",
        "    if len(pool_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Pool-specific thresholds (top 1% oracle count, bottom 10% trade count)\n",
        "    oracle_p99 = pool_data['oracle_count'].quantile(0.99)\n",
        "    trade_p10 = pool_data['trade_count'].quantile(0.10)\n",
        "    \n",
        "    # Find extreme bursts for this pool\n",
        "    pool_bursts = pool_data[\n",
        "        (pool_data['oracle_count'] >= oracle_p99) & \n",
        "        (pool_data['trade_count'] <= trade_p10)\n",
        "    ].copy()\n",
        "    \n",
        "    if len(pool_bursts) > 0:\n",
        "        pool_bursts['pool'] = pool\n",
        "        pool_bursts['pool_oracle_p99'] = oracle_p99\n",
        "        pool_bursts['pool_trade_p10'] = trade_p10\n",
        "        extreme_bursts_by_pool.append(pool_bursts)\n",
        "\n",
        "if extreme_bursts_by_pool:\n",
        "    extreme_bursts_df = pd.concat(extreme_bursts_by_pool, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\nFound {len(extreme_bursts_df):,} extreme burst periods across {extreme_bursts_df['amm_oracle'].nunique()} pools\")\n",
        "    print(\"\\nTop 10 Extreme Bursts by Oracle-to-Trade Ratio:\")\n",
        "    print(\"-\" * 80)\n",
        "    top_bursts = extreme_bursts_df.nlargest(10, 'oracle_trade_ratio')\n",
        "    display_cols = ['amm_oracle', 'time_window_sec', 'oracle_count', 'trade_count', 'oracle_trade_ratio']\n",
        "    print(top_bursts[display_cols].to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Summary by pool\n",
        "    print(\"\\nExtreme Burst Summary by Pool:\")\n",
        "    print(\"-\" * 80)\n",
        "    burst_summary = extreme_bursts_df.groupby('amm_oracle').agg({\n",
        "        'time_window_sec': 'count',\n",
        "        'oracle_count': ['mean', 'max'],\n",
        "        'trade_count': 'mean',\n",
        "        'oracle_trade_ratio': ['mean', 'max']\n",
        "    }).reset_index()\n",
        "    burst_summary.columns = ['pool', 'num_burst_periods', 'avg_oracle_count', 'max_oracle_count', \n",
        "                            'avg_trade_count', 'avg_ratio', 'max_ratio']\n",
        "    burst_summary = burst_summary.sort_values('num_burst_periods', ascending=False)\n",
        "    print(burst_summary.to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Save extreme bursts\n",
        "    extreme_bursts_output = os.path.join(output_dir, 'extreme_bursts_by_pool.csv')\n",
        "    extreme_bursts_df.to_csv(extreme_bursts_output, index=False)\n",
        "    print(f\"✓ Saved extreme bursts analysis to: {extreme_bursts_output}\")\n",
        "else:\n",
        "    print(\"No extreme burst periods found with current thresholds.\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: Extreme Low Trade Patterns Analysis\n",
        "# ============================================================================\n",
        "print(\"PART 4: Extreme Low Trade Patterns (Low Trades with Normal/High Oracle Updates)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Find periods with very low trades but normal/high oracle activity\n",
        "extreme_low_trades_by_pool = []\n",
        "for pool in events_by_pool_sec['amm_oracle'].unique():\n",
        "    pool_data = events_by_pool_sec[events_by_pool_sec['amm_oracle'] == pool]\n",
        "    if len(pool_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Pool-specific thresholds (bottom 5% trade count, above median oracle count)\n",
        "    trade_p5 = pool_data['trade_count'].quantile(0.05)\n",
        "    oracle_median = pool_data['oracle_count'].quantile(0.50)\n",
        "    \n",
        "    # Find extreme low trade periods for this pool\n",
        "    pool_low_trades = pool_data[\n",
        "        (pool_data['trade_count'] <= trade_p5) & \n",
        "        (pool_data['oracle_count'] >= oracle_median)\n",
        "    ].copy()\n",
        "    \n",
        "    if len(pool_low_trades) > 0:\n",
        "        pool_low_trades['pool'] = pool\n",
        "        pool_low_trades['pool_trade_p5'] = trade_p5\n",
        "        pool_low_trades['pool_oracle_median'] = oracle_median\n",
        "        extreme_low_trades_by_pool.append(pool_low_trades)\n",
        "\n",
        "if extreme_low_trades_by_pool:\n",
        "    extreme_low_trades_df = pd.concat(extreme_low_trades_by_pool, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\nFound {len(extreme_low_trades_df):,} extreme low trade periods across {extreme_low_trades_df['amm_oracle'].nunique()} pools\")\n",
        "    print(\"\\nTop 10 Low Trade Periods by Oracle Count:\")\n",
        "    print(\"-\" * 80)\n",
        "    top_low_trades = extreme_low_trades_df.nlargest(10, 'oracle_count')\n",
        "    display_cols = ['amm_oracle', 'time_window_sec', 'oracle_count', 'trade_count', 'oracle_trade_ratio']\n",
        "    print(top_low_trades[display_cols].to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Summary by pool\n",
        "    print(\"\\nExtreme Low Trade Summary by Pool:\")\n",
        "    print(\"-\" * 80)\n",
        "    low_trade_summary = extreme_low_trades_df.groupby('amm_oracle').agg({\n",
        "        'time_window_sec': 'count',\n",
        "        'oracle_count': ['mean', 'max'],\n",
        "        'trade_count': 'mean',\n",
        "        'oracle_trade_ratio': ['mean', 'max']\n",
        "    }).reset_index()\n",
        "    low_trade_summary.columns = ['pool', 'num_low_trade_periods', 'avg_oracle_count', 'max_oracle_count', \n",
        "                                 'avg_trade_count', 'avg_ratio', 'max_ratio']\n",
        "    low_trade_summary = low_trade_summary.sort_values('num_low_trade_periods', ascending=False)\n",
        "    print(low_trade_summary.to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Save extreme low trades\n",
        "    extreme_low_trades_output = os.path.join(output_dir, 'extreme_low_trades_by_pool.csv')\n",
        "    extreme_low_trades_df.to_csv(extreme_low_trades_output, index=False)\n",
        "    print(f\"✓ Saved extreme low trades analysis to: {extreme_low_trades_output}\")\n",
        "else:\n",
        "    print(\"No extreme low trade periods found with current thresholds.\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: Inference and Insights\n",
        "# ============================================================================\n",
        "print(\"PART 5: Key Inferences and Insights\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"1. POOL-SPECIFIC ORACLE UPDATE PATTERNS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   • Total pools analyzed: {pool_analysis['pool'].nunique()}\")\n",
        "print(f\"   • Pool with most oracle updates: {pool_analysis.iloc[0]['pool']} ({pool_analysis.iloc[0]['total_oracle_updates']:,} updates)\")\n",
        "print(f\"   • Average updates per second across pools: {pool_analysis['updates_per_second'].mean():.2f}\")\n",
        "print(f\"   • Highest oracle-to-trade ratio: {pool_analysis.iloc[0]['pool']} ({pool_analysis.iloc[0]['oracle_trade_ratio']:.2f})\")\n",
        "print()\n",
        "\n",
        "if extreme_bursts_by_pool:\n",
        "    print(\"2. EXTREME BURST PATTERNS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"   • Total extreme burst periods: {len(extreme_bursts_df):,}\")\n",
        "    print(f\"   • Pools with extreme bursts: {extreme_bursts_df['amm_oracle'].nunique()}\")\n",
        "    print(f\"   • Pool with most bursts: {burst_summary.iloc[0]['pool']} ({burst_summary.iloc[0]['num_burst_periods']} periods)\")\n",
        "    print(f\"   • Average oracle count during bursts: {extreme_bursts_df['oracle_count'].mean():.1f}\")\n",
        "    print(f\"   • Average trade count during bursts: {extreme_bursts_df['trade_count'].mean():.2f}\")\n",
        "    print(f\"   • Average oracle-to-trade ratio: {extreme_bursts_df['oracle_trade_ratio'].mean():.2f}\")\n",
        "    print()\n",
        "    print(\"   INFERENCE: Extreme bursts suggest:\")\n",
        "    print(\"   • Potential oracle spam/manipulation to overwhelm network\")\n",
        "    print(\"   • Possible MEV attack preparation (creating favorable conditions)\")\n",
        "    print(\"   • Network congestion periods where trades are suppressed\")\n",
        "    print(\"   • Validator-specific batching behavior\")\n",
        "    print()\n",
        "\n",
        "if extreme_low_trades_by_pool:\n",
        "    print(\"3. EXTREME LOW TRADE PATTERNS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"   • Total low trade periods: {len(extreme_low_trades_df):,}\")\n",
        "    print(f\"   • Pools with low trade periods: {extreme_low_trades_df['amm_oracle'].nunique()}\")\n",
        "    print(f\"   • Pool with most low trade periods: {low_trade_summary.iloc[0]['pool']} ({low_trade_summary.iloc[0]['num_low_trade_periods']} periods)\")\n",
        "    print(f\"   • Average oracle count during low trade periods: {extreme_low_trades_df['oracle_count'].mean():.1f}\")\n",
        "    print(f\"   • Average trade count during low trade periods: {extreme_low_trades_df['trade_count'].mean():.2f}\")\n",
        "    print()\n",
        "    print(\"   INFERENCE: Low trade periods with normal/high oracle activity suggest:\")\n",
        "    print(\"   • Oracle updates continuing despite low trading activity\")\n",
        "    print(\"   • Possible market-making or rebalancing without user trades\")\n",
        "    print(\"   • Periods of low liquidity or market uncertainty\")\n",
        "    print(\"   • Potential oracle manipulation without corresponding trades\")\n",
        "    print()\n",
        "\n",
        "print(\"4. OVERALL PATTERN INSIGHTS:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"   • Oracle updates are highly frequent compared to trades\")\n",
        "print(\"   • Pool-specific patterns vary significantly (some pools more active)\")\n",
        "print(\"   • Extreme bursts correlate with suppressed trade activity\")\n",
        "print(\"   • Low trade periods may indicate market conditions or manipulation\")\n",
        "print(\"   • Oracle-to-trade ratios reveal pool health and activity levels\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE ORACLE ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removed: Wide Sandwich cluster processing code moved to 02_mev_detection notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(clusters_df) > 0:\n",
        "    # Focus on cross-slot clusters (most suspicious)\n",
        "    cross_slot_clusters = clusters_df[clusters_df['is_cross_slot']].copy()\n",
        "    \n",
        "    if len(cross_slot_clusters) > 0:\n",
        "        print(\"=\"*80)\n",
        "        print(\"CROSS-SLOT CLUSTERS (HIGHEST SUSPICION)\")\n",
        "        print(\"=\"*80)\n",
        "        print()\n",
        "        \n",
        "        # Sort by number of trades (descending)\n",
        "        cross_slot_clusters = cross_slot_clusters.sort_values('num_trades', ascending=False)\n",
        "        \n",
        "        print(f\"Total cross-slot clusters: {len(cross_slot_clusters)}\")\n",
        "        print()\n",
        "        print(\"Top 20 Cross-Slot Clusters:\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        display_cols = ['window_sec', 'num_trades', 'duration_ms', 'num_slots', \n",
        "                       'start_slot', 'end_slot', 'unique_signers', 'unique_amms', 'unique_validators']\n",
        "        print(cross_slot_clusters[display_cols].head(20).to_string(index=False))\n",
        "        print()\n",
        "        \n",
        "        # Save cross-slot clusters separately\n",
        "        import os\n",
        "        output_dir = 'derived'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        cross_slot_output = os.path.join(output_dir, 'cross_slot_sandwich_clusters.csv')\n",
        "        cross_slot_clusters.to_csv(cross_slot_output, index=False)\n",
        "        print(f\"✓ Saved cross-slot clusters to: {cross_slot_output}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No cross-slot clusters found.\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No clusters found meeting the minimum criteria.\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis 3: Extreme Oracle Bursts with Minimal Trade Activity\n",
        "\n",
        "This analysis investigates periods when oracle events spike dramatically while trade events drop to near zero - a pattern that may indicate:\n",
        "- Oracle manipulation or spam attacks\n",
        "- Network congestion during oracle updates\n",
        "- Validator behavior during high oracle activity\n",
        "- Potential MEV attack preparation phases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 3: EXTREME ORACLE BURSTS WITH MINIMAL TRADE ACTIVITY\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Create per-second time windows for both oracle and trade events\n",
        "oracles['time_window_sec'] = (oracles['ms_time'] // 1000).astype(int)\n",
        "trades['time_window_sec'] = (trades['ms_time'] // 1000).astype(int)\n",
        "\n",
        "# Count events per second\n",
        "oracle_counts = oracles.groupby('time_window_sec').size().reset_index(name='oracle_count')\n",
        "trade_counts = trades.groupby('time_window_sec').size().reset_index(name='trade_count')\n",
        "\n",
        "# Merge to get both counts per second\n",
        "events_per_sec = oracle_counts.merge(trade_counts, on='time_window_sec', how='outer').fillna(0)\n",
        "events_per_sec['oracle_count'] = events_per_sec['oracle_count'].astype(int)\n",
        "events_per_sec['trade_count'] = events_per_sec['trade_count'].astype(int)\n",
        "\n",
        "# Calculate oracle-to-trade ratio\n",
        "events_per_sec['oracle_trade_ratio'] = events_per_sec['oracle_count'] / (events_per_sec['trade_count'] + 1)  # +1 to avoid division by zero\n",
        "\n",
        "# Identify extreme bursts: high oracle count with low trade count\n",
        "# Define thresholds: top 1% of oracle counts AND bottom 10% of trade counts\n",
        "oracle_p99 = events_per_sec['oracle_count'].quantile(0.99)\n",
        "trade_p10 = events_per_sec['trade_count'].quantile(0.10)\n",
        "\n",
        "print(f\"Oracle burst threshold (99th percentile): {oracle_p99:.0f} oracles/second\")\n",
        "print(f\"Low trade threshold (10th percentile): {trade_p10:.0f} trades/second\")\n",
        "print()\n",
        "\n",
        "# Find extreme burst periods\n",
        "extreme_bursts = events_per_sec[\n",
        "    (events_per_sec['oracle_count'] >= oracle_p99) & \n",
        "    (events_per_sec['trade_count'] <= trade_p10)\n",
        "].copy()\n",
        "\n",
        "print(f\"Found {len(extreme_bursts)} seconds with extreme oracle bursts and minimal trades\")\n",
        "print()\n",
        "\n",
        "if len(extreme_bursts) > 0:\n",
        "    # Sort by oracle count (descending)\n",
        "    extreme_bursts = extreme_bursts.sort_values('oracle_count', ascending=False)\n",
        "    \n",
        "    print(\"Top 20 Extreme Burst Periods:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(extreme_bursts[['time_window_sec', 'oracle_count', 'trade_count', 'oracle_trade_ratio']].head(20).to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Get detailed data for these time windows\n",
        "    burst_windows = extreme_bursts['time_window_sec'].values\n",
        "    burst_oracles = oracles[oracles['time_window_sec'].isin(burst_windows)].copy()\n",
        "    burst_trades = trades[trades['time_window_sec'].isin(burst_windows)].copy()\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"CHARACTERISTICS OF EXTREME BURST PERIODS\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "    \n",
        "    # Validator analysis\n",
        "    if 'validator' in burst_oracles.columns and not burst_oracles['validator'].isna().all():\n",
        "        print(\"Top Validators During Extreme Bursts:\")\n",
        "        print(\"-\" * 80)\n",
        "        validator_counts = burst_oracles['validator'].value_counts().head(10)\n",
        "        for validator, count in validator_counts.items():\n",
        "            pct = (count / len(burst_oracles)) * 100\n",
        "            print(f\"  {validator[:44]}... : {count:,} oracles ({pct:.1f}%)\")\n",
        "        print()\n",
        "    \n",
        "    # Signer analysis\n",
        "    if 'signer' in burst_oracles.columns and not burst_oracles['signer'].isna().all():\n",
        "        print(\"Top Signers During Extreme Bursts:\")\n",
        "        print(\"-\" * 80)\n",
        "        signer_counts = burst_oracles['signer'].value_counts().head(10)\n",
        "        for signer, count in signer_counts.items():\n",
        "            pct = (count / len(burst_oracles)) * 100\n",
        "            print(f\"  {signer[:44]}... : {count:,} oracles ({pct:.1f}%)\")\n",
        "        print()\n",
        "    \n",
        "    # AMM analysis\n",
        "    if 'amm_oracle' in burst_oracles.columns and not burst_oracles['amm_oracle'].isna().all():\n",
        "        print(\"AMMs Affected During Extreme Bursts:\")\n",
        "        print(\"-\" * 80)\n",
        "        amm_counts = burst_oracles['amm_oracle'].value_counts()\n",
        "        for amm, count in amm_counts.items():\n",
        "            pct = (count / len(burst_oracles)) * 100\n",
        "            print(f\"  {amm}: {count:,} oracles ({pct:.1f}%)\")\n",
        "        print()\n",
        "    \n",
        "    # Slot analysis\n",
        "    if 'slot' in burst_oracles.columns and not burst_oracles['slot'].isna().all():\n",
        "        print(\"Slot Distribution During Extreme Bursts:\")\n",
        "        print(\"-\" * 80)\n",
        "        slot_counts = burst_oracles['slot'].value_counts().head(10)\n",
        "        print(f\"  Unique slots: {burst_oracles['slot'].nunique():,}\")\n",
        "        print(f\"  Top 10 slots by oracle count:\")\n",
        "        for slot, count in slot_counts.items():\n",
        "            print(f\"    Slot {slot}: {count:,} oracles\")\n",
        "        print()\n",
        "    \n",
        "    # Time distribution\n",
        "    if 'datetime' in burst_oracles.columns:\n",
        "        print(\"Time Distribution of Extreme Bursts:\")\n",
        "        print(\"-\" * 80)\n",
        "        burst_oracles['datetime'] = pd.to_datetime(burst_oracles['datetime'])\n",
        "        time_range = burst_oracles['datetime'].max() - burst_oracles['datetime'].min()\n",
        "        print(f\"  First burst: {burst_oracles['datetime'].min()}\")\n",
        "        print(f\"  Last burst: {burst_oracles['datetime'].max()}\")\n",
        "        print(f\"  Time span: {time_range}\")\n",
        "        print()\n",
        "    \n",
        "    # Compare with normal periods\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPARISON: EXTREME BURSTS vs NORMAL PERIODS\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "    \n",
        "    normal_periods = events_per_sec[\n",
        "        (events_per_sec['oracle_count'] < oracle_p99) | \n",
        "        (events_per_sec['trade_count'] > trade_p10)\n",
        "    ]\n",
        "    \n",
        "    print(\"Oracle Statistics:\")\n",
        "    print(f\"  Extreme bursts - Mean: {extreme_bursts['oracle_count'].mean():.1f}, Max: {extreme_bursts['oracle_count'].max()}\")\n",
        "    print(f\"  Normal periods - Mean: {normal_periods['oracle_count'].mean():.1f}, Max: {normal_periods['oracle_count'].max()}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"Trade Statistics:\")\n",
        "    print(f\"  Extreme bursts - Mean: {extreme_bursts['trade_count'].mean():.1f}, Max: {extreme_bursts['trade_count'].max()}\")\n",
        "    print(f\"  Normal periods - Mean: {normal_periods['trade_count'].mean():.1f}, Max: {normal_periods['trade_count'].max()}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"Oracle-to-Trade Ratio:\")\n",
        "    print(f\"  Extreme bursts - Mean: {extreme_bursts['oracle_trade_ratio'].mean():.1f}, Max: {extreme_bursts['oracle_trade_ratio'].max():.1f}\")\n",
        "    print(f\"  Normal periods - Mean: {normal_periods['oracle_trade_ratio'].mean():.1f}, Max: {normal_periods['oracle_trade_ratio'].max():.1f}\")\n",
        "    print()\n",
        "    \n",
        "    # Save results\n",
        "    import os\n",
        "    output_dir = 'derived'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save extreme bursts summary\n",
        "    extreme_bursts_output = os.path.join(output_dir, 'extreme_oracle_bursts_summary.csv')\n",
        "    extreme_bursts.to_csv(extreme_bursts_output, index=False)\n",
        "    print(f\"✓ Saved extreme bursts summary to: {extreme_bursts_output}\")\n",
        "    \n",
        "    # Save detailed oracle events during bursts\n",
        "    if len(burst_oracles) > 0:\n",
        "        burst_oracles_output = os.path.join(output_dir, 'extreme_burst_oracle_events.csv')\n",
        "        burst_oracles.to_csv(burst_oracles_output, index=False)\n",
        "        print(f\"✓ Saved detailed oracle events to: {burst_oracles_output}\")\n",
        "    \n",
        "    # Save trade events during bursts (if any)\n",
        "    if len(burst_trades) > 0:\n",
        "        burst_trades_output = os.path.join(output_dir, 'extreme_burst_trade_events.csv')\n",
        "        burst_trades.to_csv(burst_trades_output, index=False)\n",
        "        print(f\"✓ Saved trade events during bursts to: {burst_trades_output}\")\n",
        "    \n",
        "    print()\n",
        "else:\n",
        "    print(\"No extreme burst periods found with the current thresholds.\")\n",
        "    print(\"Try adjusting thresholds or check data quality.\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Extreme Oracle Bursts Analysis\n",
        "if len(extreme_bursts) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    fig.suptitle('Extreme Oracle Bursts with Minimal Trade Activity Analysis', \n",
        "                 fontsize=16, fontweight='bold', y=0.995)\n",
        "    \n",
        "    # Plot 1: Oracle vs Trade counts during extreme bursts\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.scatter(extreme_bursts['oracle_count'], extreme_bursts['trade_count'], \n",
        "               alpha=0.6, s=50, color='#FF6B6B', edgecolors='black', linewidth=0.5)\n",
        "    ax1.set_xlabel('Oracle Count per Second', fontsize=11, fontweight='bold')\n",
        "    ax1.set_ylabel('Trade Count per Second', fontsize=11, fontweight='bold')\n",
        "    ax1.set_title('Oracle vs Trade Counts During Extreme Bursts', fontsize=12, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Oracle-to-Trade Ratio Distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.hist(extreme_bursts['oracle_trade_ratio'], bins=50, edgecolor='black', \n",
        "            alpha=0.7, color='#4ECDC4')\n",
        "    ax2.set_xlabel('Oracle-to-Trade Ratio', fontsize=11, fontweight='bold')\n",
        "    ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax2.set_title('Distribution of Oracle-to-Trade Ratios', fontsize=12, fontweight='bold')\n",
        "    ax2.set_xscale('log')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Time series of extreme bursts\n",
        "    ax3 = axes[1, 0]\n",
        "    # Sample for visualization if too many points\n",
        "    if len(extreme_bursts) > 1000:\n",
        "        sample_bursts = extreme_bursts.sample(1000).sort_values('time_window_sec')\n",
        "    else:\n",
        "        sample_bursts = extreme_bursts.sort_values('time_window_sec')\n",
        "    \n",
        "    ax3.scatter(sample_bursts['time_window_sec'], sample_bursts['oracle_count'], \n",
        "               alpha=0.6, s=30, color='#2E86AB', label='Oracle Count', marker='o')\n",
        "    ax3_twin = ax3.twinx()\n",
        "    ax3_twin.scatter(sample_bursts['time_window_sec'], sample_bursts['trade_count'], \n",
        "                    alpha=0.6, s=30, color='#A23B72', label='Trade Count', marker='s')\n",
        "    ax3.set_xlabel('Time Window (seconds)', fontsize=11, fontweight='bold')\n",
        "    ax3.set_ylabel('Oracle Count', fontsize=11, fontweight='bold', color='#2E86AB')\n",
        "    ax3_twin.set_ylabel('Trade Count', fontsize=11, fontweight='bold', color='#A23B72')\n",
        "    ax3.set_title('Time Series of Extreme Burst Periods', fontsize=12, fontweight='bold')\n",
        "    ax3.tick_params(axis='y', labelcolor='#2E86AB')\n",
        "    ax3_twin.tick_params(axis='y', labelcolor='#A23B72')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.legend(loc='upper left')\n",
        "    ax3_twin.legend(loc='upper right')\n",
        "    \n",
        "    # Plot 4: Comparison with normal periods\n",
        "    ax4 = axes[1, 1]\n",
        "    # Sample normal periods for comparison\n",
        "    if len(normal_periods) > 1000:\n",
        "        sample_normal = normal_periods.sample(1000)\n",
        "    else:\n",
        "        sample_normal = normal_periods\n",
        "    \n",
        "    ax4.scatter(sample_normal['oracle_count'], sample_normal['trade_count'], \n",
        "               alpha=0.3, s=20, color='gray', label='Normal Periods', marker='o')\n",
        "    ax4.scatter(extreme_bursts['oracle_count'], extreme_bursts['trade_count'], \n",
        "               alpha=0.7, s=50, color='#FF6B6B', label='Extreme Bursts', \n",
        "               edgecolors='black', linewidth=0.5, marker='^')\n",
        "    ax4.set_xlabel('Oracle Count per Second', fontsize=11, fontweight='bold')\n",
        "    ax4.set_ylabel('Trade Count per Second', fontsize=11, fontweight='bold')\n",
        "    ax4.set_title('Extreme Bursts vs Normal Periods', fontsize=12, fontweight='bold')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    import os\n",
        "    output_path = 'extreme_oracle_bursts_analysis.png'\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ Saved visualization to: {output_path}\")\n",
        "    plt.show()\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation of Extreme Oracle Bursts\n",
        "\n",
        "**Possible Explanations:**\n",
        "\n",
        "1. **Oracle Spam/Manipulation**: High-frequency oracle updates may be used to:\n",
        "   - Overwhelm the network and reduce trade visibility\n",
        "   - Create artificial price volatility\n",
        "   - Prepare for MEV attacks by manipulating oracle state\n",
        "\n",
        "2. **Network Congestion**: During extreme oracle bursts:\n",
        "   - Network resources may be consumed by oracle updates\n",
        "   - Trade transactions may be delayed or dropped\n",
        "   - Validators may prioritize oracle updates over trades\n",
        "\n",
        "3. **Validator Behavior**: \n",
        "   - Some validators may batch oracle updates\n",
        "   - Validator-specific patterns in oracle processing\n",
        "   - Potential validator collusion or coordination\n",
        "\n",
        "4. **MEV Attack Preparation**:\n",
        "   - Attackers may flood oracle updates to create favorable conditions\n",
        "   - Reduce competition by making trades harder to execute\n",
        "   - Create timing advantages for subsequent attacks\n",
        "\n",
        "**Key Metrics to Investigate:**\n",
        "- Validator concentration during bursts\n",
        "- Signer patterns (same signers repeatedly?)\n",
        "- AMM-specific patterns\n",
        "- Slot distribution (are bursts concentrated in specific slots?)\n",
        "- Temporal patterns (time of day, duration, frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deep Dive: Analyze patterns before and after extreme bursts\n",
        "if len(extreme_bursts) > 0:\n",
        "    print(\"=\"*80)\n",
        "    print(\"DEEP DIVE: PATTERNS BEFORE AND AFTER EXTREME BURSTS\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "    \n",
        "    # Look at time windows before and after bursts\n",
        "    window_before_after = 10  # seconds before/after\n",
        "    \n",
        "    burst_analysis = []\n",
        "    \n",
        "    for burst_time in extreme_bursts['time_window_sec'].head(20):  # Analyze top 20\n",
        "        # Before burst\n",
        "        before_start = burst_time - window_before_after\n",
        "        before_end = burst_time\n",
        "        before_oracles = oracles[\n",
        "            (oracles['time_window_sec'] >= before_start) & \n",
        "            (oracles['time_window_sec'] < before_end)\n",
        "        ]\n",
        "        before_trades = trades[\n",
        "            (trades['time_window_sec'] >= before_start) & \n",
        "            (trades['time_window_sec'] < before_end)\n",
        "        ]\n",
        "        \n",
        "        # During burst\n",
        "        during_oracles = oracles[oracles['time_window_sec'] == burst_time]\n",
        "        during_trades = trades[trades['time_window_sec'] == burst_time]\n",
        "        \n",
        "        # After burst\n",
        "        after_start = burst_time + 1\n",
        "        after_end = burst_time + window_before_after + 1\n",
        "        after_oracles = oracles[\n",
        "            (oracles['time_window_sec'] > burst_time) & \n",
        "            (oracles['time_window_sec'] <= after_end)\n",
        "        ]\n",
        "        after_trades = trades[\n",
        "            (trades['time_window_sec'] > burst_time) & \n",
        "            (trades['time_window_sec'] <= after_end)\n",
        "        ]\n",
        "        \n",
        "        burst_analysis.append({\n",
        "            'burst_time': burst_time,\n",
        "            'before_oracles': len(before_oracles),\n",
        "            'before_trades': len(before_trades),\n",
        "            'during_oracles': len(during_oracles),\n",
        "            'during_trades': len(during_trades),\n",
        "            'after_oracles': len(after_oracles),\n",
        "            'after_trades': len(after_trades),\n",
        "        })\n",
        "    \n",
        "    burst_patterns = pd.DataFrame(burst_analysis)\n",
        "    \n",
        "    print(\"Pattern Analysis (Top 20 Bursts):\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Before bursts - Avg oracles: {burst_patterns['before_oracles'].mean():.1f}, Avg trades: {burst_patterns['before_trades'].mean():.1f}\")\n",
        "    print(f\"During bursts - Avg oracles: {burst_patterns['during_oracles'].mean():.1f}, Avg trades: {burst_patterns['during_trades'].mean():.1f}\")\n",
        "    print(f\"After bursts  - Avg oracles: {burst_patterns['after_oracles'].mean():.1f}, Avg trades: {burst_patterns['after_trades'].mean():.1f}\")\n",
        "    print()\n",
        "    \n",
        "    # Check if trades recover after bursts\n",
        "    trade_recovery = (burst_patterns['after_trades'].mean() - burst_patterns['during_trades'].mean()) / (burst_patterns['before_trades'].mean() + 1)\n",
        "    print(f\"Trade recovery rate: {trade_recovery*100:.1f}% (after vs before)\")\n",
        "    print()\n",
        "    \n",
        "    # Check for validator patterns\n",
        "    if 'validator' in oracles.columns and not oracles['validator'].isna().all():\n",
        "        print(\"Validator Patterns During Bursts:\")\n",
        "        print(\"-\" * 80)\n",
        "        for burst_time in extreme_bursts['time_window_sec'].head(5):\n",
        "            burst_oracles_sample = oracles[oracles['time_window_sec'] == burst_time]\n",
        "            if len(burst_oracles_sample) > 0 and 'validator' in burst_oracles_sample.columns:\n",
        "                top_validator = burst_oracles_sample['validator'].value_counts().head(1)\n",
        "                if len(top_validator) > 0:\n",
        "                    validator, count = top_validator.index[0], top_validator.values[0]\n",
        "                    pct = (count / len(burst_oracles_sample)) * 100\n",
        "                    print(f\"  Burst at {burst_time}: Top validator {validator[:44]}... ({pct:.1f}% of oracles)\")\n",
        "        print()\n",
        "    \n",
        "    # Save pattern analysis\n",
        "    import os\n",
        "    output_dir = 'derived'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    pattern_output = os.path.join(output_dir, 'extreme_burst_patterns.csv')\n",
        "    burst_patterns.to_csv(pattern_output, index=False)\n",
        "    print(f\"✓ Saved burst pattern analysis to: {pattern_output}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Cluster size distribution\n",
        "if len(all_clusters) > 0:\n",
        "    clusters_df = pd.DataFrame(all_clusters)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Plot 1: Cluster size distribution\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.hist(clusters_df['num_trades'], bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax1.set_xlabel('Number of Trades per Cluster', fontsize=11, fontweight='bold')\n",
        "    ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax1.set_title('Distribution of Cluster Sizes', fontsize=12, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Cross-slot vs single-slot\n",
        "    ax2 = axes[0, 1]\n",
        "    slot_type_counts = clusters_df['is_cross_slot'].value_counts()\n",
        "    ax2.pie(slot_type_counts.values, labels=['Single-Slot', 'Cross-Slot'], \n",
        "            autopct='%1.1f%%', startangle=90, colors=['#FF6B6B', '#4ECDC4'])\n",
        "    ax2.set_title('Cross-Slot vs Single-Slot Clusters', fontsize=12, fontweight='bold')\n",
        "    \n",
        "    # Plot 3: Cluster duration distribution\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.hist(clusters_df['duration_ms'], bins=50, edgecolor='black', alpha=0.7, color='#95E1D3')\n",
        "    ax3.set_xlabel('Cluster Duration (ms)', fontsize=11, fontweight='bold')\n",
        "    ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax3.set_title('Distribution of Cluster Durations', fontsize=12, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Trades vs Slots spanned\n",
        "    ax4 = axes[1, 1]\n",
        "    scatter = ax4.scatter(clusters_df['num_trades'], clusters_df['num_slots'], \n",
        "                         c=clusters_df['is_cross_slot'], cmap='RdYlGn', \n",
        "                         alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "    ax4.set_xlabel('Number of Trades', fontsize=11, fontweight='bold')\n",
        "    ax4.set_ylabel('Number of Slots Spanned', fontsize=11, fontweight='bold')\n",
        "    ax4.set_title('Trades vs Slots Spanned (Red=Single-Slot, Green=Cross-Slot)', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=ax4, label='Cross-Slot')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    import os\n",
        "    cluster_viz_path = 'wide_sandwich_cluster_analysis.png'\n",
        "    plt.savefig(cluster_viz_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ Saved cluster analysis visualization to: {cluster_viz_path}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Analysis: Root Causes of MEV Sandwich Attacks\n",
        "\n",
        "This section performs deep analysis to find root causes of sandwich attacks:\n",
        "\n",
        "1. **Lag Correlation Analysis** - Oracle lead/lag TRADE timing (ms)\n",
        "2. **Improved Clustering** - DBSCAN automatic burst detection\n",
        "3. **Pool/Validator Grouping** - Identify \"hotspot\" pools and validators\n",
        "4. **Enhanced Visualization** - Heatmaps and statistical analysis\n",
        "5. **Root Cause Inference** - Deep insights into sandwich attack mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"OPTIMIZED DEEP ANALYSIS: Finding Root Causes of Sandwich Attacks\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Ensure we have the necessary data structures\n",
        "df_clean = df.copy()  # Use the loaded df\n",
        "df_clean = df_clean.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "# Ensure datetime is properly formatted\n",
        "if 'datetime' not in df_clean.columns or df_clean['datetime'].dtype != 'datetime64[ns]':\n",
        "    df_clean['datetime'] = pd.to_datetime(df_clean['datetime'])\n",
        "\n",
        "print(f\"Total events in df_clean: {len(df_clean):,}\")\n",
        "print(f\"ORACLE events: {len(df_clean[df_clean['kind'] == 'ORACLE']):,}\")\n",
        "print(f\"TRADE events: {len(df_clean[df_clean['kind'] == 'TRADE']):,}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1. LAG CORRELATION ANALYSIS: Oracle Lead/Lag TRADE (ms)\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 1: Oracle Lead/Lag TRADE Timing Analysis\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Sort by datetime for proper sequencing\n",
        "df_sorted = df_clean.sort_values('datetime').copy()\n",
        "\n",
        "# Get Oracle and TRADE events separately\n",
        "oracle_events = df_sorted[df_sorted['kind'] == 'ORACLE'].copy()\n",
        "trade_events = df_sorted[df_sorted['kind'] == 'TRADE'].copy()\n",
        "\n",
        "print(f\"Oracle events: {len(oracle_events):,}\")\n",
        "print(f\"TRADE events: {len(trade_events):,}\")\n",
        "print()\n",
        "\n",
        "# Use merge_asof to find the most recent Oracle before each TRADE\n",
        "# This finds the last Oracle update that occurred before each TRADE\n",
        "merged = pd.merge_asof(\n",
        "    trade_events.sort_values('datetime')[['datetime', 'ms_time', 'amm_oracle', 'slot']], \n",
        "    oracle_events.sort_values('datetime')[['datetime', 'ms_time', 'amm_oracle', 'slot']], \n",
        "    on='datetime', \n",
        "    direction='backward', \n",
        "    tolerance=pd.Timedelta('1s'),  # Only match if within 1 second\n",
        "    suffixes=('_trade', '_oracle')\n",
        ")\n",
        "\n",
        "# Calculate Oracle lead time (positive = Oracle before TRADE, negative = Oracle after TRADE)\n",
        "merged['oracle_lead_ms'] = (merged['datetime_trade'] - merged['datetime_oracle']).dt.total_seconds() * 1000\n",
        "\n",
        "# Filter out matches that are too far apart (likely not related)\n",
        "merged = merged[merged['oracle_lead_ms'] >= 0]  # Only Oracle before TRADE\n",
        "merged = merged[merged['oracle_lead_ms'] <= 1000]  # Within 1 second\n",
        "\n",
        "print(\"=== Oracle Lead TRADE Timing Statistics (ms) ===\")\n",
        "print(merged['oracle_lead_ms'].describe())\n",
        "print()\n",
        "print(f\"Oracle 在 TRADE 前 {((merged['oracle_lead_ms'] > 0).sum() / len(merged) * 100):.1f}% 的情况下领先\")\n",
        "if len(merged[merged['oracle_lead_ms'] > 0]) > 0:\n",
        "    print(f\"平均领先时间: {merged[merged['oracle_lead_ms'] > 0]['oracle_lead_ms'].mean():.1f} ms\")\n",
        "    print(f\"中位数领先时间: {merged[merged['oracle_lead_ms'] > 0]['oracle_lead_ms'].median():.1f} ms\")\n",
        "print()\n",
        "\n",
        "# Analyze by time buckets\n",
        "print(\"=== Oracle Lead Time Distribution ===\")\n",
        "bins = [0, 10, 50, 100, 200, 500, 1000]\n",
        "labels = ['0-10ms', '10-50ms', '50-100ms', '100-200ms', '200-500ms', '500-1000ms']\n",
        "merged['lead_bucket'] = pd.cut(merged['oracle_lead_ms'], bins=bins, labels=labels, include_lowest=True)\n",
        "bucket_counts = merged['lead_bucket'].value_counts().sort_index()\n",
        "for bucket, count in bucket_counts.items():\n",
        "    pct = (count / len(merged)) * 100\n",
        "    print(f\"  {bucket}: {count:,} ({pct:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Check if same pool (more relevant for sandwich attacks)\n",
        "if 'amm_oracle_trade' in merged.columns or 'amm_oracle_oracle' in merged.columns:\n",
        "    # Try to match pools\n",
        "    if 'amm_oracle_trade' in merged.columns and 'amm_oracle_oracle' in merged.columns:\n",
        "        same_pool = merged['amm_oracle_trade'] == merged['amm_oracle_oracle']\n",
        "        print(f\"Same pool Oracle-TRADE pairs: {same_pool.sum():,} ({same_pool.mean()*100:.1f}%)\")\n",
        "        if same_pool.sum() > 0:\n",
        "            same_pool_lead = merged[same_pool]['oracle_lead_ms']\n",
        "            print(f\"  Same pool average lead: {same_pool_lead.mean():.1f} ms\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2. IMPROVED CLUSTERING: DBSCAN Automatic Burst Detection\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 2: DBSCAN Automatic Oracle Burst Detection\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "try:\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    \n",
        "    oracle_df = df_clean[df_clean['kind'] == 'ORACLE'].copy()\n",
        "    \n",
        "    if len(oracle_df) > 0:\n",
        "        # Convert datetime to seconds for clustering\n",
        "        oracle_df['time_sec'] = (oracle_df['datetime'].astype('int64') // 10**9).astype(float)\n",
        "        \n",
        "        # DBSCAN on time dimension (eps=1s equivalent, min_samples=5 for burst)\n",
        "        # Scale time to standardize\n",
        "        scaler = StandardScaler()\n",
        "        time_scaled = scaler.fit_transform(oracle_df[['time_sec']])\n",
        "        \n",
        "        # eps=0.5 (scaled) means ~0.5 standard deviations = tight bursts\n",
        "        # min_samples=5 means at least 5 oracles in a burst\n",
        "        db = DBSCAN(eps=0.5, min_samples=5).fit(time_scaled)\n",
        "        oracle_df['burst_cluster'] = db.labels_\n",
        "        \n",
        "        num_bursts = (oracle_df['burst_cluster'] != -1).sum()\n",
        "        num_clusters = oracle_df['burst_cluster'].nunique() - 1  # -1 for noise\n",
        "        \n",
        "        print(f\"=== DBSCAN Burst Detection Results ===\")\n",
        "        print(f\"检测到 {num_clusters} 个 burst clusters (noise 除外)\")\n",
        "        print(f\"Burst 中 Oracle 数量: {num_bursts:,} ({num_bursts/len(oracle_df)*100:.1f}%)\")\n",
        "        print(f\"Noise (非 burst) Oracle 数量: {(oracle_df['burst_cluster'] == -1).sum():,}\")\n",
        "        print()\n",
        "        \n",
        "        # Analyze burst characteristics\n",
        "        if num_clusters > 0:\n",
        "            burst_clusters = oracle_df[oracle_df['burst_cluster'] != -1]\n",
        "            cluster_stats = burst_clusters.groupby('burst_cluster').agg({\n",
        "                'time_sec': ['count', 'min', 'max'],\n",
        "                'datetime': ['min', 'max']\n",
        "            })\n",
        "            cluster_stats.columns = ['oracle_count', 'time_start', 'time_end', 'datetime_start', 'datetime_end']\n",
        "            cluster_stats['duration_sec'] = (cluster_stats['datetime_end'] - cluster_stats['datetime_start']).dt.total_seconds()\n",
        "            \n",
        "            print(\"=== Burst Cluster Statistics ===\")\n",
        "            print(f\"Average oracles per burst: {cluster_stats['oracle_count'].mean():.1f}\")\n",
        "            print(f\"Max oracles in single burst: {cluster_stats['oracle_count'].max()}\")\n",
        "            print(f\"Average burst duration: {cluster_stats['duration_sec'].mean():.2f} seconds\")\n",
        "            print(f\"Max burst duration: {cluster_stats['duration_sec'].max():.2f} seconds\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"No Oracle events found for clustering.\")\n",
        "        print()\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"⚠️  sklearn not available. Skipping DBSCAN clustering.\")\n",
        "    print(\"   Install with: pip install scikit-learn\")\n",
        "    print()\n",
        "    oracle_df = df_clean[df_clean['kind'] == 'ORACLE'].copy()\n",
        "    oracle_df['burst_cluster'] = -1  # Mark all as noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3. POOL / VALIDATOR GROUPING: Find \"Hotspot\" Pools and Validators\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 3: Pool/Validator Grouping - Identifying Hotspots\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Pool (amm_oracle) Oracle Burst Distribution\n",
        "if 'burst_cluster' in oracle_df.columns:\n",
        "    print(\"=== Pool (amm_oracle) Oracle Burst Distribution ===\")\n",
        "    pool_burst = oracle_df[oracle_df['burst_cluster'] != -1].groupby('amm_oracle')['burst_cluster'].nunique()\n",
        "    pool_burst = pool_burst.sort_values(ascending=False)\n",
        "    \n",
        "    if len(pool_burst) > 0:\n",
        "        print(f\"Top 10 灾区 pools by burst cluster count:\")\n",
        "        print(\"-\" * 80)\n",
        "        for pool, count in pool_burst.head(10).items():\n",
        "            total_oracles = len(oracle_df[oracle_df['amm_oracle'] == pool])\n",
        "            burst_oracles = len(oracle_df[(oracle_df['amm_oracle'] == pool) & (oracle_df['burst_cluster'] != -1)])\n",
        "            pct = (burst_oracles / total_oracles * 100) if total_oracles > 0 else 0\n",
        "            print(f\"  {pool[:50]}: {count} burst clusters, {burst_oracles:,}/{total_oracles:,} oracles ({pct:.1f}%)\")\n",
        "        print()\n",
        "        \n",
        "        # Identify \"hotspot\" pools (high burst ratio)\n",
        "        pool_total = oracle_df.groupby('amm_oracle').size()\n",
        "        pool_burst_count = oracle_df[oracle_df['burst_cluster'] != -1].groupby('amm_oracle').size()\n",
        "        pool_burst_ratio = (pool_burst_count / pool_total).fillna(0).sort_values(ascending=False)\n",
        "        \n",
        "        print(\"=== Top 10 Hotspot Pools by Burst Ratio ===\")\n",
        "        print(\"-\" * 80)\n",
        "        for pool, ratio in pool_burst_ratio.head(10).items():\n",
        "            total = pool_total[pool]\n",
        "            burst = pool_burst_count.get(pool, 0)\n",
        "            print(f\"  {pool[:50]}: {ratio*100:.1f}% burst ratio ({burst:,}/{total:,} oracles)\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No burst clusters found in pools.\")\n",
        "        print()\n",
        "\n",
        "# Validator analysis (if available)\n",
        "if 'validator' in df_clean.columns and not df_clean['validator'].isna().all():\n",
        "    print(\"=== Validator Oracle Burst Distribution ===\")\n",
        "    val_burst = oracle_df[oracle_df['burst_cluster'] != -1].groupby('validator').size()\n",
        "    val_burst = val_burst.sort_values(ascending=False)\n",
        "    \n",
        "    if len(val_burst) > 0:\n",
        "        print(\"Top 10 Validators by Oracle Burst Count:\")\n",
        "        print(\"-\" * 80)\n",
        "        for validator, count in val_burst.head(10).items():\n",
        "            total_oracles = len(oracle_df[oracle_df['validator'] == validator])\n",
        "            pct = (count / total_oracles * 100) if total_oracles > 0 else 0\n",
        "            print(f\"  {str(validator)[:50]}: {count:,} burst oracles ({pct:.1f}% of {total_oracles:,} total)\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"No burst clusters found for validators.\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"⚠️  Validator column not available or all NaN. Skipping validator analysis.\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. ENHANCED VISUALIZATION: Oracle-TRADE Lag Heatmap and Distributions\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 4: Enhanced Visualizations\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig = plt.figure(figsize=(18, 14))\n",
        "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Oracle Lead TRADE Lag Distribution\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "if len(merged) > 0:\n",
        "    sns.histplot(merged['oracle_lead_ms'], bins=50, kde=True, color='#FF6B6B', ax=ax1)\n",
        "    ax1.axvline(0, color='black', linestyle='--', linewidth=1, label='Zero lag')\n",
        "    ax1.axvline(merged['oracle_lead_ms'].median(), color='blue', linestyle='--', linewidth=1, \n",
        "                label=f'Median: {merged[\"oracle_lead_ms\"].median():.1f}ms')\n",
        "    ax1.set_xlabel('Oracle Lead Time (ms)', fontsize=11, fontweight='bold')\n",
        "    ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax1.set_title('Oracle Lead TRADE 滞后分布 (ms)\\n红峰 = 典型 Sandwich 前导', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax1.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax1.transAxes)\n",
        "    ax1.set_title('Oracle Lead TRADE Lag Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 2: Lead Time by Bucket\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "if len(merged) > 0 and 'lead_bucket' in merged.columns:\n",
        "    bucket_counts = merged['lead_bucket'].value_counts().sort_index()\n",
        "    colors = plt.cm.Reds(np.linspace(0.4, 0.9, len(bucket_counts)))\n",
        "    ax2.bar(range(len(bucket_counts)), bucket_counts.values, color=colors, edgecolor='black')\n",
        "    ax2.set_xticks(range(len(bucket_counts)))\n",
        "    ax2.set_xticklabels(bucket_counts.index, rotation=45, ha='right')\n",
        "    ax2.set_xlabel('Lead Time Bucket', fontsize=11, fontweight='bold')\n",
        "    ax2.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
        "    ax2.set_title('Oracle Lead Time Distribution by Bucket', fontsize=12, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "else:\n",
        "    ax2.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax2.transAxes)\n",
        "    ax2.set_title('Oracle Lead Time by Bucket', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 3: Burst Cluster Size Distribution\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "if 'burst_cluster' in oracle_df.columns and (oracle_df['burst_cluster'] != -1).any():\n",
        "    burst_clusters = oracle_df[oracle_df['burst_cluster'] != -1]\n",
        "    cluster_sizes = burst_clusters.groupby('burst_cluster').size()\n",
        "    ax3.hist(cluster_sizes.values, bins=30, edgecolor='black', alpha=0.7, color='#4ECDC4')\n",
        "    ax3.set_xlabel('Oracles per Burst Cluster', fontsize=11, fontweight='bold')\n",
        "    ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax3.set_title('DBSCAN Burst Cluster Size Distribution', fontsize=12, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "else:\n",
        "    ax3.text(0.5, 0.5, 'No burst clusters detected', ha='center', va='center', transform=ax3.transAxes)\n",
        "    ax3.set_title('Burst Cluster Size Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 4: Top Pool Burst Analysis\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "if 'burst_cluster' in oracle_df.columns and (oracle_df['burst_cluster'] != -1).any():\n",
        "    pool_burst_counts = oracle_df[oracle_df['burst_cluster'] != -1].groupby('amm_oracle').size()\n",
        "    top_pools = pool_burst_counts.nlargest(10)\n",
        "    if len(top_pools) > 0:\n",
        "        colors = plt.cm.Oranges(np.linspace(0.4, 0.9, len(top_pools)))\n",
        "        ax4.barh(range(len(top_pools)), top_pools.values, color=colors, edgecolor='black')\n",
        "        ax4.set_yticks(range(len(top_pools)))\n",
        "        ax4.set_yticklabels([p[:40] + '...' if len(p) > 40 else p for p in top_pools.index], fontsize=9)\n",
        "        ax4.set_xlabel('Burst Oracle Count', fontsize=11, fontweight='bold')\n",
        "        ax4.set_title('Top 10 Pools by Burst Oracle Count', fontsize=12, fontweight='bold')\n",
        "        ax4.grid(True, alpha=0.3, axis='x')\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'No pool burst data', ha='center', va='center', transform=ax4.transAxes)\n",
        "        ax4.set_title('Top Pools by Burst Count', fontsize=12, fontweight='bold')\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'No burst clusters detected', ha='center', va='center', transform=ax4.transAxes)\n",
        "    ax4.set_title('Top Pools by Burst Count', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 5: Time Series of Oracle Bursts\n",
        "ax5 = fig.add_subplot(gs[2, :])\n",
        "if 'burst_cluster' in oracle_df.columns and (oracle_df['burst_cluster'] != -1).any():\n",
        "    burst_clusters = oracle_df[oracle_df['burst_cluster'] != -1].copy()\n",
        "    # Sample if too many points\n",
        "    if len(burst_clusters) > 10000:\n",
        "        burst_clusters = burst_clusters.sample(10000).sort_values('datetime')\n",
        "    \n",
        "    # Plot bursts over time\n",
        "    ax5.scatter(burst_clusters['datetime'], burst_clusters['burst_cluster'], \n",
        "               alpha=0.3, s=10, c=burst_clusters['burst_cluster'], cmap='viridis')\n",
        "    ax5.set_xlabel('Time', fontsize=11, fontweight='bold')\n",
        "    ax5.set_ylabel('Burst Cluster ID', fontsize=11, fontweight='bold')\n",
        "    ax5.set_title('Oracle Burst Clusters Over Time', fontsize=12, fontweight='bold')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    ax5.tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    ax5.text(0.5, 0.5, 'No burst clusters detected', ha='center', va='center', transform=ax5.transAxes)\n",
        "    ax5.set_title('Oracle Burst Clusters Over Time', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Deep Analysis: Oracle-TRADE Timing and Burst Patterns', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "\n",
        "# Save visualization\n",
        "output_path = 'oracle_deep_analysis_visualizations.png'\n",
        "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Saved enhanced visualizations to: {output_path}\")\n",
        "plt.show()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 5. ROOT CAUSE INFERENCE: Deep Insights Summary\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS 5: Root Cause Inference - Deep Insights\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "print(\"=== 深层次原因推断 ===\")\n",
        "print()\n",
        "\n",
        "# Inference 1: Oracle Lead TRADE Analysis\n",
        "if len(merged) > 0:\n",
        "    avg_lead = merged['oracle_lead_ms'].mean()\n",
        "    median_lead = merged['oracle_lead_ms'].median()\n",
        "    pct_leading = (merged['oracle_lead_ms'] > 0).mean() * 100\n",
        "    \n",
        "    if avg_lead > 0 and pct_leading > 50:\n",
        "        print(\"✓ Oracle 经常领先 TRADE → Bot 利用 Oracle 延迟 frontrun/backrun (sandwich 根源)\")\n",
        "        print(f\"  - 平均领先时间: {avg_lead:.1f} ms\")\n",
        "        print(f\"  - 中位数领先时间: {median_lead:.1f} ms\")\n",
        "        print(f\"  - {pct_leading:.1f}% 的 TRADE 前有 Oracle 更新\")\n",
        "        \n",
        "        # Check for typical sandwich timing (100-500ms)\n",
        "        sandwich_timing = merged[(merged['oracle_lead_ms'] >= 100) & (merged['oracle_lead_ms'] <= 500)]\n",
        "        if len(sandwich_timing) > 0:\n",
        "            pct_sandwich = (len(sandwich_timing) / len(merged)) * 100\n",
        "            print(f\"  - {pct_sandwich:.1f}% 的 Oracle-TRADE 对在典型 sandwich 时间窗口 (100-500ms)\")\n",
        "        print()\n",
        "    else:\n",
        "        print(\"⚠️  Oracle 领先模式不明显，可能需要进一步分析\")\n",
        "        print()\n",
        "\n",
        "# Inference 2: Pool Hotspot Analysis\n",
        "if 'burst_cluster' in oracle_df.columns and (oracle_df['burst_cluster'] != -1).any():\n",
        "    pool_burst_counts = oracle_df[oracle_df['burst_cluster'] != -1].groupby('amm_oracle').size()\n",
        "    pool_total_counts = oracle_df.groupby('amm_oracle').size()\n",
        "    pool_burst_ratio = (pool_burst_counts / pool_total_counts).fillna(0)\n",
        "    \n",
        "    if len(pool_burst_ratio) > 0:\n",
        "        max_ratio = pool_burst_ratio.max()\n",
        "        mean_ratio = pool_burst_ratio.mean()\n",
        "        \n",
        "        if max_ratio > mean_ratio * 2:\n",
        "            hotspot_pools = pool_burst_ratio[pool_burst_ratio > mean_ratio * 2]\n",
        "            print(\"✓ 某些 Pool Oracle burst 集中 → '一重灾区' pool 被针对跨池夹击\")\n",
        "            print(f\"  - 最高 burst 比例: {max_ratio*100:.1f}%\")\n",
        "            print(f\"  - 平均 burst 比例: {mean_ratio*100:.1f}%\")\n",
        "            print(f\"  - 热点 pool 数量: {len(hotspot_pools)}\")\n",
        "            if len(hotspot_pools) > 0:\n",
        "                print(f\"  - Top 3 热点 pools:\")\n",
        "                for pool, ratio in hotspot_pools.nlargest(3).items():\n",
        "                    print(f\"    • {pool[:50]}: {ratio*100:.1f}% burst ratio\")\n",
        "            print()\n",
        "        else:\n",
        "            print(\"⚠️  Pool burst 分布相对均匀，无明显热点\")\n",
        "            print()\n",
        "\n",
        "# Inference 3: Burst Cluster Analysis\n",
        "if 'burst_cluster' in oracle_df.columns:\n",
        "    num_clusters = oracle_df['burst_cluster'].nunique() - 1  # Exclude noise\n",
        "    burst_ratio = (oracle_df['burst_cluster'] != -1).mean() * 100\n",
        "    \n",
        "    if num_clusters > 50:\n",
        "        print(\"✓ 高 burst clusters → Bot spam Oracle 制造机会\")\n",
        "        print(f\"  - 检测到 {num_clusters} 个 burst clusters\")\n",
        "        print(f\"  - {burst_ratio:.1f}% 的 Oracle 事件属于 burst\")\n",
        "        print()\n",
        "    elif num_clusters > 0:\n",
        "        print(f\"ℹ️  检测到 {num_clusters} 个 burst clusters ({burst_ratio:.1f}% Oracle 事件)\")\n",
        "        print()\n",
        "\n",
        "# Inference 4: Cross-Slot Analysis (if available)\n",
        "if 'slot' in merged.columns:\n",
        "    # Check if Oracle and TRADE are in different slots (cross-slot sandwich)\n",
        "    if 'slot_trade' in merged.columns and 'slot_oracle' in merged.columns:\n",
        "        cross_slot = merged['slot_trade'] != merged['slot_oracle']\n",
        "        if cross_slot.any():\n",
        "            pct_cross = cross_slot.mean() * 100\n",
        "            print(f\"ℹ️  {pct_cross:.1f}% 的 Oracle-TRADE 对跨 slot (cross-slot sandwich 可能性)\")\n",
        "            print()\n",
        "\n",
        "# Summary Recommendations\n",
        "print(\"=\"*80)\n",
        "print(\"建议与下一步分析:\")\n",
        "print(\"=\"*80)\n",
        "print(\"1. 结合 mev_score 高样本，确认这些 burst 是否对应高 profit sandwich\")\n",
        "print(\"2. 分析热点 pool 的交易模式，验证是否被系统性攻击\")\n",
        "print(\"3. 检查验证节点延迟，确认 cross-slot sandwich 的成功率\")\n",
        "print(\"4. 量化 Oracle 更新延迟对 sandwich 利润的影响\")\n",
        "print(\"5. 对比不同 pool 的 Oracle 更新频率，找出异质性原因\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DEEP ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

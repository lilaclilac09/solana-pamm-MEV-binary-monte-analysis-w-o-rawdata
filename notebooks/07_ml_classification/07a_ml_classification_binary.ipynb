{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binary Classification for MEV Detection\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook converts the multi-class MEV detection problem to **binary classification** (MEV = 1, Non-MEV = 0) to better handle imbalanced data and improve model performance.\n",
        "\n",
        "### Why Binary Classification?\n",
        "\n",
        "1. **Handles Imbalance Better**: MEV class (~20k / 5.5M) is still minority, but binary classification makes weighting/sampling simpler\n",
        "2. **Model Simplification**: SVM/Logistic won't skip (classes = 2). GMM/Isolation Forest still applicable (anomaly = MEV)\n",
        "3. **Better Evaluation**: Use PR-AUC or F1 (MEV class), ignore accuracy for imbalanced data\n",
        "4. **Aggregates Non-MEV**: All other classes (Aggregator, Wash Trading, Regular) become Non-MEV, making majority class more stable\n",
        "\n",
        "### Dataset\n",
        "\n",
        "**Source**: `pamm_clean_final.parquet` (5.5M+ transaction records)\n",
        "\n",
        "**Features**: \n",
        "- `total_trades`, `trades_per_hour`, `aggregator_likelihood`\n",
        "- `late_slot_ratio`, `oracle_backrun_ratio`, `high_bytes_ratio`\n",
        "- `cluster_ratio`, `mev_score`, `wash_trading_score`\n",
        "\n",
        "**Binary Labels**:\n",
        "- **MEV (1)**: `LIKELY MEV BOT`, `POSSIBLE MEV`\n",
        "- **Non-MEV (0)**: `LIKELY AGGREGATOR`, `LIKELY WASH TRADING`, `REGULAR TRADE BOT`\n",
        "\n",
        "### Methods Tested\n",
        "\n",
        "1. **Gaussian Mixture Model (GMM)** - Unsupervised clustering (n_components=2)\n",
        "2. **Isolation Forest** - Unsupervised anomaly detection (contamination = MEV ratio)\n",
        "3. **Random Forest** - Supervised classification with class weights\n",
        "4. **XGBoost** - Advanced gradient boosting with scale_pos_weight\n",
        "5. **Support Vector Machine (SVM)** - Decision boundary optimization\n",
        "6. **Logistic Regression** - Interpretable linear baseline\n",
        "\n",
        "### Key Improvements\n",
        "\n",
        "- **SMOTE Oversampling**: Balances training data (target ratio 1:2 MEV:Non-MEV)\n",
        "- **Class Weights**: All models use balanced weights or scale_pos_weight\n",
        "- **PR-AUC Focus**: Precision-Recall curves for imbalanced data\n",
        "- **Monte Carlo Simulation**: Bootstrap validation for model stability\n",
        "- **Parameter Optimization**: GridSearchCV for hyperparameter tuning\n",
        "\n",
        "### Target Metrics\n",
        "\n",
        "- **MEV Recall > 0.7**: Catch most MEV bots\n",
        "- **MEV F1 > 0.5**: Balanced precision and recall\n",
        "- **PR-AUC > 0.6**: Good performance on imbalanced data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Binary Classification for MEV Detection\n",
        "Converts multi-class classification to binary (MEV = 1, Non-MEV = 0)\n",
        "\"\"\"\n",
        "# Install required packages if missing\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip\"\"\"\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "\n",
        "# Check and install imbalanced-learn if needed\n",
        "try:\n",
        "    import imblearn\n",
        "except ImportError:\n",
        "    print(\"Installing imbalanced-learn...\")\n",
        "    install_package(\"imbalanced-learn\")\n",
        "    print(\"✓ imbalanced-learn installed successfully\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve, PrecisionRecallDisplay, average_precision_score\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import warnings\n",
        "import json\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('derived/ml_results_binary', exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"BINARY CLASSIFICATION FOR MEV DETECTION\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Binary Label Conversion\n",
        "\n",
        "### What This Section Does:\n",
        "\n",
        "1. **Loads Transaction Data**: Reads the cleaned parquet file with 5.5M+ transaction records\n",
        "2. **Feature Engineering**: Creates signer-level features (same as original notebook)\n",
        "3. **Binary Label Conversion**: Converts multi-class labels to binary (MEV=1, Non-MEV=0)\n",
        "4. **Class Distribution Analysis**: Shows imbalance ratio and distribution\n",
        "\n",
        "### Binary Label Mapping:\n",
        "\n",
        "- **MEV (1)**: `LIKELY MEV BOT`, `POSSIBLE MEV (Sandwich patterns)`, `POSSIBLE MEV (Cluster patterns)`\n",
        "- **Non-MEV (0)**: `LIKELY AGGREGATOR`, `LIKELY WASH TRADING`, `REGULAR TRADE BOT / UNKNOWN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"1. DATA LOADING AND BINARY LABEL CONVERSION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Load cleaned transaction data\n",
        "print(\"Loading cleaned transaction data...\")\n",
        "try:\n",
        "    # Try multiple possible paths\n",
        "    possible_paths = [\n",
        "        '../01_data_cleaning/outputs/pamm_clean_final.parquet',\n",
        "        '../../01_data_cleaning/outputs/pamm_clean_final.parquet',\n",
        "        '/Users/aileen/Downloads/pamm/pamm_clean_final.parquet',\n",
        "        'pamm_clean_final.parquet'\n",
        "    ]\n",
        "    \n",
        "    df_clean = None\n",
        "    for path in possible_paths:\n",
        "        try:\n",
        "            df_clean = pd.read_parquet(path)\n",
        "            print(f\"✓ Loaded from: {path}\")\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    if df_clean is None:\n",
        "        raise FileNotFoundError(\"Could not find pamm_clean_final.parquet in any expected location\")\n",
        "    \n",
        "    print(f\"✓ Loaded {len(df_clean):,} transaction records\")\n",
        "    print(f\"✓ Time range: {df_clean['datetime'].min()} to {df_clean['datetime'].max()}\")\n",
        "    print(f\"✓ Event types: {df_clean['kind'].value_counts().to_dict()}\")\n",
        "    print()\n",
        "    \n",
        "    # Filter to TRADE events only\n",
        "    df_trades = df_clean[df_clean['kind'] == 'TRADE'].copy()\n",
        "    print(f\"✓ TRADE events: {len(df_trades):,} records\")\n",
        "    print(f\"✓ Unique signers: {df_trades['signer'].nunique():,}\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Feature Engineering (same as original notebook)\n",
        "print(\"Engineering features from transaction data...\")\n",
        "print(\"This may take a few minutes for large datasets...\")\n",
        "\n",
        "signer_features = []\n",
        "unique_signers = df_trades['signer'].unique()\n",
        "print(f\"Processing {len(unique_signers):,} unique signers...\")\n",
        "\n",
        "# Sample if too large\n",
        "if len(unique_signers) > 5000:\n",
        "    print(f\"⚠️  Large dataset detected. Sampling 5,000 signers for faster processing...\")\n",
        "    np.random.seed(42)\n",
        "    unique_signers = np.random.choice(unique_signers, size=5000, replace=False)\n",
        "    print(f\"Processing {len(unique_signers):,} sampled signers...\")\n",
        "\n",
        "print(\"Processing signers...\")\n",
        "for i, signer in enumerate(unique_signers):\n",
        "    if (i + 1) % 500 == 0:\n",
        "        print(f\"  Processed {i+1}/{len(unique_signers)} signers...\")\n",
        "    signer_trades = df_trades[df_trades['signer'] == signer].copy()\n",
        "    \n",
        "    if len(signer_trades) < 2:\n",
        "        continue\n",
        "    \n",
        "    total_trades = len(signer_trades)\n",
        "    time_span_hours = (signer_trades['datetime'].max() - signer_trades['datetime'].min()).total_seconds() / 3600\n",
        "    trades_per_hour = total_trades / max(time_span_hours, 0.1)\n",
        "    \n",
        "    late_slot_trades = (signer_trades['us_since_first_shred'] > 300000).sum()\n",
        "    late_slot_ratio = late_slot_trades / total_trades\n",
        "    \n",
        "    signer_slots = signer_trades['slot'].unique()\n",
        "    oracle_backrun_count = 0\n",
        "    slot_oracles = df_clean[(df_clean['slot'].isin(signer_slots)) & \n",
        "                           (df_clean['kind'] == 'ORACLE')][['slot', 'ms_time']]\n",
        "    \n",
        "    if len(slot_oracles) > 0:\n",
        "        oracle_by_slot = slot_oracles.groupby('slot')['ms_time'].apply(list).to_dict()\n",
        "        for _, trade in signer_trades.iterrows():\n",
        "            slot = trade['slot']\n",
        "            trade_time = trade['ms_time']\n",
        "            if slot in oracle_by_slot:\n",
        "                oracle_times = oracle_by_slot[slot]\n",
        "                time_diffs = [abs(ot - trade_time) for ot in oracle_times]\n",
        "                if min(time_diffs) < 50:\n",
        "                    oracle_backrun_count += 1\n",
        "    oracle_backrun_ratio = oracle_backrun_count / total_trades if total_trades > 0 else 0\n",
        "    \n",
        "    high_bytes_trades = (signer_trades['bytes_changed_trade'] > 50).sum()\n",
        "    high_bytes_ratio = high_bytes_trades / total_trades if total_trades > 0 else 0\n",
        "    \n",
        "    slot_counts = signer_trades.groupby('slot').size()\n",
        "    clustered_slots = (slot_counts >= 2).sum()\n",
        "    cluster_ratio = clustered_slots / signer_trades['slot'].nunique() if signer_trades['slot'].nunique() > 0 else 0\n",
        "    \n",
        "    signer_slots = signer_trades['slot'].unique()\n",
        "    sample_size = min(100, len(signer_slots))\n",
        "    \n",
        "    if sample_size > 0:\n",
        "        sampled_slots = np.random.choice(signer_slots, size=sample_size, replace=False)\n",
        "        slot_trade_counts = df_trades[df_trades['slot'].isin(sampled_slots)].groupby('slot').agg({\n",
        "            'signer': ['count', 'nunique']\n",
        "        }).reset_index()\n",
        "        slot_trade_counts.columns = ['slot', 'total_trades', 'unique_signers']\n",
        "        slot_trade_counts = slot_trade_counts[slot_trade_counts['total_trades'] > 1]\n",
        "        \n",
        "        if len(slot_trade_counts) > 0:\n",
        "            slot_trade_counts['unique_ratio'] = slot_trade_counts['unique_signers'] / slot_trade_counts['total_trades']\n",
        "            aggregator_slots = (slot_trade_counts['unique_ratio'] > 0.7).sum()\n",
        "            aggregator_likelihood = aggregator_slots / len(slot_trade_counts)\n",
        "        else:\n",
        "            aggregator_likelihood = 0\n",
        "    else:\n",
        "        aggregator_likelihood = 0\n",
        "    \n",
        "    mev_score = (late_slot_ratio * 0.3 + \n",
        "                 oracle_backrun_ratio * 0.3 + \n",
        "                 high_bytes_ratio * 0.2 + \n",
        "                 cluster_ratio * 0.2)\n",
        "    \n",
        "    wash_trading_score = trades_per_hour / max(mev_score + 0.1, 0.1)\n",
        "    \n",
        "    # Classification logic\n",
        "    if aggregator_likelihood > 0.5:\n",
        "        classification = \"LIKELY AGGREGATOR (Jupiter, etc.)\"\n",
        "    elif wash_trading_score > 1.0 and mev_score < 0.2:\n",
        "        classification = \"LIKELY WASH TRADING (Volume Inflation)\"\n",
        "    elif mev_score > 0.3:\n",
        "        classification = \"LIKELY MEV BOT\"\n",
        "    elif cluster_ratio > 0.3:\n",
        "        classification = \"POSSIBLE MEV (Sandwich patterns)\"\n",
        "    else:\n",
        "        classification = \"REGULAR TRADE BOT / UNKNOWN\"\n",
        "    \n",
        "    signer_features.append({\n",
        "        'signer': signer,\n",
        "        'total_trades': total_trades,\n",
        "        'trades_per_hour': trades_per_hour,\n",
        "        'aggregator_likelihood': aggregator_likelihood,\n",
        "        'late_slot_ratio': late_slot_ratio,\n",
        "        'oracle_backrun_ratio': oracle_backrun_ratio,\n",
        "        'high_bytes_ratio': high_bytes_ratio,\n",
        "        'cluster_ratio': cluster_ratio,\n",
        "        'mev_score': mev_score,\n",
        "        'wash_trading_score': wash_trading_score,\n",
        "        'classification': classification\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(signer_features)\n",
        "print(f\"✓ Created features for {len(df)} signers\")\n",
        "print()\n",
        "\n",
        "# Feature selection\n",
        "feature_cols = [\n",
        "    'total_trades', 'trades_per_hour', 'aggregator_likelihood',\n",
        "    'late_slot_ratio', 'oracle_backrun_ratio', 'high_bytes_ratio',\n",
        "    'cluster_ratio', 'mev_score', 'wash_trading_score'\n",
        "]\n",
        "\n",
        "df_clean_features = df[feature_cols + ['classification']].dropna()\n",
        "\n",
        "if len(df_clean_features) == 0:\n",
        "    raise ValueError(\"❌ Error: No data remaining after cleaning.\")\n",
        "\n",
        "print(f\"✓ Cleaned dataset: {len(df_clean_features)} records\")\n",
        "print(f\"✓ Features: {len(feature_cols)}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# BINARY LABEL CONVERSION\n",
        "# ============================================================================\n",
        "print(\"Converting multi-class labels to binary...\")\n",
        "\n",
        "# Define MEV labels (class = 1)\n",
        "mev_labels = [\n",
        "    'LIKELY MEV BOT',\n",
        "    'POSSIBLE MEV (Sandwich patterns)',\n",
        "    'POSSIBLE MEV (Cluster patterns)',\n",
        "    'LIKELY MEV BOT (Fat Sandwich)'\n",
        "]\n",
        "\n",
        "# Create binary label: 1 = MEV, 0 = Non-MEV\n",
        "df_clean_features['binary_label'] = df_clean_features['classification'].apply(\n",
        "    lambda x: 1 if x in mev_labels else 0\n",
        ")\n",
        "\n",
        "# Check distribution\n",
        "print(\"\\nBinary Class Distribution:\")\n",
        "binary_dist = Counter(df_clean_features['binary_label'])\n",
        "for label, count in sorted(binary_dist.items()):\n",
        "    label_name = 'MEV' if label == 1 else 'Non-MEV'\n",
        "    print(f\"  {label_name} ({label}): {count:,} samples ({count/len(df_clean_features)*100:.2f}%)\")\n",
        "\n",
        "imbalance_ratio = binary_dist[0] / binary_dist[1] if 1 in binary_dist else float('inf')\n",
        "print(f\"\\nImbalance Ratio (Non-MEV:MEV): {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "# Prepare X and y\n",
        "X = df_clean_features[feature_cols]\n",
        "y = df_clean_features['binary_label'].values\n",
        "\n",
        "print(f\"\\n✓ Features shape: {X.shape}\")\n",
        "print(f\"✓ Labels shape: {y.shape}\")\n",
        "print(f\"✓ MEV samples: {np.sum(y == 1)} ({np.sum(y == 1)/len(y)*100:.2f}%)\")\n",
        "print(f\"✓ Non-MEV samples: {np.sum(y == 0)} ({np.sum(y == 0)/len(y)*100:.2f}%)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. SMOTE Oversampling and Train/Test Split\n",
        "\n",
        "### What This Section Does:\n",
        "\n",
        "1. **Train/Test Split**: Splits data 80/20 with stratification\n",
        "2. **SMOTE Oversampling**: Balances training data (target ratio 1:2 MEV:Non-MEV)\n",
        "3. **Feature Scaling**: Standardizes features for better model performance\n",
        "\n",
        "### SMOTE Configuration:\n",
        "\n",
        "- **sampling_strategy=0.5**: Target ratio of MEV:Non-MEV = 1:2 (instead of 1:10+)\n",
        "- This helps models learn MEV patterns without overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"2. SMOTE OVERSAMPLING AND TRAIN/TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Train/test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"  - MEV: {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.2f}%)\")\n",
        "print(f\"  - Non-MEV: {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.2f}%)\")\n",
        "print(f\"\\nTest set: {len(X_test)} samples\")\n",
        "print(f\"  - MEV: {np.sum(y_test == 1)} ({np.sum(y_test == 1)/len(y_test)*100:.2f}%)\")\n",
        "print(f\"  - Non-MEV: {np.sum(y_test == 0)} ({np.sum(y_test == 0)/len(y_test)*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply SMOTE for binary classification\n",
        "print(\"Applying SMOTE oversampling...\")\n",
        "print(f\"Before SMOTE - Training distribution: {Counter(y_train)}\")\n",
        "\n",
        "# SMOTE with sampling_strategy=0.5 (target ratio MEV:Non-MEV = 1:2)\n",
        "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"After SMOTE - Training distribution: {Counter(y_train_res)}\")\n",
        "print(f\"  - MEV: {np.sum(y_train_res == 1)} ({np.sum(y_train_res == 1)/len(y_train_res)*100:.2f}%)\")\n",
        "print(f\"  - Non-MEV: {np.sum(y_train_res == 0)} ({np.sum(y_train_res == 0)/len(y_train_res)*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# Calculate scale_pos_weight for XGBoost\n",
        "scale_pos_weight = Counter(y_train_res)[0] / Counter(y_train_res)[1] if Counter(y_train_res)[1] > 0 else 1\n",
        "print(f\"XGBoost scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training - Binary Classification\n",
        "\n",
        "### Models:\n",
        "\n",
        "1. **Random Forest** - With class_weight='balanced'\n",
        "2. **XGBoost** - With scale_pos_weight for imbalanced data\n",
        "3. **SVM** - With class_weight='balanced'\n",
        "4. **Logistic Regression** - With class_weight='balanced'\n",
        "5. **GMM** - Unsupervised clustering (n_components=2)\n",
        "6. **Isolation Forest** - Anomaly detection (contamination = MEV ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"3. MODEL TRAINING - BINARY CLASSIFICATION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Store all models and predictions\n",
        "models = {}\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "# ============================================================================\n",
        "# 1. RANDOM FOREST\n",
        "# ============================================================================\n",
        "print(\"1. RANDOM FOREST\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100, \n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    max_depth=10\n",
        ")\n",
        "rf.fit(X_train_res, y_train_res)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test_scaled)\n",
        "y_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "models['Random Forest'] = rf\n",
        "predictions['Random Forest'] = y_pred_rf\n",
        "probabilities['Random Forest'] = y_proba_rf\n",
        "\n",
        "print(\"✓ Training complete\")\n",
        "print(f\"  - Train Accuracy: {rf.score(X_train_res, y_train_res):.4f}\")\n",
        "print(f\"  - Test Accuracy: {rf.score(X_test_scaled, y_test):.4f}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 2. XGBOOST\n",
        "# ============================================================================\n",
        "print(\"2. XGBOOST\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "y_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "models['XGBoost'] = xgb_model\n",
        "predictions['XGBoost'] = y_pred_xgb\n",
        "probabilities['XGBoost'] = y_proba_xgb\n",
        "\n",
        "print(\"✓ Training complete\")\n",
        "print(f\"  - Train Accuracy: {xgb_model.score(X_train_res, y_train_res):.4f}\")\n",
        "print(f\"  - Test Accuracy: {xgb_model.score(X_test_scaled, y_test):.4f}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. SVM\n",
        "# ============================================================================\n",
        "print(\"3. SUPPORT VECTOR MACHINE (SVM)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "svm = SVC(\n",
        "    kernel='rbf',\n",
        "    class_weight='balanced',\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "svm.fit(X_train_res, y_train_res)\n",
        "\n",
        "y_pred_svm = svm.predict(X_test_scaled)\n",
        "y_proba_svm = svm.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "models['SVM'] = svm\n",
        "predictions['SVM'] = y_pred_svm\n",
        "probabilities['SVM'] = y_proba_svm\n",
        "\n",
        "print(\"✓ Training complete\")\n",
        "print(f\"  - Train Accuracy: {svm.score(X_train_res, y_train_res):.4f}\")\n",
        "print(f\"  - Test Accuracy: {svm.score(X_test_scaled, y_test):.4f}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 4. LOGISTIC REGRESSION\n",
        "# ============================================================================\n",
        "print(\"4. LOGISTIC REGRESSION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    max_iter=1000\n",
        ")\n",
        "lr.fit(X_train_res, y_train_res)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "y_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "models['Logistic Regression'] = lr\n",
        "predictions['Logistic Regression'] = y_pred_lr\n",
        "probabilities['Logistic Regression'] = y_proba_lr\n",
        "\n",
        "print(\"✓ Training complete\")\n",
        "print(f\"  - Train Accuracy: {lr.score(X_train_res, y_train_res):.4f}\")\n",
        "print(f\"  - Test Accuracy: {lr.score(X_test_scaled, y_test):.4f}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 5. GAUSSIAN MIXTURE MODEL (GMM)\n",
        "# ============================================================================\n",
        "print(\"5. GAUSSIAN MIXTURE MODEL (GMM)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# For binary classification, use n_components=2\n",
        "gmm = GaussianMixture(n_components=2, random_state=42)\n",
        "gmm.fit(X_train_res)\n",
        "\n",
        "# Predict clusters\n",
        "train_clusters = gmm.predict(X_train_res)\n",
        "test_clusters = gmm.predict(X_test_scaled)\n",
        "\n",
        "# Map clusters to binary labels using majority voting\n",
        "cluster_to_label = {}\n",
        "for cluster_id in range(2):\n",
        "    cluster_mask = (train_clusters == cluster_id)\n",
        "    cluster_labels = y_train_res[cluster_mask]\n",
        "    if len(cluster_labels) > 0:\n",
        "        most_common_label = np.bincount(cluster_labels).argmax()\n",
        "        cluster_to_label[cluster_id] = most_common_label\n",
        "    else:\n",
        "        cluster_to_label[cluster_id] = 0\n",
        "\n",
        "# Map test clusters to binary labels\n",
        "y_pred_gmm = np.array([cluster_to_label.get(c, 0) for c in test_clusters])\n",
        "\n",
        "# Get probabilities (use log probabilities)\n",
        "log_proba = gmm.score_samples(X_test_scaled)\n",
        "# Normalize to [0, 1] range for probability\n",
        "y_proba_gmm = 1 / (1 + np.exp(-log_proba))  # Simple sigmoid transformation\n",
        "\n",
        "models['GMM'] = gmm\n",
        "predictions['GMM'] = y_pred_gmm\n",
        "probabilities['GMM'] = y_proba_gmm\n",
        "\n",
        "print(\"✓ Training complete\")\n",
        "print(f\"  - Cluster-to-label mapping: {cluster_to_label}\")\n",
        "print(f\"  - Test Accuracy: {accuracy_score(y_test, y_pred_gmm):.4f}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# 6. ISOLATION FOREST\n",
        "# ============================================================================\n",
        "print(\"6. ISOLATION FOREST\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Set contamination to MEV ratio\n",
        "mev_ratio = np.sum(y_train_res == 1) / len(y_train_res)\n",
        "iso_forest = IsolationForest(\n",
        "    contamination=mev_ratio,\n",
        "    random_state=42\n",
        ")\n",
        "iso_forest.fit(X_train_res)\n",
        "\n",
        "# Predict anomalies (-1 = anomaly/MEV, 1 = normal/Non-MEV)\n",
        "test_anomalies = iso_forest.predict(X_test_scaled)\n",
        "# Convert to binary: -1 -> 1 (MEV), 1 -> 0 (Non-MEV)\n",
        "y_pred_iso = (test_anomalies == -1).astype(int)\n",
        "\n",
        "# Get anomaly scores (lower = more anomalous)\n",
        "anomaly_scores = iso_forest.score_samples(X_test_scaled)\n",
        "# Normalize scores to probabilities (lower scores = higher MEV probability)\n",
        "y_proba_iso = 1 / (1 + np.exp(anomaly_scores))  # Inverse sigmoid\n",
        "\n",
        "models['Isolation Forest'] = iso_forest\n",
        "predictions['Isolation Forest'] = y_pred_iso\n",
        "probabilities['Isolation Forest'] = y_proba_iso\n",
        "\n",
        "print(\"✓ Training complete\")\n",
        "print(f\"  - Contamination rate: {mev_ratio:.4f}\")\n",
        "print(f\"  - Test Accuracy: {accuracy_score(y_test, y_pred_iso):.4f}\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ALL MODELS TRAINED\")\n",
        "print(\"=\"*80)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation - Binary Classification Metrics\n",
        "\n",
        "### Metrics for Imbalanced Data:\n",
        "\n",
        "- **Precision (MEV)**: Of predicted MEV, how many are actually MEV?\n",
        "- **Recall (MEV)**: Of actual MEV, how many did we catch?\n",
        "- **F1-Score (MEV)**: Harmonic mean of precision and recall\n",
        "- **PR-AUC**: Area under Precision-Recall curve (better than ROC-AUC for imbalanced data)\n",
        "- **ROC-AUC**: Area under ROC curve\n",
        "\n",
        "### Target Goals:\n",
        "\n",
        "- **MEV Recall > 0.7**: Catch most MEV bots\n",
        "- **MEV F1 > 0.5**: Balanced precision and recall\n",
        "- **PR-AUC > 0.6**: Good performance on imbalanced data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"4. MODEL EVALUATION - BINARY CLASSIFICATION METRICS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Calculate metrics for each model\n",
        "results = []\n",
        "\n",
        "for model_name in models.keys():\n",
        "    y_pred = predictions[model_name]\n",
        "    y_proba = probabilities[model_name]\n",
        "    \n",
        "    # Binary classification metrics\n",
        "    precision_mev = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
        "    recall_mev = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
        "    f1_mev = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    # ROC-AUC and PR-AUC\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "    \n",
        "    try:\n",
        "        pr_auc = average_precision_score(y_test, y_proba)\n",
        "    except:\n",
        "        pr_auc = 0.0\n",
        "    \n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision_mev': precision_mev,\n",
        "        'recall_mev': recall_mev,\n",
        "        'f1_mev': f1_mev,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc\n",
        "    })\n",
        "    \n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  - MEV Precision: {precision_mev:.4f}\")\n",
        "    print(f\"  - MEV Recall: {recall_mev:.4f}\")\n",
        "    print(f\"  - MEV F1: {f1_mev:.4f}\")\n",
        "    print(f\"  - ROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"  - PR-AUC: {pr_auc:.4f}\")\n",
        "    print()\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Non-MEV', 'MEV'], zero_division=0))\n",
        "    print()\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"=\"*80)\n",
        "print(\"SUMMARY - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Find best model by F1 (MEV)\n",
        "best_model_f1 = results_df.loc[results_df['f1_mev'].idxmax(), 'model']\n",
        "best_f1 = results_df.loc[results_df['f1_mev'].idxmax(), 'f1_mev']\n",
        "print(f\"Best model by MEV F1: {best_model_f1} (F1={best_f1:.4f})\")\n",
        "\n",
        "# Find best model by PR-AUC\n",
        "best_model_pr = results_df.loc[results_df['pr_auc'].idxmax(), 'model']\n",
        "best_pr = results_df.loc[results_df['pr_auc'].idxmax(), 'pr_auc']\n",
        "print(f\"Best model by PR-AUC: {best_model_pr} (PR-AUC={best_pr:.4f})\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "print(\"Creating visualizations...\")\n",
        "\n",
        "# 1. Confusion Matrices\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, model_name in enumerate(models.keys()):\n",
        "    if idx < len(axes):\n",
        "        cm = confusion_matrix(y_test, predictions[model_name])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                   xticklabels=['Non-MEV', 'MEV'],\n",
        "                   yticklabels=['Non-MEV', 'MEV'])\n",
        "        axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
        "        axes[idx].set_ylabel('True Label')\n",
        "        axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('derived/ml_results_binary/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Saved: confusion_matrices.png\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Precision-Recall Curves\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for model_name in models.keys():\n",
        "    y_proba = probabilities[model_name]\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "    pr_auc = average_precision_score(y_test, y_proba)\n",
        "    ax.plot(recall, precision, label=f'{model_name} (AUC={pr_auc:.3f})', linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Recall', fontsize=12)\n",
        "ax.set_ylabel('Precision', fontsize=12)\n",
        "ax.set_title('Precision-Recall Curves (Binary Classification)', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('derived/ml_results_binary/pr_curves.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Saved: pr_curves.png\")\n",
        "plt.show()\n",
        "\n",
        "# 3. ROC Curves\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for model_name in models.keys():\n",
        "    y_proba = probabilities[model_name]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    ax.plot(fpr, tpr, label=f'{model_name} (AUC={roc_auc:.3f})', linewidth=2)\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', label='Random', alpha=0.5)\n",
        "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
        "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
        "ax.set_title('ROC Curves (Binary Classification)', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('derived/ml_results_binary/roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Saved: roc_curves.png\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Metrics Comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "metrics_to_plot = ['precision_mev', 'recall_mev', 'f1_mev', 'pr_auc']\n",
        "metric_labels = ['MEV Precision', 'MEV Recall', 'MEV F1-Score', 'PR-AUC']\n",
        "\n",
        "for idx, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    values = results_df[metric].values\n",
        "    model_names = results_df['model'].values\n",
        "    \n",
        "    bars = ax.barh(model_names, values, alpha=0.7)\n",
        "    ax.set_xlabel(label, fontsize=11)\n",
        "    ax.set_title(f'{label} Comparison', fontweight='bold')\n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
        "        ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('derived/ml_results_binary/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Saved: metrics_comparison.png\")\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"VISUALIZATIONS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Monte Carlo Simulation - Model Stability\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "Bootstrap validation to assess model stability and uncertainty in F1 scores. This helps understand:\n",
        "- How robust the model is to different training samples\n",
        "- Confidence intervals for MEV F1-score\n",
        "- Model reliability for production use\n",
        "\n",
        "### Method:\n",
        "\n",
        "- **N=1000 bootstrap iterations**: Sample with replacement from training data\n",
        "- **Train model on each bootstrap sample**: Use same hyperparameters\n",
        "- **Evaluate on test set**: Calculate MEV F1-score\n",
        "- **Calculate statistics**: Mean, 95% confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"5. MONTE CARLO SIMULATION - MODEL STABILITY\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Monte Carlo simulation for best model (XGBoost)\n",
        "print(\"Running Monte Carlo simulation for XGBoost...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "N = 1000  # Number of bootstrap iterations\n",
        "f1_mev_bootstrap = []\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "for i in range(N):\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"  Iteration {i+1}/{N}...\")\n",
        "    \n",
        "    # Bootstrap sample with replacement\n",
        "    bootstrap_indices = np.random.choice(len(X_train_res), size=len(X_train_res), replace=True)\n",
        "    X_boot = X_train_res[bootstrap_indices]\n",
        "    y_boot = y_train_res[bootstrap_indices]\n",
        "    \n",
        "    # Train model on bootstrap sample\n",
        "    boot_model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=42,\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    boot_model.fit(X_boot, y_boot)\n",
        "    \n",
        "    # Predict on test set\n",
        "    y_pred_boot = boot_model.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate MEV F1-score\n",
        "    f1_mev = f1_score(y_test, y_pred_boot, pos_label=1, zero_division=0)\n",
        "    f1_mev_bootstrap.append(f1_mev)\n",
        "\n",
        "f1_mev_bootstrap = np.array(f1_mev_bootstrap)\n",
        "\n",
        "# Calculate statistics\n",
        "mean_f1 = np.mean(f1_mev_bootstrap)\n",
        "std_f1 = np.std(f1_mev_bootstrap)\n",
        "ci_95 = np.percentile(f1_mev_bootstrap, [2.5, 97.5])\n",
        "\n",
        "print()\n",
        "print(\"Monte Carlo Results:\")\n",
        "print(f\"  - Mean MEV F1: {mean_f1:.4f}\")\n",
        "print(f\"  - Std Dev: {std_f1:.4f}\")\n",
        "print(f\"  - 95% CI: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\n",
        "print()\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.hist(f1_mev_bootstrap, bins=50, alpha=0.7, edgecolor='black')\n",
        "ax.axvline(mean_f1, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_f1:.4f}')\n",
        "ax.axvline(ci_95[0], color='orange', linestyle='--', linewidth=1, label=f'95% CI: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]')\n",
        "ax.axvline(ci_95[1], color='orange', linestyle='--', linewidth=1)\n",
        "ax.set_xlabel('MEV F1-Score', fontsize=12)\n",
        "ax.set_ylabel('Frequency', fontsize=12)\n",
        "ax.set_title('Monte Carlo Simulation: MEV F1-Score Distribution (N=1000)', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('derived/ml_results_binary/monte_carlo_f1_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Saved: monte_carlo_f1_distribution.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MONTE CARLO SIMULATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Parameter Optimization - GridSearchCV\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "Find optimal hyperparameters for the best model (XGBoost) using cross-validation with F1-score as the metric.\n",
        "\n",
        "### Grid Search Parameters:\n",
        "\n",
        "- **max_depth**: Tree depth [3, 5, 7]\n",
        "- **learning_rate**: Learning rate [0.01, 0.1, 0.2]\n",
        "- **n_estimators**: Number of trees [100, 200]\n",
        "\n",
        "### Scoring:\n",
        "\n",
        "- **scoring='f1'**: Focus on MEV F1-score (pos_label=1)\n",
        "- **cv=5**: 5-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"6. PARAMETER OPTIMIZATION - GRIDSEARCHCV\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Parameter grid for XGBoost\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200]\n",
        "}\n",
        "\n",
        "print(\"Grid Search Parameters:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  - {param}: {values}\")\n",
        "print()\n",
        "\n",
        "# Create base model\n",
        "base_model = xgb.XGBClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Grid search with F1 scoring (for MEV class)\n",
        "print(\"Running GridSearchCV (this may take several minutes)...\")\n",
        "grid_search = GridSearchCV(\n",
        "    base_model,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',  # F1-score for MEV class (pos_label=1)\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "print()\n",
        "print(\"Grid Search Results:\")\n",
        "print(f\"  - Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"  - Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
        "print()\n",
        "\n",
        "# Evaluate best model on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test_scaled)\n",
        "y_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "test_f1 = f1_score(y_test, y_pred_best, pos_label=1, zero_division=0)\n",
        "test_precision = precision_score(y_test, y_pred_best, pos_label=1, zero_division=0)\n",
        "test_recall = recall_score(y_test, y_pred_best, pos_label=1, zero_division=0)\n",
        "test_pr_auc = average_precision_score(y_test, y_proba_best)\n",
        "\n",
        "print(\"Best Model Performance on Test Set:\")\n",
        "print(f\"  - MEV Precision: {test_precision:.4f}\")\n",
        "print(f\"  - MEV Recall: {test_recall:.4f}\")\n",
        "print(f\"  - MEV F1: {test_f1:.4f}\")\n",
        "print(f\"  - PR-AUC: {test_pr_auc:.4f}\")\n",
        "print()\n",
        "\n",
        "# Compare with original XGBoost\n",
        "original_f1 = f1_score(y_test, y_pred_xgb, pos_label=1, zero_division=0)\n",
        "improvement = test_f1 - original_f1\n",
        "\n",
        "print(\"Comparison with Original XGBoost:\")\n",
        "print(f\"  - Original F1: {original_f1:.4f}\")\n",
        "print(f\"  - Optimized F1: {test_f1:.4f}\")\n",
        "print(f\"  - Improvement: {improvement:+.4f}\")\n",
        "print()\n",
        "\n",
        "# Save best model\n",
        "models['XGBoost (Optimized)'] = best_model\n",
        "predictions['XGBoost (Optimized)'] = y_pred_best\n",
        "probabilities['XGBoost (Optimized)'] = y_proba_best\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PARAMETER OPTIMIZATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Results Export\n",
        "\n",
        "### Final Results:\n",
        "\n",
        "- All models trained and evaluated\n",
        "- Best model identified by F1-score and PR-AUC\n",
        "- Monte Carlo simulation shows model stability\n",
        "- Parameter optimization completed\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "- `derived/ml_results_binary/confusion_matrices.png`\n",
        "- `derived/ml_results_binary/pr_curves.png`\n",
        "- `derived/ml_results_binary/roc_curves.png`\n",
        "- `derived/ml_results_binary/metrics_comparison.png`\n",
        "- `derived/ml_results_binary/monte_carlo_f1_distribution.png`\n",
        "- `derived/ml_results_binary/results_summary.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"7. SUMMARY AND RESULTS EXPORT\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Prepare summary data\n",
        "summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'dataset_info': {\n",
        "        'total_samples': len(df_clean_features),\n",
        "        'mev_samples': int(np.sum(y == 1)),\n",
        "        'non_mev_samples': int(np.sum(y == 0)),\n",
        "        'imbalance_ratio': float(imbalance_ratio),\n",
        "        'features': feature_cols\n",
        "    },\n",
        "    'models': {}\n",
        "}\n",
        "\n",
        "# Add model results\n",
        "for model_name in models.keys():\n",
        "    if model_name in predictions:\n",
        "        y_pred = predictions[model_name]\n",
        "        y_proba = probabilities[model_name]\n",
        "        \n",
        "        summary['models'][model_name] = {\n",
        "            'accuracy': float(accuracy_score(y_test, y_pred)),\n",
        "            'precision_mev': float(precision_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "            'recall_mev': float(recall_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "            'f1_mev': float(f1_score(y_test, y_pred, pos_label=1, zero_division=0)),\n",
        "            'roc_auc': float(roc_auc_score(y_test, y_proba)) if len(np.unique(y_test)) > 1 else 0.0,\n",
        "            'pr_auc': float(average_precision_score(y_test, y_proba))\n",
        "        }\n",
        "\n",
        "# Add Monte Carlo results\n",
        "summary['monte_carlo'] = {\n",
        "    'n_iterations': N,\n",
        "    'mean_f1_mev': float(mean_f1),\n",
        "    'std_f1_mev': float(std_f1),\n",
        "    'ci_95_lower': float(ci_95[0]),\n",
        "    'ci_95_upper': float(ci_95[1])\n",
        "}\n",
        "\n",
        "# Add grid search results\n",
        "if 'grid_search' in locals():\n",
        "    summary['grid_search'] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'best_cv_score': float(grid_search.best_score_),\n",
        "        'test_f1': float(test_f1),\n",
        "        'test_pr_auc': float(test_pr_auc)\n",
        "    }\n",
        "\n",
        "# Save to JSON\n",
        "output_file = 'derived/ml_results_binary/results_summary.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"✓ Results saved to: {output_file}\")\n",
        "print()\n",
        "\n",
        "# Print final summary\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "print(\"Best Models:\")\n",
        "print(f\"  - By MEV F1: {best_model_f1} (F1={best_f1:.4f})\")\n",
        "print(f\"  - By PR-AUC: {best_model_pr} (PR-AUC={best_pr:.4f})\")\n",
        "print()\n",
        "\n",
        "print(\"Target Goals:\")\n",
        "targets_met = []\n",
        "if best_f1 > 0.5:\n",
        "    targets_met.append(\"✓ MEV F1 > 0.5\")\n",
        "else:\n",
        "    targets_met.append(\"✗ MEV F1 > 0.5 (current: {:.4f})\".format(best_f1))\n",
        "\n",
        "best_recall = results_df.loc[results_df['f1_mev'].idxmax(), 'recall_mev']\n",
        "if best_recall > 0.7:\n",
        "    targets_met.append(\"✓ MEV Recall > 0.7\")\n",
        "else:\n",
        "    targets_met.append(\"✗ MEV Recall > 0.7 (current: {:.4f})\".format(best_recall))\n",
        "\n",
        "if best_pr > 0.6:\n",
        "    targets_met.append(\"✓ PR-AUC > 0.6\")\n",
        "else:\n",
        "    targets_met.append(\"✗ PR-AUC > 0.6 (current: {:.4f})\".format(best_pr))\n",
        "\n",
        "for target in targets_met:\n",
        "    print(f\"  {target}\")\n",
        "\n",
        "print()\n",
        "print(\"Model Stability (Monte Carlo):\")\n",
        "print(f\"  - Mean MEV F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
        "print(f\"  - 95% CI: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"BINARY CLASSIFICATION ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aggregator Separation and Pool Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook separates **aggregators** (Jupiter, DeFlow, Phoenix, etc.) from the binary classification results and analyzes their corresponding pools/AMMs.\n",
        "\n",
        "### Why This Notebook?\n",
        "\n",
        "In binary classification (07a), all non-MEV classes (aggregators, wash trading, regular) were grouped into **Non-MEV (0)**. This simplified the problem but \"lost\" the aggregator distinction.\n",
        "\n",
        "**Aggregators** are legitimate routers that help users find the best prices across multiple pools. They are characterized by:\n",
        "- High  (>0.5, often >0.8)\n",
        "- Trading across **many pools** (8+ unique pools/AMMs)\n",
        "- Low MEV score (not malicious)\n",
        "- High trade volume but distributed across pools\n",
        "\n",
        "### What This Notebook Does:\n",
        "\n",
        "1. **Loads Binary Classification Results**: Uses the same data as 07a\n",
        "2. **Separates Aggregators**: Uses  threshold to identify aggregators from Non-MEV class\n",
        "3. **Finds Corresponding Pools**: Analyzes which pools/AMMs each aggregator uses\n",
        "4. **Visualizes Separation**: Shows aggregators vs MEV vs other Non-MEV\n",
        "5. **Pool Analysis**: Identifies top aggregators and their pool usage patterns\n",
        "\n",
        "### Key Outputs:\n",
        "\n",
        "- **Aggregator List**: Signers identified as aggregators with their characteristics\n",
        "- **Pool Mapping**: Which pools each aggregator uses\n",
        "- **Visualizations**: Scatter plots showing aggregator separation\n",
        "- **Statistics**: Aggregator counts, pool distributions, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Aggregator Separation and Pool Analysis\n",
        "Separates aggregators from binary classification results and finds their pools\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('derived/aggregator_analysis', exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"AGGREGATOR SEPARATION AND POOL ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Recreate Features\n",
        "\n",
        "Load the transaction data and recreate signer-level features (same as 07a) to get `aggregator_likelihood` and other features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"1. LOAD DATA AND CREATE FEATURES\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Load cleaned transaction data\n",
        "print(\"Loading cleaned transaction data...\")\n",
        "try:\n",
        "    # Try multiple possible paths\n",
        "    possible_paths = [\n",
        "        '../01_data_cleaning/outputs/pamm_clean_final.parquet',\n",
        "        '../../01_data_cleaning/outputs/pamm_clean_final.parquet',\n",
        "        '/Users/aileen/Downloads/pamm/solana-pamm-analysis/notebooks/01_data_cleaning/outputs/pamm_clean_final.parquet',\n",
        "        'pamm_clean_final.parquet'\n",
        "    ]\n",
        "    \n",
        "    df_clean = None\n",
        "    for path in possible_paths:\n",
        "        try:\n",
        "            df_clean = pd.read_parquet(path)\n",
        "            print(f\"✓ Loaded from: {path}\")\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    if df_clean is None:\n",
        "        raise FileNotFoundError(\"Could not find pamm_clean_final.parquet in any expected location\")\n",
        "    \n",
        "    print(f\"✓ Loaded {len(df_clean):,} transaction records\")\n",
        "    print(f\"✓ Time range: {df_clean['datetime'].min()} to {df_clean['datetime'].max()}\")\n",
        "    print(f\"✓ Event types: {df_clean['kind'].value_counts().to_dict()}\")\n",
        "    print()\n",
        "    \n",
        "    # Filter to TRADE events only\n",
        "    df_trades = df_clean[df_clean['kind'] == 'TRADE'].copy()\n",
        "    print(f\"✓ TRADE events: {len(df_trades):,} records\")\n",
        "    print(f\"✓ Unique signers: {df_trades['signer'].nunique():,}\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Feature Engineering (same as 07a)\n",
        "print(\"Engineering features from transaction data...\")\n",
        "print(\"This may take a few minutes for large datasets...\")\n",
        "\n",
        "signer_features = []\n",
        "unique_signers = df_trades['signer'].unique()\n",
        "print(f\"Processing {len(unique_signers):,} unique signers...\")\n",
        "\n",
        "# Sample if too large\n",
        "if len(unique_signers) > 5000:\n",
        "    print(f\"⚠️  Large dataset detected. Sampling 5,000 signers for faster processing...\")\n",
        "    np.random.seed(42)\n",
        "    unique_signers = np.random.choice(unique_signers, size=5000, replace=False)\n",
        "    print(f\"Processing {len(unique_signers):,} sampled signers...\")\n",
        "\n",
        "print(\"Processing signers...\")\n",
        "for i, signer in enumerate(unique_signers):\n",
        "    if (i + 1) % 500 == 0:\n",
        "        print(f\"  Processed {i+1}/{len(unique_signers)} signers...\")\n",
        "    signer_trades = df_trades[df_trades['signer'] == signer].copy()\n",
        "    \n",
        "    if len(signer_trades) < 2:\n",
        "        continue\n",
        "    \n",
        "    total_trades = len(signer_trades)\n",
        "    time_span_hours = (signer_trades['datetime'].max() - signer_trades['datetime'].min()).total_seconds() / 3600\n",
        "    trades_per_hour = total_trades / max(time_span_hours, 0.1)\n",
        "    \n",
        "    late_slot_trades = (signer_trades['us_since_first_shred'] > 300000).sum()\n",
        "    late_slot_ratio = late_slot_trades / total_trades\n",
        "    \n",
        "    signer_slots = signer_trades['slot'].unique()\n",
        "    oracle_backrun_count = 0\n",
        "    slot_oracles = df_clean[(df_clean['slot'].isin(signer_slots)) & \n",
        "                           (df_clean['kind'] == 'ORACLE')][['slot', 'ms_time']]\n",
        "    \n",
        "    if len(slot_oracles) > 0:\n",
        "        oracle_by_slot = slot_oracles.groupby('slot')['ms_time'].apply(list).to_dict()\n",
        "        for _, trade in signer_trades.iterrows():\n",
        "            slot = trade['slot']\n",
        "            trade_time = trade['ms_time']\n",
        "            if slot in oracle_by_slot:\n",
        "                oracle_times = oracle_by_slot[slot]\n",
        "                time_diffs = [abs(ot - trade_time) for ot in oracle_times]\n",
        "                if min(time_diffs) < 50:\n",
        "                    oracle_backrun_count += 1\n",
        "    oracle_backrun_ratio = oracle_backrun_count / total_trades if total_trades > 0 else 0\n",
        "    \n",
        "    high_bytes_trades = (signer_trades['bytes_changed_trade'] > 50).sum()\n",
        "    high_bytes_ratio = high_bytes_trades / total_trades if total_trades > 0 else 0\n",
        "    \n",
        "    slot_counts = signer_trades.groupby('slot').size()\n",
        "    clustered_slots = (slot_counts >= 2).sum()\n",
        "    cluster_ratio = clustered_slots / signer_trades['slot'].nunique() if signer_trades['slot'].nunique() > 0 else 0\n",
        "    \n",
        "    signer_slots = signer_trades['slot'].unique()\n",
        "    sample_size = min(100, len(signer_slots))\n",
        "    \n",
        "    if sample_size > 0:\n",
        "        sampled_slots = np.random.choice(signer_slots, size=sample_size, replace=False)\n",
        "        slot_trade_counts = df_trades[df_trades['slot'].isin(sampled_slots)].groupby('slot').agg({\n",
        "            'signer': ['count', 'nunique']\n",
        "        }).reset_index()\n",
        "        slot_trade_counts.columns = ['slot', 'total_trades', 'unique_signers']\n",
        "        slot_trade_counts = slot_trade_counts[slot_trade_counts['total_trades'] > 1]\n",
        "        \n",
        "        if len(slot_trade_counts) > 0:\n",
        "            slot_trade_counts['unique_ratio'] = slot_trade_counts['unique_signers'] / slot_trade_counts['total_trades']\n",
        "            aggregator_slots = (slot_trade_counts['unique_ratio'] > 0.7).sum()\n",
        "            aggregator_likelihood = aggregator_slots / len(slot_trade_counts)\n",
        "        else:\n",
        "            aggregator_likelihood = 0\n",
        "    else:\n",
        "        aggregator_likelihood = 0\n",
        "    \n",
        "    mev_score = (late_slot_ratio * 0.3 + \n",
        "                 oracle_backrun_ratio * 0.3 + \n",
        "                 high_bytes_ratio * 0.2 + \n",
        "                 cluster_ratio * 0.2)\n",
        "    \n",
        "    wash_trading_score = trades_per_hour / max(mev_score + 0.1, 0.1)\n",
        "    \n",
        "    # Count unique pools/AMMs for this signer\n",
        "    unique_pools = 0\n",
        "    if 'amm_trade' in signer_trades.columns:\n",
        "        unique_pools = signer_trades['amm_trade'].nunique()\n",
        "    \n",
        "    signer_features.append({\n",
        "        'signer': signer,\n",
        "        'total_trades': total_trades,\n",
        "        'trades_per_hour': trades_per_hour,\n",
        "        'aggregator_likelihood': aggregator_likelihood,\n",
        "        'late_slot_ratio': late_slot_ratio,\n",
        "        'oracle_backrun_ratio': oracle_backrun_ratio,\n",
        "        'high_bytes_ratio': high_bytes_ratio,\n",
        "        'cluster_ratio': cluster_ratio,\n",
        "        'mev_score': mev_score,\n",
        "        'wash_trading_score': wash_trading_score,\n",
        "        'unique_pools': unique_pools\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(signer_features)\n",
        "print(f\"✓ Created features for {len(df)} signers\")\n",
        "print()\n",
        "\n",
        "# Feature selection\n",
        "feature_cols = [\n",
        "    'total_trades', 'trades_per_hour', 'aggregator_likelihood',\n",
        "    'late_slot_ratio', 'oracle_backrun_ratio', 'high_bytes_ratio',\n",
        "    'cluster_ratio', 'mev_score', 'wash_trading_score'\n",
        "]\n",
        "\n",
        "df_clean_features = df[feature_cols + ['signer', 'unique_pools']].dropna()\n",
        "\n",
        "if len(df_clean_features) == 0:\n",
        "    raise ValueError(\"❌ Error: No data remaining after cleaning.\")\n",
        "\n",
        "print(f\"✓ Cleaned dataset: {len(df_clean_features)} records\")\n",
        "print(f\"✓ Features: {len(feature_cols)}\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Separate Aggregators from Binary Classification\n",
        "\n",
        "Use `aggregator_likelihood` threshold to separate aggregators from Non-MEV class. Also use `unique_pools` count as additional indicator (aggregators typically use 8+ pools)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"2. SEPARATE AGGREGATORS FROM BINARY CLASSIFICATION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# First, create binary labels (same as 07a)\n",
        "mev_labels = [\n",
        "    'LIKELY MEV BOT',\n",
        "    'POSSIBLE MEV (Sandwich patterns)',\n",
        "    'POSSIBLE MEV (Cluster patterns)',\n",
        "    'LIKELY MEV BOT (Fat Sandwich)'\n",
        "]\n",
        "\n",
        "# Recreate classification for binary mapping\n",
        "def classify_signer(row):\n",
        "    if row['aggregator_likelihood'] > 0.5:\n",
        "        return \"LIKELY AGGREGATOR (Jupiter, etc.)\"\n",
        "    elif row['wash_trading_score'] > 1.0 and row['mev_score'] < 0.2:\n",
        "        return \"LIKELY WASH TRADING (Volume Inflation)\"\n",
        "    elif row['mev_score'] > 0.3:\n",
        "        return \"LIKELY MEV BOT\"\n",
        "    elif row['cluster_ratio'] > 0.3:\n",
        "        return \"POSSIBLE MEV (Sandwich patterns)\"\n",
        "    else:\n",
        "        return \"REGULAR TRADE BOT / UNKNOWN\"\n",
        "\n",
        "df_clean_features['classification'] = df_clean_features.apply(classify_signer, axis=1)\n",
        "\n",
        "# Create binary label: 1 = MEV, 0 = Non-MEV\n",
        "df_clean_features['binary_label'] = df_clean_features['classification'].apply(\n",
        "    lambda x: 1 if x in mev_labels else 0\n",
        ")\n",
        "\n",
        "print(\"Original Binary Class Distribution:\")\n",
        "binary_dist = df_clean_features['binary_label'].value_counts()\n",
        "for label, count in sorted(binary_dist.items()):\n",
        "    label_name = 'MEV' if label == 1 else 'Non-MEV'\n",
        "    print(f\"  {label_name} ({label}): {count:,} samples ({count/len(df_clean_features)*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# === Separate Aggregator subclass (from Non-MEV) ===\n",
        "print(\"Separating Aggregators from Non-MEV...\")\n",
        "\n",
        "# Threshold for aggregator identification\n",
        "# High aggregator_likelihood (>0.8) OR many pools (8+) = aggregator\n",
        "high_agg_threshold = 0.8  # Can be adjusted based on data distribution\n",
        "\n",
        "# Create predicted_class column\n",
        "df_clean_features['predicted_class'] = 'Other Non-MEV'  # Default\n",
        "df_clean_features.loc[df_clean_features['binary_label'] == 1, 'predicted_class'] = 'MEV (Sandwich etc.)'\n",
        "\n",
        "# Aggregator identification: high likelihood OR many pools\n",
        "aggregator_mask = (\n",
        "    (df_clean_features['binary_label'] == 0) & \n",
        "    (\n",
        "        (df_clean_features['aggregator_likelihood'] > high_agg_threshold) |\n",
        "        (df_clean_features['unique_pools'] >= 8)\n",
        "    )\n",
        ")\n",
        "df_clean_features.loc[aggregator_mask, 'predicted_class'] = 'Aggregator (Jupiter/DeFlow etc.)'\n",
        "\n",
        "# Statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SEPARATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"Class Distribution:\")\n",
        "class_dist = df_clean_features['predicted_class'].value_counts()\n",
        "for cls, count in class_dist.items():\n",
        "    print(f\"  {cls}: {count:,} ({count/len(df_clean_features)*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# Aggregator statistics\n",
        "aggregators = df_clean_features[df_clean_features['predicted_class'] == 'Aggregator (Jupiter/DeFlow etc.)']\n",
        "print(f\"Aggregator Statistics:\")\n",
        "print(f\"  Total Aggregators: {len(aggregators):,}\")\n",
        "if len(aggregators) > 0:\n",
        "    print(f\"  Avg aggregator_likelihood: {aggregators['aggregator_likelihood'].mean():.3f}\")\n",
        "    print(f\"  Avg unique_pools: {aggregators['unique_pools'].mean():.1f}\")\n",
        "    print(f\"  Avg total_trades: {aggregators['total_trades'].mean():.0f}\")\n",
        "    print(f\"  Avg mev_score: {aggregators['mev_score'].mean():.3f}\")\n",
        "print()\n",
        "\n",
        "# Show aggregator_likelihood distribution\n",
        "print(\"Aggregator Likelihood Distribution:\")\n",
        "print(df_clean_features['aggregator_likelihood'].describe())\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Find Pools for Each Aggregator\n",
        "\n",
        "For each identified aggregator, find which pools/AMMs they use by analyzing the original trade data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"3. FIND POOLS FOR EACH AGGREGATOR\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Get list of aggregator signers\n",
        "aggregator_signers = aggregators['signer'].tolist()\n",
        "print(f\"Analyzing {len(aggregator_signers):,} aggregators...\")\n",
        "print()\n",
        "\n",
        "# Check if amm_trade column exists in trade data\n",
        "if 'amm_trade' not in df_trades.columns:\n",
        "    print(\"⚠️  Warning: 'amm_trade' column not found in trade data\")\n",
        "    print(\"   Will use 'account_trade' or other pool identifiers if available\")\n",
        "    print()\n",
        "    # Try to find alternative pool identifier\n",
        "    pool_cols = [col for col in df_trades.columns if 'pool' in col.lower() or 'amm' in col.lower() or 'account' in col.lower()]\n",
        "    if pool_cols:\n",
        "        print(f\"   Found alternative columns: {pool_cols}\")\n",
        "        pool_col = pool_cols[0]\n",
        "    else:\n",
        "        pool_col = None\n",
        "        print(\"   ⚠️  No pool identifier found - cannot map aggregators to pools\")\n",
        "else:\n",
        "    pool_col = 'amm_trade'\n",
        "    print(f\"✓ Using '{pool_col}' column for pool identification\")\n",
        "    print()\n",
        "\n",
        "# Analyze pools for each aggregator\n",
        "aggregator_pool_data = []\n",
        "\n",
        "for signer in aggregator_signers:\n",
        "    signer_trades = df_trades[df_trades['signer'] == signer].copy()\n",
        "    \n",
        "    if len(signer_trades) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Get aggregator features\n",
        "    agg_features = aggregators[aggregators['signer'] == signer].iloc[0]\n",
        "    \n",
        "    # Get unique pools\n",
        "    if pool_col and pool_col in signer_trades.columns:\n",
        "        unique_pools = signer_trades[pool_col].dropna().unique()\n",
        "        pool_list = [str(p) for p in unique_pools if str(p) != 'nan']\n",
        "        num_pools = len(pool_list)\n",
        "        \n",
        "        # Get pool trade counts\n",
        "        pool_counts = signer_trades[pool_col].value_counts().to_dict()\n",
        "    else:\n",
        "        pool_list = []\n",
        "        num_pools = 0\n",
        "        pool_counts = {}\n",
        "    \n",
        "    aggregator_pool_data.append({\n",
        "        'signer': signer,\n",
        "        'aggregator_likelihood': agg_features['aggregator_likelihood'],\n",
        "        'unique_pools': num_pools,\n",
        "        'pool_list': ', '.join(pool_list[:10]) + (f' (+{len(pool_list)-10} more)' if len(pool_list) > 10 else ''),\n",
        "        'total_trades': agg_features['total_trades'],\n",
        "        'trades_per_hour': agg_features['trades_per_hour'],\n",
        "        'mev_score': agg_features['mev_score'],\n",
        "        'top_pools': ', '.join([f\"{p}({c})\" for p, c in list(pool_counts.items())[:5]])\n",
        "    })\n",
        "\n",
        "aggregator_pools_df = pd.DataFrame(aggregator_pool_data)\n",
        "\n",
        "if len(aggregator_pools_df) > 0:\n",
        "    print(f\"✓ Analyzed {len(aggregator_pools_df)} aggregators\")\n",
        "    print()\n",
        "    \n",
        "    # Sort by aggregator_likelihood and unique_pools\n",
        "    aggregator_pools_df = aggregator_pools_df.sort_values(\n",
        "        ['aggregator_likelihood', 'unique_pools'], \n",
        "        ascending=[False, False]\n",
        "    )\n",
        "    \n",
        "    print(\"Top 20 Aggregators by Likelihood:\")\n",
        "    print(\"=\"*80)\n",
        "    display_cols = ['signer', 'aggregator_likelihood', 'unique_pools', 'total_trades', 'mev_score']\n",
        "    print(aggregator_pools_df[display_cols].head(20).to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Pool distribution statistics\n",
        "    print(\"Pool Distribution Statistics:\")\n",
        "    print(f\"  Aggregators with 8+ pools: {(aggregator_pools_df['unique_pools'] >= 8).sum()}\")\n",
        "    print(f\"  Aggregators with 5-7 pools: {((aggregator_pools_df['unique_pools'] >= 5) & (aggregator_pools_df['unique_pools'] < 8)).sum()}\")\n",
        "    print(f\"  Aggregators with 3-4 pools: {((aggregator_pools_df['unique_pools'] >= 3) & (aggregator_pools_df['unique_pools'] < 5)).sum()}\")\n",
        "    print(f\"  Aggregators with 1-2 pools: {(aggregator_pools_df['unique_pools'] < 3).sum()}\")\n",
        "    print()\n",
        "    \n",
        "    # Save to CSV\n",
        "    output_file = 'derived/aggregator_analysis/aggregators_with_pools.csv'\n",
        "    aggregator_pools_df.to_csv(output_file, index=False)\n",
        "    print(f\"✓ Saved aggregator pool data to: {output_file}\")\n",
        "    print()\n",
        "else:\n",
        "    print(\"⚠️  No aggregator pool data found\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Aggregator Separation\n",
        "\n",
        "Create visualizations showing how aggregators are separated from MEV and other Non-MEV classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"4. VISUALIZE AGGREGATOR SEPARATION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Scatter: aggregator_likelihood vs mev_score\n",
        "ax1 = axes[0, 0]\n",
        "for cls in df_clean_features['predicted_class'].unique():\n",
        "    data = df_clean_features[df_clean_features['predicted_class'] == cls]\n",
        "    ax1.scatter(data['aggregator_likelihood'], data['mev_score'], \n",
        "                label=cls, alpha=0.6, s=30)\n",
        "ax1.set_xlabel('Aggregator Likelihood', fontsize=12)\n",
        "ax1.set_ylabel('MEV Score', fontsize=12)\n",
        "ax1.set_title('Aggregator Separation: Likelihood vs MEV Score', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Scatter: unique_pools vs aggregator_likelihood\n",
        "ax2 = axes[0, 1]\n",
        "for cls in df_clean_features['predicted_class'].unique():\n",
        "    data = df_clean_features[df_clean_features['predicted_class'] == cls]\n",
        "    ax2.scatter(data['unique_pools'], data['aggregator_likelihood'], \n",
        "                label=cls, alpha=0.6, s=30)\n",
        "ax2.set_xlabel('Unique Pools', fontsize=12)\n",
        "ax2.set_ylabel('Aggregator Likelihood', fontsize=12)\n",
        "ax2.set_title('Pool Count vs Aggregator Likelihood', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Histogram: aggregator_likelihood distribution by class\n",
        "ax3 = axes[1, 0]\n",
        "for cls in df_clean_features['predicted_class'].unique():\n",
        "    data = df_clean_features[df_clean_features['predicted_class'] == cls]\n",
        "    ax3.hist(data['aggregator_likelihood'], bins=30, alpha=0.6, label=cls, density=True)\n",
        "ax3.set_xlabel('Aggregator Likelihood', fontsize=12)\n",
        "ax3.set_ylabel('Density', fontsize=12)\n",
        "ax3.set_title('Aggregator Likelihood Distribution by Class', fontsize=14, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Bar chart: class counts\n",
        "ax4 = axes[1, 1]\n",
        "class_counts = df_clean_features['predicted_class'].value_counts()\n",
        "colors = ['red' if 'MEV' in cls else 'green' if 'Aggregator' in cls else 'gray' \n",
        "          for cls in class_counts.index]\n",
        "ax4.bar(range(len(class_counts)), class_counts.values, color=colors, alpha=0.7)\n",
        "ax4.set_xticks(range(len(class_counts)))\n",
        "ax4.set_xticklabels(class_counts.index, rotation=45, ha='right')\n",
        "ax4.set_ylabel('Count', fontsize=12)\n",
        "ax4.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "output_path = 'derived/aggregator_analysis/aggregator_separation_visualization.png'\n",
        "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Saved visualization to: {output_path}\")\n",
        "plt.show()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Top Aggregators Analysis\n",
        "\n",
        "Show detailed information about top aggregators and their pool usage patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"5. TOP AGGREGATORS ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "if len(aggregator_pools_df) > 0:\n",
        "    # Top aggregators by likelihood\n",
        "    print(\"Top 10 Aggregators by Aggregator Likelihood:\")\n",
        "    print(\"=\"*80)\n",
        "    top_by_likelihood = aggregator_pools_df.nlargest(10, 'aggregator_likelihood')\n",
        "    display_cols = ['signer', 'aggregator_likelihood', 'unique_pools', 'total_trades', \n",
        "                    'trades_per_hour', 'mev_score']\n",
        "    print(top_by_likelihood[display_cols].to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Top aggregators by pool count\n",
        "    print(\"Top 10 Aggregators by Pool Count:\")\n",
        "    print(\"=\"*80)\n",
        "    top_by_pools = aggregator_pools_df.nlargest(10, 'unique_pools')\n",
        "    print(top_by_pools[display_cols].to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Show pool lists for top aggregators\n",
        "    print(\"Top 5 Aggregators - Pool Details:\")\n",
        "    print(\"=\"*80)\n",
        "    for idx, row in aggregator_pools_df.head(5).iterrows():\n",
        "        print(f\"\\nSigner: {row['signer'][:50]}...\")\n",
        "        print(f\"  Aggregator Likelihood: {row['aggregator_likelihood']:.3f}\")\n",
        "        print(f\"  Unique Pools: {row['unique_pools']}\")\n",
        "        print(f\"  Total Trades: {row['total_trades']:,}\")\n",
        "        print(f\"  MEV Score: {row['mev_score']:.3f}\")\n",
        "        if row['pool_list']:\n",
        "            print(f\"  Pools: {row['pool_list']}\")\n",
        "        print()\n",
        "    \n",
        "    # Aggregate pool usage across all aggregators\n",
        "    if pool_col and pool_col in df_trades.columns:\n",
        "        print(\"Most Used Pools by Aggregators:\")\n",
        "        print(\"=\"*80)\n",
        "        aggregator_trades = df_trades[df_trades['signer'].isin(aggregator_signers)]\n",
        "        pool_usage = aggregator_trades[pool_col].value_counts().head(20)\n",
        "        print(pool_usage.to_string())\n",
        "        print()\n",
        "        \n",
        "        # Save pool usage\n",
        "        pool_usage_df = pd.DataFrame({\n",
        "            'pool': pool_usage.index,\n",
        "            'trade_count': pool_usage.values,\n",
        "            'unique_aggregators': [aggregator_trades[aggregator_trades[pool_col] == pool]['signer'].nunique() \n",
        "                                   for pool in pool_usage.index]\n",
        "        })\n",
        "        pool_usage_file = 'derived/aggregator_analysis/top_pools_by_aggregators.csv'\n",
        "        pool_usage_df.to_csv(pool_usage_file, index=False)\n",
        "        print(f\"✓ Saved pool usage to: {pool_usage_file}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"⚠️  No aggregator data available for detailed analysis\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary Statistics\n",
        "\n",
        "Final summary of aggregator separation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"6. SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "print(\"Final Summary:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Total Signers Analyzed: {len(df_clean_features):,}\")\n",
        "print()\n",
        "print(\"Class Breakdown:\")\n",
        "for cls, count in df_clean_features['predicted_class'].value_counts().items():\n",
        "    print(f\"  {cls}: {count:,} ({count/len(df_clean_features)*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "if len(aggregators) > 0:\n",
        "    print(\"Aggregator Summary:\")\n",
        "    print(f\"  Total Aggregators Identified: {len(aggregators):,}\")\n",
        "    print(f\"  Average Aggregator Likelihood: {aggregators['aggregator_likelihood'].mean():.3f}\")\n",
        "    print(f\"  Median Aggregator Likelihood: {aggregators['aggregator_likelihood'].median():.3f}\")\n",
        "    print(f\"  Average Unique Pools: {aggregators['unique_pools'].mean():.1f}\")\n",
        "    print(f\"  Median Unique Pools: {aggregators['unique_pools'].median():.1f}\")\n",
        "    print(f\"  Total Trades by Aggregators: {aggregators['total_trades'].sum():,}\")\n",
        "    print()\n",
        "\n",
        "print(\"Key Insights:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"1. Aggregators are successfully separated from MEV bots\")\n",
        "print(\"   - Aggregators have high aggregator_likelihood (>0.8) OR many pools (8+)\")\n",
        "print(\"   - Aggregators have low MEV scores (not malicious)\")\n",
        "print()\n",
        "print(\"2. Pool Analysis:\")\n",
        "if len(aggregator_pools_df) > 0:\n",
        "    print(f\"   - {len(aggregator_pools_df)} aggregators identified\")\n",
        "    print(f\"   - Average pools per aggregator: {aggregator_pools_df['unique_pools'].mean():.1f}\")\n",
        "    print(f\"   - Aggregators with 8+ pools: {(aggregator_pools_df['unique_pools'] >= 8).sum()}\")\n",
        "else:\n",
        "    print(\"   - Pool data not available\")\n",
        "print()\n",
        "print(\"3. Binary classification successfully separates:\")\n",
        "print(\"   - MEV bots (high mev_score, low aggregator_likelihood)\")\n",
        "print(\"   - Aggregators (high aggregator_likelihood, many pools, low mev_score)\")\n",
        "print(\"   - Other Non-MEV (regular trading, wash trading)\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"Output files saved to: derived/aggregator_analysis/\")\n",
        "print(\"  - aggregators_with_pools.csv\")\n",
        "print(\"  - top_pools_by_aggregators.csv\")\n",
        "print(\"  - aggregator_separation_visualization.png\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}